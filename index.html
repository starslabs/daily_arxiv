<!DOCTYPE html>
<html lang="en">

<head>
    <title>Daily Paper from Arxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-27T00:00:00Z">2024-03-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">92</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mini-Gemini: Mining the Potential of Multi-modality Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Mini-Gemini, a simple and effective framework
enhancing multi-modality Vision Language Models (VLMs). Despite the
advancements in VLMs facilitating basic visual dialog and reasoning, a
performance gap persists compared to advanced models like GPT-4 and Gemini. We
try to narrow the gap by mining the potential of VLMs for better performance
and any-to-any workflow from three aspects, i.e., high-resolution visual
tokens, high-quality data, and VLM-guided generation. To enhance visual tokens,
we propose to utilize an additional visual encoder for high-resolution
refinement without increasing the visual token count. We further construct a
high-quality dataset that promotes precise image comprehension and
reasoning-based generation, expanding the operational scope of current VLMs. In
general, Mini-Gemini further mines the potential of VLMs and empowers current
frameworks with image understanding, reasoning, and generation simultaneously.
Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs)
from 2B to 34B. It is demonstrated to achieve leading performance in several
zero-shot benchmarks and even surpasses the developed private models. Code and
models are available at https://github.com/dvlab-research/MiniGemini.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/dvlab-research/MiniGemini</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Modularity Transferable? A Case Study through the Lens of Knowledge
  Distillation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Klimaszewski, Piotr Andruszkiewicz, Alexandra Birch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of Modular Deep Learning showcases its potential in various Natural
Language Processing applications. Parameter-efficient fine-tuning (PEFT)
modularity has been shown to work for various use cases, from domain adaptation
to multilingual setups. However, all this work covers the case where the
modular components are trained and deployed within one single Pre-trained
Language Model (PLM). This model-specific setup is a substantial limitation on
the very modularity that modular architectures are trying to achieve. We ask
whether current modular approaches are transferable between models and whether
we can transfer the modules from more robust and larger PLMs to smaller ones.
In this work, we aim to fill this gap via a lens of Knowledge Distillation,
commonly used for model compression, and present an extremely straightforward
approach to transferring pre-trained, task-specific PEFT modules between
same-family PLMs. Moreover, we propose a method that allows the transfer of
modules between incompatible PLMs without any change in the inference
complexity. The experiments on Named Entity Recognition, Natural Language
Inference, and Paraphrase Identification tasks over multiple languages and PEFT
methods showcase the initial potential of transferable modularity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Projective Methods for Mitigating Gender Bias in <span class="highlight-title">Pre-train</span>ed Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hillary Dawkins, Isar Nejadgholi, Daniel Gillis, Judi McCuaig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigation of gender bias in NLP has a long history tied to debiasing static
word embeddings. More recently, attention has shifted to debiasing pre-trained
language models. We study to what extent the simplest projective debiasing
methods, developed for word embeddings, can help when applied to BERT's
internal representations. Projective methods are fast to implement, use a small
number of saved parameters, and make no updates to the existing model
parameters. We evaluate the efficacy of the methods in reducing both intrinsic
bias, as measured by BERT's next sentence prediction task, and in mitigating
observed bias in a downstream setting when fine-tuned. To this end, we also
provide a critical analysis of a popular gender-bias assessment test for
quantifying intrinsic bias, resulting in an enhanced test set and new bias
measures. We find that projective methods can be effective at both intrinsic
bias and downstream bias mitigation, but that the two outcomes are not
necessarily correlated. This finding serves as a warning that intrinsic bias
test sets, based either on language modeling tasks or next sentence prediction,
should not be the only benchmark in developing a debiased language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-form factuality in large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can achieve superhuman rating
performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a World-English Language Model for On-Device Virtual Assistants <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rricha Jalota, Lyan Verwimp, Markus Nussbaum-Thom, Amr Mousa, Arturo Argueta, Youssef Oualil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are
generally language-, region-, and in some cases, device-dependent, which
increases the effort to scale and maintain them. Combining NNLMs for one or
more of the categories is one way to improve scalability. In this work, we
combine regional variants of English to build a ``World English'' NNLM for
on-device VAs. In particular, we investigate the application of adapter
bottlenecks to model dialect-specific characteristics in our existing
production NNLMs {and enhance the multi-dialect baselines}. We find that
adapter modules are more effective in modeling dialects than specializing
entire sub-networks. Based on this insight and leveraging the design of our
production models, we introduce a new architecture for World English NNLM that
meets the accuracy, latency, and memory constraints of our single-dialect
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CheckEval: Robust Evaluation Framework using Large Language Model via
  Checklist 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukyung Lee, Joonghoon Kim, Jaehee Kim, Hyowon Cho, Pilsung Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CheckEval, a novel evaluation framework using Large Language
Models, addressing the challenges of ambiguity and inconsistency in current
evaluation methods. CheckEval addresses these challenges by dividing evaluation
criteria into detailed sub-aspects and constructing a checklist of Boolean
questions for each, simplifying the evaluation. This approach not only renders
the process more interpretable but also significantly enhances the robustness
and reliability of results by focusing on specific evaluation dimensions.
Validated through a focused case study using the SummEval benchmark, CheckEval
indicates a strong correlation with human judgments. Furthermore, it
demonstrates a highly consistent Inter-Annotator Agreement. These findings
highlight the effectiveness of CheckEval for objective, flexible, and precise
evaluations. By offering a customizable and interactive framework, CheckEval
sets a new standard for the use of LLMs in evaluation, responding to the
evolving needs of the field and establishing a clear method for future
LLM-based evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>HEAL at CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Neural Protoform Reconstruction via Reflex Prediction <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Lu, Jingzhi Wang, David R. Mortensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protolanguage reconstruction is central to historical linguistics. The
comparative method, one of the most influential theoretical and methodological
frameworks in the history of the language sciences, allows linguists to infer
protoforms (reconstructed ancestral words) from their reflexes (related modern
words) based on the assumption of regular sound change. Not surprisingly,
numerous computational linguists have attempted to operationalize comparative
reconstruction through various computational models, the most successful of
which have been supervised encoder-decoder models, which treat the problem of
predicting protoforms given sets of reflexes as a sequence-to-sequence problem.
We argue that this framework ignores one of the most important aspects of the
comparative method: not only should protoforms be inferable from cognate sets
(sets of related reflexes) but the reflexes should also be inferable from the
protoforms. Leveraging another line of research -- reflex prediction -- we
propose a system in which candidate protoforms from a reconstruction model are
reranked by a reflex prediction model. We show that this more complete
implementation of the comparative method allows us to surpass state-of-the-art
protoform reconstruction methods on three of four Chinese and Romance datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CYCLE: Learning to Self-Refine the Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangruibo Ding, Marcus J. Min, Gail Kaiser, Baishakhi Ray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained code language models have achieved promising performance in code
generation and improved the programming efficiency of human developers.
However, their self-refinement capability is typically overlooked by the
existing evaluations of code LMs, which focus only on the accuracy of the
one-time prediction. For the cases when code LMs fail to implement the correct
program, developers actually find it hard to debug and fix the faulty
prediction since it is not written by the developers themselves. Unfortunately,
our study reveals that code LMs cannot efficiently self-refine their faulty
generations as well.
  In this paper, we propose CYCLE framework, learning to self-refine the faulty
generation according to the available feedback, such as the execution results
reported by the test suites. We evaluate CYCLE on three popular code generation
benchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE
successfully maintains, sometimes improves, the quality of one-time code
generation, while significantly improving the self-refinement capability of
code LMs. We implement four variants of CYCLE with varied numbers of parameters
across 350M, 1B, 2B, and 3B, and the experiments show that CYCLE consistently
boosts the code generation performance, by up to 63.5%, across benchmarks and
varied model sizes. We also notice that CYCLE outperforms code LMs that have
3$\times$ more parameters in self-refinement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready for OOPSLA'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large Vision-Language Models with
  Instruction Contrastive Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) are increasingly adept at generating
contextually detailed and coherent responses from visual inputs. However, their
application in multimodal decision-making and open-ended generation is hindered
by a notable rate of hallucinations, where generated text inaccurately
represents the visual contents. To address this issue, this paper introduces
the Instruction Contrastive Decoding (ICD) method, a novel approach designed to
reduce hallucinations during LVLM inference. Our method is inspired by our
observation that what we call disturbance instructions significantly exacerbate
hallucinations in multimodal fusion modules. ICD contrasts distributions from
standard and instruction disturbance, thereby increasing alignment uncertainty
and effectively subtracting hallucinated concepts from the original
distribution. Through comprehensive experiments on discriminative benchmarks
(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that
ICD significantly mitigates both object-level and attribute-level
hallucinations. Moreover, our method not only addresses hallucinations but also
significantly enhances the general perception and recognition capabilities of
LVLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Invalsi Benchmark: measuring Language Models Mathematical and
  Language understanding in Italian 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Esuli, Giovanni Puccetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Italian is by all metrics a high resource language, currently, there
are isn't a Language Model pre-trained exclusively in this language. This
results in a lower number of available benchmarks to evaluate the performance
of language models in Italian.
  This work presents two new benchmarks to evaluate the models performance on
mathematical understanding and language understanding in Italian. These
benchmarks are based on real tests that are undertaken by students of age
between 11 and 18 within the Italian school system and have therefore been
validated by several experts in didactics and pedagogy.
  To validate this dataset we evaluate the performance of 9 language models
that are the best performing when writing in Italian, including our own
fine-tuned models. We show that this is a challenging benchmark where current
language models are bound by 60\% accuracy.
  We believe that the release of this dataset paves the way for improving
future models mathematical and language understanding in Italian.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Laws For Dense Retrieval <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up neural models has yielded significant advancements in a wide array
of tasks, particularly in language generation. Previous studies have found that
the performance of neural models frequently adheres to predictable scaling
laws, correlated with factors such as training set size and model size. This
insight is invaluable, especially as large-scale experiments grow increasingly
resource-intensive. Yet, such scaling law has not been fully explored in dense
retrieval due to the discrete nature of retrieval metrics and complex
relationships between training data and model sizes in retrieval tasks. In this
study, we investigate whether the performance of dense retrieval models follows
the scaling law as other neural models. We propose to use contrastive
log-likelihood as the evaluation metric and conduct extensive experiments with
dense retrieval models implemented with different numbers of parameters and
trained with different amounts of annotated data. Results indicate that, under
our settings, the performance of dense retrieval models follows a precise
power-law scaling related to the model size and the number of annotations.
Additionally, we examine scaling with prevalent data augmentation methods to
assess the impact of annotation quality, and apply the scaling law to find the
best resource allocation strategy under a budget constraint. We believe that
these insights will significantly contribute to understanding the scaling
effect of dense retrieval models and offer meaningful guidance for future
research endeavors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NL-ITI: Optimizing Probing and Intervention for Improvement of ITI
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Hoscilowicz, Adam Wiacek, Jan Chojnacki, Adam Cieslak, Leszek Michon, Vitalii Urbanevych, Artur Janicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM) are prone to returning false information. It
constitutes one of major challenges in the AI field. In our work, we explore
paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it
identifies attention heads, which contain the highest amount of desired type of
knowledge (e.g., truthful). Afterwards, during inference, LLM activations are
shifted for chosen subset of attention heads. We further improved the ITI
framework by introducing a nonlinear probing and multi-token intervention -
Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice
benchmarks, including TruthfulQA, on which we report around 14% MC1 metric
improvement with respect to the baseline ITI results. NL-ITI achieves also
encouraging results on other testsets - on Business Ethics subdomain of MMLU,
around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI
performs better while being less invasive in the behavior of LLM at the same
time (as measured by Kullback-Leibler divergence).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/Samsung/NL-ITI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact Checking Beyond Training Set <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Payam Karisani, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the veracity of everyday claims is time consuming and in some
cases requires domain expertise. We empirically demonstrate that the commonly
used fact checking pipeline, known as the retriever-reader, suffers from
performance deterioration when it is trained on the labeled data from one
domain and used in another domain. Afterwards, we delve into each component of
the pipeline and propose novel algorithms to address this problem. We propose
an adversarial algorithm to make the retriever component robust against
distribution shift. Our core idea is to initially train a bi-encoder on the
labeled source data, and then, to adversarially train two separate document and
claim encoders using unlabeled target data. We then focus on the reader
component and propose to train it such that it is insensitive towards the order
of claims and evidence documents. Our empirical evaluations support the
hypothesis that such a reader shows a higher robustness against distribution
shift. To our knowledge, there is no publicly available multi-topic fact
checking dataset. Thus, we propose a simple automatic method to re-purpose two
well-known fact checking datasets. We then construct eight fact checking
scenarios from these datasets, and compare our model to a set of strong
baseline models, including recent domain adaptation models that use GPT4 for
generating synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Content Recommendation: Knowledge Graph-Based Semantic
  Contrastive Learning for Diversity and Cold-Start Users <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejin Kim, Scott Rome, Kevin Foley, Mayur Nankani, Rimon Melamed, Javier Morales, Abhay Yadav, Maria Peifer, Sardar Hamidian, H. Howie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the challenges related to data sparsity, cold-start problems, and
diversity in recommendation systems is both crucial and demanding. Many current
solutions leverage knowledge graphs to tackle these issues by combining both
item-based and user-item collaborative signals. A common trend in these
approaches focuses on improving ranking performance at the cost of escalating
model complexity, reducing diversity, and complicating the task. It is
essential to provide recommendations that are both personalized and diverse,
rather than solely relying on achieving high rank-based performance, such as
Click-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task
learning approach, training on user-item and item-item interactions. We apply
item-based contrastive learning on descriptive text, sampling positive and
negative pairs based on item metadata. Our approach allows the model to better
understand the relationships between entities within the knowledge graph by
utilizing semantic information from text. It leads to more accurate, relevant,
and diverse user recommendations and a benefit that extends even to cold-start
users who have few interactions with items. We perform extensive experiments on
two widely used datasets to validate the effectiveness of our approach. Our
findings demonstrate that jointly training user-item interactions and
item-based signals using synopsis text is highly effective. Furthermore, our
results provide evidence that item-based contrastive learning enhances the
quality of entity embeddings, as indicated by metrics such as uniformity and
alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDSAT: Accelerating LLM Inference through Speculative Decoding with
  Semantic Adaptive Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengbo Liu, Yong Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an acceleration scheme for large language models (LLMs) through
Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary
objective of this design is to enhance the LLM model's ability to generate
draft tokens more accurately without compromising the model's accuracy. The
core strategies involve: 1) Fine-tune the model by incorporating semantic
adaptive tokens that possess flexible decoding capabilities without changing
its structure, allowing them to generate high-quality draft tokens. 2) By
employing a training method that does not affect the standard tokens, the model
can acquire parallel decoding abilities atop its original framework with
minimal training overhead. 3) We have designed the "two-step-draft-then-verify"
generation strategies using both greedy search and nucleus sampling.
Experiments conducted on the CodeLlama-13B and 7B models have yielded speed
increases of over 3.5X and 3.0X, respectively. Please refer to
https://github.com/hasuoshenyun/SDSAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vulnerability Detection with Code Language Models: How Far Are We? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangruibo Ding, Yanjun Fu, Omniyyah Ibrahim, Chawin Sitawarin, Xinyun Chen, Basel Alomair, David Wagner, Baishakhi Ray, Yizheng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of the rising interest in code language models (code LMs) and
vulnerability detection, we study the effectiveness of code LMs for detecting
vulnerabilities. Our analysis reveals significant shortcomings in existing
vulnerability datasets, including poor data quality, low label accuracy, and
high duplication rates, leading to unreliable model performance in realistic
vulnerability detection scenarios. Additionally, the evaluation methods used
with these datasets are not representative of real-world vulnerability
detection.
  To address these challenges, we introduce PrimeVul, a new dataset for
training and evaluating code LMs for vulnerability detection. PrimeVul
incorporates a novel set of data labeling techniques that achieve comparable
label accuracy to human-verified benchmarks while significantly expanding the
dataset. It also implements a rigorous data de-duplication and chronological
data splitting strategy to mitigate data leakage issues, alongside introducing
more realistic evaluation metrics and settings. This comprehensive approach
aims to provide a more accurate assessment of code LMs' performance in
real-world conditions.
  Evaluating code LMs on PrimeVul reveals that existing benchmarks
significantly overestimate the performance of these models. For instance, a
state-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on
PrimeVul. Attempts to improve performance through advanced training techniques
and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin
to random guessing in the most stringent settings. These findings underscore
the considerable gap between current capabilities and the practical
requirements for deploying code LMs in security roles, highlighting the need
for more innovative research in this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">survey</span> on learning models of spiking neural membrane systems and
  spiking neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prithwineel Paul, Petr Sosik, Lucie Ciencialova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks (SNN) are a biologically inspired model of neural
networks with certain brain-like properties. In the past few decades, this
model has received increasing attention in computer science community, owing
also to the successful phenomenon of deep learning. In SNN, communication
between neurons takes place through the spikes and spike trains. This
differentiates these models from the ``standard'' artificial neural networks
(ANN) where the frequency of spikes is replaced by real-valued signals. Spiking
neural P systems (SNPS) can be considered a branch of SNN based more on the
principles of formal automata, with many variants developed within the
framework of the membrane computing theory. In this paper, we first briefly
compare structure and function, advantages and drawbacks of SNN and SNPS. A key
part of the article is a survey of recent results and applications of machine
learning and deep learning models of both SNN and SNPS formalisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Debiasing Sentence Embedders through Contrastive Word Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Sarah Schröder, Alexander Schulz, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the last years, various sentence embedders have been an integral part in
the success of current machine learning approaches to Natural Language
Processing (NLP). Unfortunately, multiple sources have shown that the bias,
inherent in the datasets upon which these embedding methods are trained, is
learned by them. A variety of different approaches to remove biases in
embeddings exists in the literature. Most of these approaches are applicable to
word embeddings and in fewer cases to sentence embeddings. It is problematic
that most debiasing approaches are directly transferred from word embeddings,
therefore these approaches fail to take into account the nonlinear nature of
sentence embedders and the embeddings they produce. It has been shown in
literature that bias information is still present if sentence embeddings are
debiased using such methods. In this contribution, we explore an approach to
remove linear and nonlinear bias information for NLP solutions, without
impacting downstream performance. We compare our approach to common debiasing
methods on classical bias metrics and on bias metrics which take nonlinear
information into account.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention-aware semantic relevance predicting Chinese sentence reading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, several influential computational models and metrics have
been proposed to predict how humans comprehend and process sentence. One
particularly promising approach is contextual semantic similarity. Inspired by
the attention algorithm in Transformer and human memory mechanisms, this study
proposes an ``attention-aware'' approach for computing contextual semantic
relevance. This new approach takes into account the different contributions of
contextual parts and the expectation effect, allowing it to incorporate
contextual information fully. The attention-aware approach also facilitates the
simulation of existing reading models and evaluate them. The resulting
``attention-aware'' metrics of semantic relevance can more accurately predict
fixation durations in Chinese reading tasks recorded in an eye-tracking corpus
than those calculated by existing approaches. The study's findings further
provide strong support for the presence of semantic preview benefits in Chinese
naturalistic reading. Furthermore, the attention-aware metrics of semantic
relevance, being memory-based, possess high interpretability from both
linguistic and cognitive standpoints, making them a valuable computational tool
for modeling eye-movements in reading and further gaining insight into the
process of language comprehension. Our approach underscores the potential of
these metrics to advance our comprehension of how humans understand and process
language, ultimately leading to a better understanding of language
comprehension and processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Path Towards Legal Autonomy: An interoperable and explainable approach
  to extracting, transforming, loading and computing legal information using
  large language models, expert systems and Bayesian networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Constant, Hannes Westermann, Bryan Wilson, Alex Kiefer, Ines Hipolito, Sylvain Pronovost, Steven Swanson, Mahault Albarracin, Maxwell J. D. Ramstead
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal autonomy - the lawful activity of artificial intelligence agents - can
be achieved in one of two ways. It can be achieved either by imposing
constraints on AI actors such as developers, deployers and users, and on AI
resources such as data, or by imposing constraints on the range and scope of
the impact that AI agents can have on the environment. The latter approach
involves encoding extant rules concerning AI driven devices into the software
of AI agents controlling those devices (e.g., encoding rules about limitations
on zones of operations into the agent software of an autonomous drone device).
This is a challenge since the effectivity of such an approach requires a method
of extracting, loading, transforming and computing legal information that would
be both explainable and legally interoperable, and that would enable AI agents
to reason about the law. In this paper, we sketch a proof of principle for such
a method using large language models (LLMs), expert legal systems known as
legal decision paths, and Bayesian networks. We then show how the proposed
method could be applied to extant regulation in matters of autonomous cars,
such as the California Vehicle Code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Plays a Pivotal Role in the Object-Attribute Compositional
  Generalization of CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Abbasi, Mohammad Samiei, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models, such as CLIP, have shown promising
Out-of-Distribution (OoD) generalization under various types of distribution
shifts. Recent studies attempted to investigate the leading cause of this
capability. In this work, we follow the same path, but focus on a specific type
of OoD data - images with novel compositions of attribute-object pairs - and
study whether such models can successfully classify those images into
composition classes. We carefully designed an authentic image test dataset
called ImageNet-AO, consisting of attributes for objects that are unlikely
encountered in the CLIP training sets. We found that CLIPs trained with large
datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude
improvement in effective compositional OoD generalization compared to both
supervised models and CLIPs trained with smaller datasets, such as CC-12M and
YFCC-15M. Our results provide evidence that the scale and diversity of training
data and language supervision play a key role in unlocking the compositional
generalization abilities of vision-language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral accepted at OODCV 2023(http://www.ood-cv.org)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AcTED: Automatic Acquisition of Typical Event Duration for
  Semi-supervised Temporal Commonsense QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Virgo, Fei Cheng, Lis Kanashiro Pereira, Masayuki Asahara, Ichiro Kobayashi, Sadao Kurohashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a voting-driven semi-supervised approach to automatically acquire
the typical duration of an event and use it as pseudo-labeled data. The human
evaluation demonstrates that our pseudo labels exhibit surprisingly high
accuracy and balanced coverage. In the temporal commonsense QA task,
experimental results show that using only pseudo examples of 400 events, we
achieve performance comparable to the existing BERT-based weakly supervised
approaches that require a significant amount of training examples. When
compared to the RoBERTa baselines, our best approach establishes
state-of-the-art performance with a 7% improvement in Exact Match.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Beat Numerical Regression? Language-Based Multimodal
  Trajectory Prediction <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Junoh Lee, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have demonstrated impressive ability in context understanding
and generative performance. Inspired by the recent success of language
foundation models, in this paper, we propose LMTraj (Language-based Multimodal
Trajectory predictor), which recasts the trajectory prediction task into a sort
of question-answering problem. Departing from traditional numerical regression
models, which treat the trajectory coordinate sequence as continuous signals,
we consider them as discrete signals like text prompts. Specially, we first
transform an input space for the trajectory coordinate into the natural
language space. Here, the entire time-series trajectories of pedestrians are
converted into a text prompt, and scene images are described as text
information through image captioning. The transformed numerical and image data
are then wrapped into the question-answering template for use in a language
model. Next, to guide the language model in understanding and reasoning
high-level knowledge, such as scene context and social relationships between
pedestrians, we introduce an auxiliary multi-task question and answering. We
then train a numerical tokenizer with the prompt data. We encourage the
tokenizer to separate the integer and decimal parts well, and leverage it to
capture correlations between the consecutive numbers in the language model.
Lastly, we train the language model using the numerical tokenizer and all of
the question-answer prompts. Here, we propose a beam-search-based most-likely
prediction and a temperature-based multimodal prediction to implement both
deterministic and stochastic inferences. Applying our LMTraj, we show that the
language-based model can be a powerful pedestrian trajectory predictor, and
outperforms existing numerical-based predictor methods. Code is publicly
available at https://github.com/inhwanbae/LMTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DELTA: <span class="highlight-title">Pre-train</span> a Discriminative Encoder for Legal Case Retrieval via
  Structural Word Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Li, Qingyao Ai, Xinyan Han, Jia Chen, Qian Dong, Yiqun Liu, Chong Chen, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research demonstrates the effectiveness of using pre-trained language
models for legal case retrieval. Most of the existing works focus on improving
the representation ability for the contextualized embedding of the [CLS] token
and calculate relevance using textual semantic similarity. However, in the
legal domain, textual semantic similarity does not always imply that the cases
are relevant enough. Instead, relevance in legal cases primarily depends on the
similarity of key facts that impact the final judgment. Without proper
treatments, the discriminative ability of learned representations could be
limited since legal cases are lengthy and contain numerous non-key facts. To
this end, we introduce DELTA, a discriminative model designed for legal case
retrieval. The basic idea involves pinpointing key facts in legal cases and
pulling the contextualized embedding of the [CLS] token closer to the key facts
while pushing away from the non-key facts, which can warm up the case embedding
space in an unsupervised manner. To be specific, this study brings the word
alignment mechanism to the contextual masked auto-encoder. First, we leverage
shallow decoders to create information bottlenecks, aiming to enhance the
representation ability. Second, we employ the deep decoder to enable
translation between different structures, with the goal of pinpointing key
facts to enhance discriminative ability. Comprehensive experiments conducted on
publicly available legal benchmarks show that our approach can outperform
existing state-of-the-art methods in legal case retrieval. It provides a new
perspective on the in-depth understanding and processing of legal case
documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring language relations through syntactic distances and geographic
  proximity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan De Gregorio, Raúl Toral, David Sánchez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Languages are grouped into families that share common linguistic traits.
While this approach has been successful in understanding genetic relations
between diverse languages, more analyses are needed to accurately quantify
their relatedness, especially in less studied linguistic levels such as syntax.
Here, we explore linguistic distances using series of parts of speech (POS)
extracted from the Universal Dependencies dataset. Within an
information-theoretic framework, we show that employing POS trigrams maximizes
the possibility of capturing syntactic variations while being at the same time
compatible with the amount of available data. Linguistic connections are then
established by assessing pairwise distances based on the POS distributions.
Intriguingly, our analysis reveals definite clusters that correspond to well
known language families and groups, with exceptions explained by distinct
morphological typologies. Furthermore, we obtain a significant correlation
between language similarity and geographic distance, which underscores the
influence of spatial proximity on language kinships.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriviaHG: A <span class="highlight-title">Dataset</span> for Automatic Hint Generation from Factoid Questions <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jamshid Mozafari, Anubhav Jangra, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, individuals tend to engage in dialogues with Large Language Models,
seeking answers to their questions. In times when such answers are readily
accessible to anyone, the stimulation and preservation of human's cognitive
abilities, as well as the assurance of maintaining good reasoning skills by
humans becomes crucial. This study addresses such needs by proposing hints
(instead of final answers or before giving answers) as a viable solution. We
introduce a framework for the automatic hint generation for factoid questions,
employing it to construct TriviaHG, a novel large-scale dataset featuring
160,230 hints corresponding to 16,645 questions from the TriviaQA dataset.
Additionally, we present an automatic evaluation method that measures the
Convergence and Familiarity quality attributes of hints. To evaluate the
TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals
to annotate 2,791 hints and tasked 6 humans with answering questions using the
provided hints. The effectiveness of hints varied, with success rates of 96%,
78%, and 36% for questions with easy, medium, and hard answers, respectively.
Moreover, the proposed automatic evaluation methods showed a robust correlation
with annotators' results. Conclusively, the findings highlight three key
insights: the facilitative role of hints in resolving unknown questions, the
dependence of hint quality on answer difficulty, and the feasibility of
employing automatic evaluation methods for hint assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SemRoDe: Macro Adversarial Training to Learn Representations That are
  Robust to Word-Level Attacks <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Formento, Wenjie Feng, Chuan Sheng Foo, Luu Anh Tuan, See-Kiong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are indispensable tools for natural language processing
tasks, but their vulnerability to adversarial attacks remains a concern. While
current research has explored adversarial training techniques, their
improvements to defend against word-level attacks have been limited. In this
work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a
Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing
inspiration from recent studies in the image domain, we investigate and later
confirm that in a discrete data setting such as language, adversarial samples
generated via word substitutions do indeed belong to an adversarial domain
exhibiting a high Wasserstein distance from the base domain. Our method learns
a robust representation that bridges these two domains. We hypothesize that if
samples were not projected into an adversarial domain, but instead to a domain
with minimal shift, it would improve attack robustness. We align the domains by
incorporating a new distance-based objective. With this, our model is able to
learn more generalized representations by aligning the model's high-level
output features and therefore better handling unseen adversarial samples. This
method can be generalized across word embeddings, even when they share minimal
overlap at both vocabulary and word-substitution levels. To evaluate the
effectiveness of our approach, we conduct experiments on BERT and RoBERTa
models on three datasets. The results demonstrate promising state-of-the-art
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NAACL 2024 (Main Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, Christopher D. Manning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance
on a wide variety of biomedical NLP tasks. However, these models have hundreds
of billions of parameters, are computationally expensive to run, require users
to send their input data over the internet, and are trained on unknown data
sources. Can smaller, more targeted models compete? To address this question,
we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive
model trained exclusively on PubMed abstracts and full articles. When
fine-tuned, BioMedLM can produce strong multiple-choice biomedical
question-answering results competitive with much larger models, such as
achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical
Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to
patient questions on medical topics. This demonstrates that smaller models can
potentially serve as transparent, privacy-preserving, economical and
environmentally friendly foundations for particular NLP applications, such as
in biomedicine. The model is available on the Hugging Face Hub:
https://huggingface.co/stanford-crfm/BioMedLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering
  Using a VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stimulated by the sophisticated reasoning capabilities of recent Large
Language Models (LLMs), a variety of strategies for bridging video modality
have been devised. A prominent strategy involves Video Language Models
(VideoLMs), which train a learnable interface with video data to connect
advanced vision encoders with LLMs. Recently, an alternative strategy has
surfaced, employing readily available foundation models, such as VideoLMs and
LLMs, across multiple stages for modality bridging. In this study, we introduce
a simple yet novel strategy where only a single Vision Language Model (VLM) is
utilized. Our starting point is the plain insight that a video comprises a
series of images, or frames, interwoven with temporal information. The essence
of video comprehension lies in adeptly managing the temporal aspects along with
the spatial details of each frame. Initially, we transform a video into a
single composite image by arranging multiple frames in a grid layout. The
resulting single image is termed as an image grid. This format, while
maintaining the appearance of a solitary image, effectively retains temporal
information within the grid structure. Therefore, the image grid approach
enables direct application of a single high-performance VLM without
necessitating any video-data training. Our extensive experimental analysis
across ten zero-shot video question answering benchmarks, including five
open-ended and five multiple-choice benchmarks, reveals that the proposed Image
Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out
of ten benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/imagegridworth/IG-VLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Attributed Text Generation of Large Language Models via
  Preference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongfang Li, Zetian Sun, Baotian Hu, Zhenyu Liu, Xinshuo Hu, Xuebo Liu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have been widely adopted in natural language
processing, yet they face the challenge of generating unreliable content.
Recent works aim to reduce misinformation and hallucinations by resorting to
attribution as a means to provide evidence (i.e., citations). However, current
attribution methods usually focus on the retrieval stage and automatic
evaluation that neglect mirroring the citation mechanisms in human scholarly
writing to bolster credibility. In this paper, we address these challenges by
modelling the attribution task as preference learning and introducing an
Automatic Preference Optimization (APO) framework. First, we create a curated
collection for post-training with 6,330 examples by collecting and filtering
from existing datasets. Second, considering the high cost of labelling
preference data, we further propose an automatic method to synthesize
attribution preference data resulting in 95,263 pairs. Moreover, inspired by
the human citation process, we further propose a progressive preference
optimization method by leveraging fine-grained information. Extensive
experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate
that APO achieves state-of-the-art citation F1 with higher answer quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 15 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BLADE: Enhancing Black-box Large Language Models with Small
  Domain-Specific Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, Yiqun Liu, Chong Chen, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable
of addressing a diverse range of tasks. However, general LLMs, which are
developed on open-domain data, may lack the domain-specific knowledge essential
for tasks in vertical domains, such as legal, medical, etc. To address this
issue, previous approaches either conduct continuous pre-training with
domain-specific data or employ retrieval augmentation to support general LLMs.
Unfortunately, these strategies are either cost-intensive or unreliable in
practical applications. To this end, we present a novel framework named BLADE,
which enhances Black-box LArge language models with small Domain-spEcific
models. BLADE consists of a black-box LLM and a small domain-specific LM. The
small LM preserves domain-specific knowledge and offers specialized insights,
while the general LLM contributes robust language comprehension and reasoning
capabilities. Specifically, our method involves three steps: 1) pre-training
the small LM with domain-specific data, 2) fine-tuning this model using
knowledge instruction data, and 3) joint Bayesian optimization of the general
LLM and the small LM. Extensive experiments conducted on public legal and
medical benchmarks reveal that BLADE significantly outperforms existing
approaches. This shows the potential of BLADE as an effective and
cost-efficient solution in adapting general LLMs for vertical domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Semantic Search and its Role in
  Retrieved-Augmented-Generation (RAG) for Arabic Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Mahboub, Muhy Eddin Za'ter, Bashar Alfrou, Yazan Estaitia, Adnan Jaljuli, Asma Hakouz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The latest advancements in machine learning and deep learning have brought
forth the concept of semantic similarity, which has proven immensely beneficial
in multiple applications and has largely replaced keyword search. However,
evaluating semantic similarity and conducting searches for a specific query
across various documents continue to be a complicated task. This complexity is
due to the multifaceted nature of the task, the lack of standard benchmarks,
whereas these challenges are further amplified for Arabic language. This paper
endeavors to establish a straightforward yet potent benchmark for semantic
search in Arabic. Moreover, to precisely evaluate the effectiveness of these
metrics and the dataset, we conduct our assessment of semantic search within
the framework of retrieval augmented generation (RAG).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rejection Improves Reliability: Training LLMs to Refuse Unknown
  Questions Using RL from Knowledge Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongshen Xu, Zichen Zhu, Da Ma, Situo Zhang, Shuai Fan, Lu Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) often generate erroneous outputs, known as
hallucinations, due to their limitations in discerning questions beyond their
knowledge scope. While addressing hallucination has been a focal point in
research, previous efforts primarily concentrate on enhancing correctness
without giving due consideration to the significance of rejection mechanisms.
In this paper, we conduct a comprehensive examination of the role of rejection,
introducing the notion of model reliability along with corresponding metrics.
These metrics measure the model's ability to provide accurate responses while
adeptly rejecting questions exceeding its knowledge boundaries, thereby
minimizing hallucinations. To improve the inherent reliability of LLMs, we
present a novel alignment framework called Reinforcement Learning from
Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically
determine the model's knowledge boundary and trains a reliable reward model to
encourage the refusal of out-of-knowledge questions. Experimental results on
mathematical questions affirm the substantial efficacy of RLKF in significantly
enhancing LLM reliability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying and Mitigating Unimodal Biases in Multimodal Large Language
  Models: A Causal Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meiqi Chen, Yixin Cao, Yan Zhang, Chaochao Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have facilitated the
development of Multimodal LLMs (MLLMs). Despite their impressive capabilities,
MLLMs often suffer from an over-reliance on unimodal biases (e.g., language
bias and vision bias), leading to incorrect answers in complex multimodal
tasks. To investigate this issue, we propose a causal framework to interpret
the biases in Visual Question Answering (VQA) problems. Within our framework,
we devise a causal graph to elucidate the predictions of MLLMs on VQA problems,
and assess the causal effect of biases through an in-depth causal analysis.
Motivated by the causal graph, we introduce a novel MORE dataset, consisting of
12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities,
necessitating multi-hop reasoning and the surmounting of unimodal biases.
Furthermore, we propose two strategies to mitigate unimodal biases and enhance
MLLMs' reasoning capabilities, including a Decompose-Verify-Answer (DeVA)
framework for limited-access MLLMs and the refinement of open-source MLLMs
through fine-tuning. Extensive quantitative and qualitative experiments offer
valuable insights for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IterAlign: Iterative Constitutional Alignment of Large Language Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiusi Chen, Hongzhi Wen, Sreyashi Nag, Chen Luo, Qingyu Yin, Ruirui Li, Zheng Li, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of large language models (LLMs), aligning LLMs
with human values and societal norms to ensure their reliability and safety has
become crucial. Reinforcement learning with human feedback (RLHF) and
Constitutional AI (CAI) have been proposed for LLM alignment. However, these
methods require either heavy human annotations or explicitly pre-defined
constitutions, which are labor-intensive and resource-consuming. To overcome
these drawbacks, we study constitution-based LLM alignment and propose a
data-driven constitution discovery and self-alignment framework called
IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM
and automatically discovers new constitutions using a stronger LLM. These
constitutions are then used to guide self-correction of the base LLM. Such a
constitution discovery pipeline can be run iteratively and automatically to
discover new constitutions that specifically target the alignment gaps in the
current LLM. Empirical results on several safety benchmark datasets and
multiple base LLMs show that IterAlign successfully improves truthfulness,
helpfulness, harmlessness and honesty, improving the LLM alignment by up to
$13.5\%$ in harmlessness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Dataset</span> for Pharmacovigilance in German, French, and Japanese:
  Annotating Adverse Drug Reactions across Languages <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Raithel, Hui-Syuan Yeh, Shuntaro Yada, Cyril Grouin, Thomas Lavergne, Aurélie Névéol, Patrick Paroubek, Philippe Thomas, Tomohiro Nishiyama, Sebastian Möller, Eiji Aramaki, Yuji Matsumoto, Roland Roller, Pierre Zweigenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User-generated data sources have gained significance in uncovering Adverse
Drug Reactions (ADRs), with an increasing number of discussions occurring in
the digital world. However, the existing clinical corpora predominantly revolve
around scientific articles in English. This work presents a multilingual corpus
of texts concerning ADRs gathered from diverse sources, including patient fora,
social media, and clinical reports in German, French, and Japanese. Our corpus
contains annotations covering 12 entity types, four attribute types, and 13
relation types. It contributes to the development of real-world multilingual
language models for healthcare. We provide statistics to highlight certain
challenges associated with the corpus and conduct preliminary experiments
resulting in strong baselines for extracting entities and relations between
these entities, both within and across languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLMs Converse Formally? Automatically Assessing LLMs in Translating
  and Interpreting Formal Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rushang Karia, Daksh Dobhal, Daniel Bramblett, Pulkit Verma, Siddharth Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stakeholders often describe system requirements using natural language which
are then converted to formal syntax by a domain-expert leading to increased
design costs. This paper assesses the capabilities of Large Language Models
(LLMs) in converting between natural language descriptions and formal
specifications. Existing work has evaluated the capabilities of LLMs in
generating formal syntax such as source code but such experiments are typically
hand-crafted and use problems that are likely to be in the training set of
LLMs, and often require human-annotated datasets. We propose an approach that
can use two copies of an LLM in conjunction with an off-the-shelf verifier to
automatically evaluate its translation abilities without any additional human
input. Our approach generates formal syntax using language grammars to
automatically generate a dataset. We conduct an empirical evaluation to measure
the accuracy of this translation task and show that SOTA LLMs cannot adequately
solve this task, limiting their current utility in the design of complex
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chinese Offensive Language Detection:Current Status and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunze Xiao, Houda Bouamor, Wajdi Zaghouani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the considerable efforts being made to monitor and regulate
user-generated content on social media platforms, the pervasiveness of
offensive language, such as hate speech or cyberbullying, in the digital space
remains a significant challenge. Given the importance of maintaining a
civilized and respectful online environment, there is an urgent and growing
need for automatic systems capable of detecting offensive speech in real time.
However, developing effective systems for processing languages such as Chinese
presents a significant challenge, owing to the language's complex and nuanced
nature, which makes it difficult to process automatically. This paper provides
a comprehensive overview of offensive language detection in Chinese, examining
current benchmarks and approaches and highlighting specific models and tools
for addressing the unique challenges of detecting offensive language in this
complex language. The primary objective of this survey is to explore the
existing techniques and identify potential avenues for further research that
can address the cultural and linguistic complexities of Chinese.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Instruction Tuning with Large Language Models for Mathematical
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongwei Zhou, Tiejun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements highlight the success of instruction tuning with large
language models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical
reasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as
incorrect, missing, and redundant steps in CoT generation leading to
inaccuracies in answer predictions. To alleviate this problem, we propose a
dual instruction tuning strategy to meticulously model mathematical reasoning
from both forward and reverse directions. This involves introducing the
Intermediate Reasoning State Prediction task (forward reasoning) and the
Instruction Reconstruction task (reverse reasoning) to enhance the LLMs'
understanding and execution of instructions. Training instances for these tasks
are constructed based on existing mathematical instruction tuning datasets.
Subsequently, LLMs undergo multi-task fine-tuning using both existing
mathematical instructions and the newly created data. Comprehensive experiments
validate the effectiveness and domain generalization of the dual instruction
tuning strategy across various mathematical reasoning tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Recalibration of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has uncovered promising ways to extract well-calibrated
confidence estimates from language models (LMs), where the model's confidence
score reflects how likely it is to be correct. However, while LMs may appear
well-calibrated over broad distributions, this often hides significant
miscalibration within narrower slices (e.g., systemic over-confidence in math
can balance out systemic under-confidence in history, yielding perfect
calibration in aggregate). To attain well-calibrated confidence estimates for
any slice of a distribution, we propose a new framework for few-shot
slice-specific recalibration. Specifically, we train a recalibration model that
takes in a few unlabeled examples from any given slice and predicts a curve
that remaps confidence scores to be more accurate for that slice. Our trained
model can recalibrate for arbitrary new slices, without using any labeled data
from that slice. This enables us to identify domain-specific confidence
thresholds above which the LM's predictions can be trusted, and below which it
should abstain. Experiments show that our few-shot recalibrator consistently
outperforms existing calibration methods, for instance improving calibration
error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BlendX: Complex Multi-Intent Detection with Blended Patterns <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yejin Yoon, Jungyeon Lee, Kangsan Kim, Chanhee Park, Taeuk Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented dialogue (TOD) systems are commonly designed with the
presumption that each utterance represents a single intent. However, this
assumption may not accurately reflect real-world situations, where users
frequently express multiple intents within a single utterance. While there is
an emerging interest in multi-intent detection (MID), existing in-domain
datasets such as MixATIS and MixSNIPS have limitations in their formulation. To
address these issues, we present BlendX, a suite of refined datasets featuring
more diverse patterns than their predecessors, elevating both its complexity
and diversity. For dataset construction, we utilize both rule-based heuristics
as well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a
similarity-driven strategy for utterance selection. To ensure the quality of
the proposed datasets, we also introduce three novel metrics that assess the
statistical properties of an utterance related to word count, conjunction use,
and pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art
MID models struggle with the challenges posed by the new datasets, highlighting
the need to reexamine the current state of the MID field. The dataset is
available at https://github.com/HYU-NLP/BlendX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era
  of <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer structure has achieved great success in multiple applied machine
learning communities, such as natural language processing (NLP), computer
vision (CV) and information retrieval (IR). Transformer architecture's core
mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$
time complexity in inference. Many works have been proposed to improve the
attention mechanism's scalability, such as Flash Attention and Multi-query
Attention. A different line of work aims to design new mechanisms to replace
attention. Recently, a notable model structure -- Mamba, which is based on
state space models, has achieved transformer-equivalent performance in multiple
sequence modeling tasks.
  In this work, we examine \mamba's efficacy through the lens of a classical IR
task -- document ranking. A reranker model takes a query and a document as
input, and predicts a scalar relevance score. This task demands the language
model's ability to comprehend lengthy contextual inputs and to capture the
interaction between query and document tokens. We find that (1) Mamba models
achieve competitive performance compared to transformer-based models with the
same training recipe; (2) but also have a lower training throughput in
comparison to efficient transformer implementations such as flash attention. We
hope this study can serve as a starting point to explore Mamba models in other
classical IR tasks. Our code implementation and trained checkpoints are made
public to facilitate
reproducibility.\footnote{https://github.com/zhichaoxu-shufe/RankMamba}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Interactive Regional Understanding in Vision-Large Language
  Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungbeom Lee, Sanghyuk Chun, Sangdoo Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Vision-Language Pre-training (VLP) models have demonstrated
significant advancements. Nevertheless, these models heavily rely on image-text
pairs that capture only coarse and global information of an image, leading to a
limitation in their regional understanding ability. In this work, we introduce
\textbf{RegionVLM}, equipped with explicit regional modeling capabilities,
allowing them to understand user-indicated image regions. To achieve this, we
design a simple yet innovative architecture, requiring no modifications to the
model architecture or objective function. Additionally, we leverage a dataset
that contains a novel source of information, namely Localized Narratives, which
has been overlooked in previous VLP research. Our experiments demonstrate that
our single generalist model not only achieves an interactive dialogue system
but also exhibits superior performance on various zero-shot region
understanding tasks, without compromising its ability for global image
understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MD-PK: Metaphor Detection via <span class="highlight-title">Prompt</span> Learning and Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaidi Jia, Rongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaphors are ubiquitous in daily life, yet detecting them poses a
significant challenge. Previous approaches often struggled with improper
application of language rules and overlooked the issue of data sparsity. To
address these challenges, we introduce knowledge distillation and prompt
learning into metaphor detection. Specifically, we devise a prompt learning
template tailored for the metaphor detection task. By masking target words and
providing relevant prompt information, we guide the model to accurately infer
the contextual meaning of these words. This approach not only mitigates the
interference from the literal meaning of target words but also ensures the
proper utilization of MIP language rules for metaphor detection. Moreover, we
employ a teacher model equipped with prior knowledge to generate meaningful
soft labels, guiding the optimization process of the student model. The
inclusion of soft labels, akin to label smoothing, helps alleviate the model's
tendency towards over-confidence and effectively addresses the challenge of
data sparsity. Experimental results demonstrate that our proposed model
achieves state-of-the-art performance across multiple datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning has been a cornerstone in computer vision,
evolving from supervised learning with human-annotated labels to aligning
image-text pairs from the Internet. Despite recent advancements in multi-modal
large language models (MLLMs), the visual representations they rely on, such as
CLIP embeddings, often lack access to external world knowledge critical for
real-world visual reasoning. In this work, we propose Visual Table, a novel
visual representation tailored for MLLMs. It provides hierarchical text
descriptions of holistic visual scenes, consisting of a scene description and
multiple object-centric descriptions that encompass categories, attributes, and
knowledge at instance level. We further develop a scalable generator for visual
table generation and train it on small-scale annotations from GPT4V. Extensive
evaluations demonstrate that, with generated visual tables as additional visual
representations, our model can consistently outperform the state-of-the-art
(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone
visual representations, our model can closely match or even beat the SOTA MLLMs
that are built on CLIP visual embeddings. Our code is available at
https://github.com/LaVi-Lab/Visual-Table.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/LaVi-Lab/Visual-Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Since the Scientific Literature Is Multilingual, Our Models Should Be
  Too 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abteen Ebrahimi, Kenneth Church
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  English has long been assumed the $\textit{lingua franca}$ of scientific
research, and this notion is reflected in the natural language processing (NLP)
research involving scientific document representation. In this position piece,
we quantitatively show that the literature is largely multilingual and argue
that current models and benchmarks should reflect this linguistic diversity. We
provide evidence that text-based models fail to create meaningful
representations for non-English papers and highlight the negative user-facing
impacts of using English-only models non-discriminately across a multilingual
domain. We end with suggestions for the NLP community on how to improve
performance on non-English documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Deceptive Power of LLM-Generated Fake News: A Study of
  Real-World Detection Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanshen Sun, Jianfeng He, Limeng Cui, Shuo Lei, Chang-Tien Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have enabled the creation
of fake news, particularly in complex fields like healthcare. Studies highlight
the gap in the deceptive power of LLM-generated fake news with and without
human assistance, yet the potential of prompting techniques has not been fully
explored. Thus, this work aims to determine whether prompting strategies can
effectively narrow this gap. Current LLM-based fake news attacks require human
intervention for information gathering and often miss details and fail to
maintain context consistency. Therefore, to better understand threat tactics,
we propose a strong fake news attack method called conditional
Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods,
VLPrompt eliminates the need for additional data collection while maintaining
contextual coherence and preserving the intricacies of the original text. To
propel future research on detecting VLPrompt attacks, we created a new dataset
named VLPrompt fake news (VLPFN) containing real and fake texts. Our
experiments, including various detection methods and novel human study metrics,
were conducted to assess their performance on our dataset, yielding numerous
findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech
  Corpus <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Injy Hamed, Fadhl Eryani, David Palfreyman, Nizar Habash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ZAEBUC-Spoken, a multilingual multidialectal Arabic-English speech
corpus. The corpus comprises twelve hours of Zoom meetings involving multiple
speakers role-playing a work situation where Students brainstorm ideas for a
certain topic and then discuss it with an Interlocutor. The meetings cover
different topics and are divided into phases with different language setups.
The corpus presents a challenging set for automatic speech recognition (ASR),
including two languages (Arabic and English) with Arabic spoken in multiple
variants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic) and English
used with various accents. Adding to the complexity of the corpus, there is
also code-switching between these languages and dialects. As part of our work,
we take inspiration from established sets of transcription guidelines to
present a set of guidelines handling issues of conversational speech,
code-switching and orthography of both languages. We further enrich the corpus
with two layers of annotations; (1) dialectness level annotation for the
portion of the corpus where mixing occurs between different variants of Arabic,
and (2) automatic morphological annotations, including tokenization,
lemmatization, and part-of-speech tagging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanisms of non-factual hallucinations in language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yu, Meng Cao, Jackie Chi Kit Cheung, Yue Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art language models (LMs) sometimes generate non-factual
hallucinations that misalign with world knowledge. Despite extensive efforts to
detect and mitigate hallucinations, understanding their internal mechanisms
remains elusive. Our study investigates the mechanistic causes of
hallucination, specifically non-factual ones where the LM incorrectly predicts
object attributes in response to subject-relation queries. With causal
mediation analysis and embedding space projection, we identify two general
mechanistic causes of hallucinations shared across LMs of various scales and
designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and
2) failing to select the correct object attribute in upper layer attention
heads and MLPs. These two mechanisms exhibit varying degrees of subject-object
association, predictive uncertainty and perturbation robustness. Additionally,
we scrutinize LM pre-training checkpoints, revealing distinct learning dynamics
for the two mechanistic causes of hallucinations. We also highlight how
attribution features from our causal analysis can effectively construct
hallucination detectors. Our work proposes a mechanistic understanding of LM
factual errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent-Pro: Learning to Evolve via Policy-Level Reflection and
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models exhibit robust problem-solving capabilities for diverse
tasks. However, most LLM-based agents are designed as specific task solvers
with sophisticated prompt engineering, rather than agents capable of learning
and evolving through interactions. These task solvers necessitate manually
crafted prompts to inform task rules and regulate LLM behaviors, inherently
incapacitating to address complex dynamic scenarios e.g., large interactive
games. In light of this, we propose Agent-Pro: an LLM-based Agent with
Policy-level Reflection and Optimization that can learn a wealth of expertise
from interactive experiences and progressively elevate its behavioral policy.
Specifically, it involves a dynamic belief generation and reflection process
for policy evolution. Rather than action-level reflection, Agent-Pro
iteratively reflects on past trajectories and beliefs, fine-tuning its
irrational beliefs for a better policy. Moreover, a depth-first search is
employed for policy optimization, ensuring continual enhancement in policy
payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,
outperforming vanilla LLM and specialized models. Our results show Agent-Pro
can learn and evolve in complex and dynamic scenes, which also benefits
numerous LLM-based applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLM-based Agent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Contrast: Better Reflection Through Inconsistent Solving
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reflection capacity of Large Language Model (LLM) has garnered extensive
attention. A post-hoc prompting strategy, e.g., reflexion and self-refine,
refines LLM's response based on self-evaluated or external feedback. However,
recent research indicates without external feedback, LLM's intrinsic reflection
is unstable. Our investigation unveils that the key bottleneck is the quality
of the self-evaluated feedback. We find LLMs often exhibit overconfidence or
high randomness when self-evaluate, offering stubborn or inconsistent feedback,
which causes poor reflection. To remedy this, we advocate Self-Contrast: It
adaptively explores diverse solving perspectives tailored to the request,
contrasts the differences, and summarizes these discrepancies into a checklist
which could be used to re-examine and eliminate discrepancies. Our method
endows LLM with diverse perspectives to alleviate stubborn biases. Moreover,
their discrepancies indicate potential errors or inherent uncertainties that
LLM often overlooks. Reflecting upon these can catalyze more accurate and
stable reflection. Experiments conducted on a series of reasoning and
translation tasks with different LLMs serve to underscore the effectiveness and
generality of our strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent large-scale text-to-speech (TTS) models have achieved
significant progress, they still fall short in speech quality, similarity, and
prosody. Considering speech intricately encompasses various attributes (e.g.,
content, prosody, timbre, and acoustic details) that pose significant
challenges for generation, a natural idea is to factorize speech into
individual subspaces representing different attributes and generate them
individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with
novel factorized diffusion models to generate natural speech in a zero-shot
way. Specifically, 1) we design a neural codec with factorized vector
quantization (FVQ) to disentangle speech waveform into subspaces of content,
prosody, timbre, and acoustic details; 2) we propose a factorized diffusion
model to generate attributes in each subspace following its corresponding
prompt. With this factorization design, NaturalSpeech 3 can effectively and
efficiently model intricate speech with disentangled subspaces in a
divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the
state-of-the-art TTS systems on quality, similarity, prosody, and
intelligibility, and achieves on-par quality with human recordings.
Furthermore, we achieve better performance by scaling to 1B parameters and 200K
hours of training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Achieving human-level quality and naturalness on multi-speaker
  datasets (e.g., LibriSpeech) in a zero-shot way</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for EU AI policy act concerning ethics, digital
divide, and sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants'
  API Invocation Capabilities <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honglin Mu, Yang Xu, Yunlong Feng, Xiaofeng Han, Yitong Li, Yutai Hou, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of Large Language Models (LLMs), AI assistants' ability to
utilize tools, especially through API calls, has advanced notably. This
progress has necessitated more accurate evaluation methods. Many existing
studies adopt static evaluation, where they assess AI assistants' API call
based on pre-defined dialogue histories. However, such evaluation method can be
misleading, as an AI assistant might fail in generating API calls from
preceding human interaction in real cases. Instead of the resource-intensive
method of direct human-machine interactions, we propose Automated Dynamic
Evaluation (AutoDE) to assess an assistant's API call capability without human
involvement. In our framework, we endeavor to closely mirror genuine human
conversation patterns in human-machine interactions, using a LLM-based user
agent, equipped with a user script to ensure human alignment. Experimental
results highlight that AutoDE uncovers errors overlooked by static evaluations,
aligning more closely with human assessment. Testing four AI assistants using
our crafted benchmark, our method further mirrored human evaluation compared to
conventional static evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guided Distant Supervision for Multilingual Relation Extraction Data:
  Adapting to a New Language <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Plum, Tharindu Ranasinghe, Christoph Purschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction is essential for extracting and understanding
biographical information in the context of digital humanities and related
subjects. There is a growing interest in the community to build datasets
capable of training machine learning models to extract relationships. However,
annotating such datasets can be expensive and time-consuming, in addition to
being limited to English. This paper applies guided distant supervision to
create a large biographical relationship extraction dataset for German. Our
dataset, composed of more than 80,000 instances for nine relationship types, is
the largest biographical German relationship extraction dataset. We also create
a manually annotated dataset with 2000 instances to evaluate the models and
release it together with the dataset compiled using guided distant supervision.
We train several state-of-the-art machine learning models on the automatically
created dataset and release them as well. Furthermore, we experiment with
multilingual and cross-lingual experiments that could benefit many low-resource
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GlotScript: A Resource and Tool for Low Resource Writing System
  Identification <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Hossein Kargaran, François Yvon, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present GlotScript, an open resource and tool for low resource writing
system identification. GlotScript-R is a resource that provides the attested
writing systems for more than 7,000 languages. It is compiled by aggregating
information from existing writing system resources. GlotScript-T is a writing
system identification tool that covers all 161 Unicode 15.0 scripts. For an
input text, it returns its script distribution where scripts are identified by
ISO 15924 codes. We also present two use cases for GlotScript. First, we
demonstrate that GlotScript can help cleaning multilingual corpora such as mC4
and OSCAR. Second, we analyze the tokenization of a number of language models
such as GPT-4 using GlotScript and provide insights on the coverage of low
resource scripts and languages by each language model. We hope that GlotScript
will become a useful resource for work on low resource languages in the NLP
community. GlotScript-R and GlotScript-T are available at
https://github.com/cisnlp/GlotScript.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NLPre: a revised approach towards language-centric benchmarking of
  Natural Language Preprocessing systems <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martyna Wiącek, Piotr Rybak, Łukasz Pszenny, Alina Wróblewska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancements of transformer-based architectures, we observe the rise
of natural language preprocessing (NLPre) tools capable of solving preliminary
NLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or
morphological analysis) without any external linguistic guidance. It is arduous
to compare novel solutions to well-entrenched preprocessing toolkits, relying
on rule-based morphological analysers or dictionaries. Aware of the
shortcomings of existing NLPre evaluation approaches, we investigate a novel
method of reliable and fair evaluation and performance reporting. Inspired by
the GLUE benchmark, the proposed language-centric benchmarking system enables
comprehensive ongoing evaluation of multiple NLPre tools, while credibly
tracking their performance. The prototype application is configured for Polish
and integrated with the thoroughly assembled NLPre-PL benchmark. Based on this
benchmark, we conduct an extensive evaluation of a variety of Polish NLPre
systems. To facilitate the construction of benchmarking environments for other
languages, e.g. NLPre-GA for Irish or NLPre-ZH for Chinese, we ensure full
customization of the publicly released source code of the benchmarking system.
The links to all the resources (deployed platforms, source code, trained
models, datasets etc.) can be found on the project website:
https://sites.google.com/view/nlpre-benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure Guided Large Language Model for SQL Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinggang Zhang, Junnan Dong, Hao Chen, Wentao Li, Feiran Huang, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating accurate Structured Querying Language (SQL) is a long-standing
problem, especially in matching users' semantic queries with structured
databases and then generating structured SQL. Existing models typically input
queries and database schemas into the LLM and rely on the LLM to perform
semantic-structure matching and generate structured SQL. However, such
solutions overlook the structural information within user queries and
databases, which can be utilized to enhance the generation of structured SQL.
This oversight can lead to inaccurate or unexecutable SQL generation. To fully
exploit the structure, we propose a structure-to-SQL framework, which leverages
the inherent structure information to improve the SQL generation of LLMs.
Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.
SGU-SQL first links user queries and databases in a structure-enhanced manner.
It then decomposes complicated linked structures with grammar trees to guide
the LLM to generate the SQL step by step. Extensive experiments on two
benchmark datasets illustrate that SGU-SQL can outperform sixteen SQL
generation baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Trustworthy Reranking: A Simple yet Effective Abstention
  Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.12997v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.12997v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Information Retrieval (NIR) has significantly improved upon
heuristic-based IR systems. Yet, failures remain frequent, the models used
often being unable to retrieve documents relevant to the user's query. We
address this challenge by proposing a lightweight abstention mechanism tailored
for real-world constraints, with particular emphasis placed on the reranking
phase. We introduce a protocol for evaluating abstention strategies in a
black-box scenario, demonstrating their efficacy, and propose a simple yet
effective data-driven mechanism. We provide open-source code for experiment
replication and abstention implementation, fostering wider adoption and
application in diverse contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacks, Defenses and Evaluations for LLM Conversation Safety: A <span class="highlight-title">Survey</span> <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09283v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09283v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CARE: Co-Attention Network for Joint Entity and Relation Extraction <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Kong, Yamei Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint entity and relation extraction is the fundamental task of information
extraction, consisting of two subtasks: named entity recognition and relation
extraction. However, most existing joint extraction methods suffer from issues
of feature confusion or inadequate interaction between the two subtasks.
Addressing these challenges, in this work, we propose a Co-Attention network
for joint entity and Relation Extraction (CARE). Our approach includes adopting
a parallel encoding strategy to learn separate representations for each
subtask, aiming to avoid feature overlap or confusion. At the core of our
approach is the co-attention module that captures two-way interaction between
the two subtasks, allowing the model to leverage entity information for
relation prediction and vice versa, thus promoting mutual enhancement. Through
extensive experiments on three benchmark datasets for joint entity and relation
extraction (NYT, WebNLG, and SciERC), we demonstrate that our proposed model
outperforms existing baseline models. Our code will be available at
https://github.com/kwj0x7f/CARE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-Shot Detection of Machine-Generated Text using Style Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Rivera Soto, Kailin Koch, Aleem Khan, Barry Chen, Marcus Bishop, Nicholas Andrews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of instruction-tuned language models that convincingly mimic human
writing poses a significant risk of abuse. However, such abuse may be
counteracted with the ability to detect whether a piece of text was composed by
a language model rather than a human author. Some previous approaches to this
problem have relied on supervised methods by training on corpora of confirmed
human- and machine- written documents. Unfortunately, model under-specification
poses an unavoidable challenge for neural network-based detectors, making them
brittle in the face of data shifts, such as the release of newer language
models producing still more fluent text than the models used to train the
detectors. Other approaches require access to the models that may have
generated a document in question, which is often impractical. In light of these
challenges, we pursue a fundamentally different approach not relying on samples
from language models of concern at training time. Instead, we propose to
leverage representations of writing style estimated from human-authored text.
Indeed, we find that features effective at distinguishing among human authors
are also effective at distinguishing human from machine authors, including
state-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.
Furthermore, given a handful of examples composed by each of several specific
language models of interest, our approach affords the ability to predict which
model generated a given document. The code and data to reproduce our
experiments are available at
https://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Wolf in Sheep's Clothing: Generalized Nested Jailbreak <span class="highlight-title">Prompt</span>s can
  Fool Large Language Models Easily <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, Shujian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to
provide useful and safe responses. However, adversarial prompts known as
'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially
harmful content. Exploring jailbreak prompts can help to better reveal the
weaknesses of LLMs and further steer us to secure them. Unfortunately, existing
jailbreak methods either suffer from intricate manual design or require
optimization on other white-box models, which compromises either generalization
or efficiency. In this paper, we generalize jailbreak prompt attacks into two
aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we
propose ReNeLLM, an automatic framework that leverages LLMs themselves to
generate effective jailbreak prompts. Extensive experiments demonstrate that
ReNeLLM significantly improves the attack success rate while greatly reducing
the time cost compared to existing baselines. Our study also reveals the
inadequacy of current defense methods in safeguarding LLMs. Finally, we analyze
the failure of LLMs defense from the perspective of prompt execution priority,
and propose corresponding defense strategies. We hope that our research can
catalyze both the academic community and LLMs developers towards the provision
of safer and more regulated LLMs. The code is available at
https://github.com/NJUNLP/ReNeLLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Acccepted by NAACL 2024, 18 pages, 7 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visually Guided Generative Text-Layout <span class="highlight-title">Pre-train</span>ing for Document
  Intelligence <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiming Mao, Haoli Bai, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior study shows that pre-training techniques can boost the performance of
visual document understanding (VDU), which typically requires models to gain
abilities to perceive and reason both document texts and layouts (e.g.,
locations of texts and table-cells). To this end, we propose visually guided
generative text-layout pre-training, named ViTLP. Given a document image, the
model optimizes hierarchical language and layout modeling objectives to
generate the interleaved text and layout sequence. In addition, to address the
limitation of processing long documents by Transformers, we introduce a
straightforward yet effective multi-segment generative pre-training scheme,
facilitating ViTLP to process word-intensive documents of any length. ViTLP can
function as a native OCR model to localize and recognize texts of document
images. Besides, ViTLP can be effectively applied to various downstream VDU
tasks. Extensive experiments show that ViTLP achieves competitive performance
over existing baselines on benchmark VDU tasks, including information
extraction, document classification, and document question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 main conference. The first version of this
  paper was submitted to OpenReview
  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\textit{Link<span class="highlight-title">Prompt</span>}$: Natural and Universal Adversarial Attacks on
  <span class="highlight-title">Prompt</span>-based Language Models <span class="chip">NAACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Xu, Wenjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based learning is a new language model training paradigm that adapts
the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes
the performance benchmarks across various natural language processing (NLP)
tasks. Instead of using a fixed prompt template to fine-tune the model, some
research demonstrates the effectiveness of searching for the prompt via
optimization. Such prompt optimization process of prompt-based learning on PLMs
also gives insight into generating adversarial prompts to mislead the model,
raising concerns about the adversarial vulnerability of this paradigm. Recent
studies have shown that universal adversarial triggers (UATs) can be generated
to alter not only the predictions of the target PLMs but also the prediction of
corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based
learning paradigm. However, UATs found in previous works are often unreadable
tokens or characters and can be easily distinguished from natural texts with
adaptive defenses. In this work, we consider the naturalness of the UATs and
develop $\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs
by a gradient-based beam search algorithm that not only effectively attacks the
target PLMs and PFMs but also maintains the naturalness among the trigger
tokens. Extensive results demonstrate the effectiveness of
$\textit{LinkPrompt}$, as well as the transferability of UATs generated by
$\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and
API-accessed LLM GPT-3.5-turbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLatrieval: LLM-Verified Retrieval for Verifiable Generation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07838v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07838v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifiable generation aims to let the large language model (LLM) generate
text with supporting documents, which enables the user to flexibly verify the
answer and makes the LLM's output more reliable. Retrieval plays a crucial role
in verifiable generation. Specifically, the retrieved documents not only
supplement knowledge to help the LLM generate correct answers, but also serve
as supporting evidence for the user to verify the LLM's output. However, the
widely used retrievers become the bottleneck of the entire pipeline and limit
the overall performance. Their capabilities are usually inferior to LLMs since
they often have much fewer parameters than the large language model and have
not been demonstrated to scale well to the size of LLMs. If the retriever does
not correctly find the supporting documents, the LLM can not generate the
correct and verifiable answer, which overshadows the LLM's remarkable
abilities. To address these limitations, we propose \LLatrieval (Large Language
Model Verified Retrieval), where the LLM updates the retrieval result until it
verifies that the retrieved documents can sufficiently support answering the
question. Thus, the LLM can iteratively provide feedback to retrieval and
facilitate the retrieval result to fully support verifiable generation.
Experiments show that LLatrieval significantly outperforms extensive baselines
and achieves state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual
  Topic Modeling <span class="chip">AAAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobao Wu, Xinshuai Dong, Thong Nguyen, Chaoqun Liu, Liangming Pan, Anh Tuan Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual topic models have been prevalent for cross-lingual text
analysis by revealing aligned latent topics. However, most existing methods
suffer from producing repetitive topics that hinder further analysis and
performance decline caused by low-coverage dictionaries. In this paper, we
propose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM).
Instead of the direct alignment in previous work, we propose a topic alignment
with mutual information method. This works as a regularization to properly
align topics and prevent degenerate topic representations of words, which
mitigates the repetitive topic issue. To address the low-coverage dictionary
issue, we further propose a cross-lingual vocabulary linking method that finds
more linked cross-lingual words for topic alignment beyond the translations of
a given dictionary. Extensive experiments on English, Chinese, and Japanese
datasets demonstrate that our method outperforms state-of-the-art baselines,
producing more coherent, diverse, and well-aligned topics and showing better
transferability for cross-lingual classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI2023 conference. Code is available at
  https://github.com/BobXWu/InfoCTM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Text to Source: Results in Detecting Large Language Model-Generated
  Content <span class="chip">COLING</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13322v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13322v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wissam Antoun, Benoît Sagot, Djamé Seddah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread use of Large Language Models (LLMs), celebrated for their
ability to generate human-like text, has raised concerns about misinformation
and ethical implications. Addressing these concerns necessitates the
development of robust methods to detect and attribute text generated by LLMs.
This paper investigates "Cross-Model Detection," by evaluating whether a
classifier trained to distinguish between source LLM-generated and
human-written text can also detect text from a target LLM without further
training. The study comprehensively explores various LLM sizes and families,
and assesses the impact of conversational fine-tuning techniques, quantization,
and watermarking on classifier generalization. The research also explores Model
Attribution, encompassing source model identification, model family, and model
size classification, in addition to quantization and watermarking detection.
Our results reveal several key findings: a clear inverse relationship between
classifier effectiveness and model size, with larger LLMs being more
challenging to detect, especially when the classifier is trained on data from
smaller models. Training on data from similarly sized LLMs can improve
detection performance from larger models but may lead to decreased performance
when dealing with smaller models. Additionally, model attribution experiments
show promising results in identifying source models and model families,
highlighting detectable signatures in LLM-generated text, with particularly
remarkable outcomes in watermarking detection, while no detectable signatures
of quantization were observed. Overall, our study contributes valuable insights
into the interplay of model size, family, and training data in LLM detection
and attribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING-LREC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To help the open-source community have a better understanding of
Mixture-of-Experts (MoE) based large language models (LLMs), we train and
release OpenMoE, a series of fully open-sourced and reproducible decoder-only
MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T
tokens. Our investigation confirms that MoE-based LLMs can offer a more
favorable cost-effectiveness trade-off than dense LLMs, highlighting the
potential effectiveness for future LLM development.
  One more important contribution of this study is an in-depth analysis of the
routing mechanisms within our OpenMoE models, leading to three significant
findings: Context-Independent Specialization, Early Routing Learning, and
Drop-towards-the-End. We discovered that routing decisions in MoE models are
predominantly based on token IDs, with minimal context relevance. The
token-to-expert assignments are determined early in the pre-training phase and
remain largely unchanged. This imperfect routing can result in performance
degradation, particularly in sequential tasks like multi-turn conversations,
where tokens appearing later in a sequence are more likely to be dropped.
Finally, we rethink our design based on the above-mentioned observations and
analysis. To facilitate future MoE LLM development, we propose potential
strategies for mitigating the issues we found and further improving
off-the-shelf MoE LLM designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intrinsic Subgraph Generation for Interpretable Graph based Visual
  Question Answering <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Tilli, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large success of deep learning based methods in Visual Question Answering
(VQA) has concurrently increased the demand for explainable methods. Most
methods in Explainable Artificial Intelligence (XAI) focus on generating
post-hoc explanations rather than taking an intrinsic approach, the latter
characterizing an interpretable model. In this work, we introduce an
interpretable approach for graph-based VQA and demonstrate competitive
performance on the GQA dataset. This approach bridges the gap between
interpretability and performance. Our model is designed to intrinsically
produce a subgraph during the question-answering process as its explanation,
providing insight into the decision making. To evaluate the quality of these
generated subgraphs, we compare them against established post-hoc
explainability methods for graph neural networks, and perform a human
evaluation. Moreover, we present quantitative metrics that correlate with the
evaluations of human assessors, acting as automatic metrics for the generated
explanatory subgraphs. Our implementation is available at
https://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation for Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10997v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10997v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) showcase impressive capabilities but encounter
challenges like hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the generation,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval, the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces up-to-date evaluation framework and
benchmark. At the end, this article delineates the challenges currently faced
and points out prospective avenues for research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing Work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.16071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.16071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tolulope Ogunremi, Kola Tubosun, Anuoluwapo Aremu, Iroro Orife, David Ifeoluwa Adelani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce \`{I}r\`{o}y\`{i}nSpeech, a new corpus influenced by the desire
to increase the amount of high quality, contemporary Yor\`{u}b\'{a} speech
data, which can be used for both Text-to-Speech (TTS) and Automatic Speech
Recognition (ASR) tasks. We curated about 23000 text sentences from news and
creative writing domains with the open license CC-BY-4.0. To encourage a
participatory approach to data creation, we provide 5000 curated sentences to
the Mozilla Common Voice platform to crowd-source the recording and validation
of Yor\`{u}b\'{a} speech data. In total, we created about 42 hours of speech
data recorded by 80 volunteers in-house, and 6 hours of validated recordings on
Mozilla Common Voice platform. Our TTS evaluation suggests that a
high-fidelity, general domain, single-speaker Yor\`{u}b\'{a} voice is possible
with as little as 5 hours of speech. Similarly, for ASR we obtained a baseline
word error rate (WER) of 23.8.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Centered Masking for Language-Image <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingliang Liang, Martha Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,
straightforward, and effective technique for masking image patches during
pre-training of a vision-language model. GLIP builds on Fast Language-Image
Pre-Training (FLIP), which randomly masks image patches while training a CLIP
model. GLIP replaces random masking with centered masking, that uses a Gaussian
distribution and is inspired by the importance of image patches at the center
of the image. GLIP retains the same computational savings as FLIP, while
improving performance across a range of downstream datasets and tasks, as
demonstrated by our experimental results. We show the benefits of GLIP to be
easy to obtain, requiring no delicate tuning of the Gaussian, and also
applicable to data sets containing images without an obvious center focus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying the Correlation Between Language Distance and Cross-Lingual
  Transfer in a Multilingual Representation Space <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fred Philippy, Siwen Guo, Shohreh Haddadan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research has investigated the impact of various linguistic features on
cross-lingual transfer performance. In this study, we investigate the manner in
which this effect can be mapped onto the representation space. While past
studies have focused on the impact on cross-lingual alignment in multilingual
language models during fine-tuning, this study examines the absolute evolution
of the respective language representation spaces produced by MLLMs. We place a
specific emphasis on the role of linguistic characteristics and investigate
their inter-correlation with the impact on representation spaces and
cross-lingual transfer performance. Additionally, this paper provides
preliminary evidence of how these findings can be leveraged to enhance transfer
to linguistically distant languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGTYP Workshop 2023 (co-located with EACL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11399v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11399v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjae Shin, Hyunseok Lim, Inho Won, Changsu Choi, Minjun Kim, Seungwoo Song, Hangyeol Yoo, Sangmin Kim, Kyungtae Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive development of large language models (LLMs) is expanding into
the realm of large multimodal models (LMMs), which incorporate multiple types
of data beyond text. However, the nature of multimodal models leads to
significant expenses in the creation of training data. Furthermore,
constructing multilingual data for LMMs presents its own set of challenges due
to language diversity and complexity. Therefore, in this study, we propose two
cost-effective methods to solve this problem: (1) vocabulary expansion and
pretraining of multilingual LLM for specific languages, and (2) automatic and
elaborate construction of multimodal datasets using GPT4-V. Based on015 these
methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal
training dataset. Additionally, we developed a bilingual multimodal model that
exhibits excellent performance in both Korean and English, surpassing existing
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Knowledge for Few-shot Table-to-Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12468v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12468v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixin Guo, Minyxuan Yan, Jiexing Qi, Jianping Zhou, Ziwei He, Guanjie Zheng, Xinbing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models (PLMs) have made remarkable progress in
table-to-text generation tasks. However, the lack of domain-specific knowledge
makes it challenging to bridge the topological gap between tabular data and
text, especially in real-world applications with limited resources. To mitigate
the limitation of insufficient labeled data, we propose a novel framework:
Adapt-Knowledge-to-Generate (AKG). The core insight of AKG is to adapt
unlabeled domain-specific knowledge into the model, which brings at least three
benefits: (1) it injects representation of normal table-related descriptions to
bridge the topological gap between tabular data and texts; (2) it enables us to
use large amounts of unlabeled domain-specific knowledge fully, which can
alleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it
allows us to design various tasks to employ the domain-specific knowledge.
Extensive experiments and analyses are conducted on three open-domain, few-shot
natural language generation (NLG) data sets: Humans, Songs, and Books. Compared
to previous state-of-the-art approaches, our model achieves superior
performance in terms of both fluency and accuracy as judged by human and
automatic evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2302.04415</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06201v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06201v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, Deqing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address intricate real-world tasks, there has been a rising interest in
tool utilization in applications of large language models (LLMs). To develop
LLM-based agents, it usually requires LLMs to understand many tool functions
from different tool documentation. But these documentations could be diverse,
redundant or incomplete, which immensely affects the capability of LLMs in
using tools. To solve this, we introduce EASYTOOL, a framework transforming
diverse and lengthy tool documentation into a unified and concise tool
instruction for easier tool usage. EasyTool purifies essential information from
extensive tool documentation of different sources, and elaborates a unified
interface (i.e., tool instruction) to offer standardized tool descriptions and
functionalities for LLM-based agents. Extensive experiments on multiple
different tasks demonstrate that EasyTool can significantly reduce token
consumption and improve the performance of tool utilization in real-world
scenarios. Our code will be available at
\url{https://github.com/microsoft/JARVIS/} in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs Are Few-Shot In-Context Low-Resource Language Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Cahyawijaya, Holy Lovenia, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) empowers large language models (LLMs) to perform
diverse tasks in underrepresented languages using only short in-context
information, offering a crucial avenue for narrowing the gap between
high-resource and low-resource languages. Nonetheless, there is only a handful
of works explored ICL for low-resource languages with most of them focusing on
relatively high-resource languages, such as French and Spanish. In this work,
we extensively study ICL and its cross-lingual variation (X-ICL) on 25
low-resource and 7 relatively higher-resource languages. Our study not only
assesses the effectiveness of ICL with LLMs in low-resource languages but also
identifies the shortcomings of in-context label alignment, and introduces a
more effective alternative: query alignment. Moreover, we provide valuable
insights into various facets of ICL for low-resource languages. Our study
concludes the significance of few-shot in-context information on enhancing the
low-resource understanding quality of LLMs through semantically relevant
information by closing the language gap in the target language and aligning the
semantics between the targeted low-resource and the high-resource language that
the model is proficient in. Our work highlights the importance of advancing ICL
research, particularly for low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mix-Initiative Response Generation with Dynamic Prefix Tuning <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17636v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17636v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Nie, Heyan Huang, Xian-Ling Mao, Lizi Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed initiative serves as one of the key factors in controlling conversation
directions. For a speaker, responding passively or leading proactively would
result in rather different responses. However, most dialogue systems focus on
training a holistic response generation model without any distinction among
different initiatives. It leads to the cross-contamination problem, where the
model confuses different initiatives and generates inappropriate responses.
Moreover, obtaining plenty of human annotations for initiative labels can be
expensive. To address this issue, we propose a general mix-Initiative Dynamic
Prefix Tuning framework (IDPT) to decouple different initiatives from the
generation model, which learns initiative-aware prefixes in both supervised and
unsupervised settings. Specifically, IDPT decouples initiative factors into
different prefix parameters and uses the attention mechanism to adjust the
selection of initiatives in guiding generation dynamically. The prefix
parameters can be tuned towards accurate initiative prediction as well as
mix-initiative response generation. Extensive experiments on two public
dialogue datasets show that the proposed IDPT outperforms previous baselines on
both automatic metrics and human evaluations. It also manages to generate
appropriate responses with manipulated initiatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language
  Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        HyunJin Kim, Young Jin Kim, JinYeong Bak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) show impressive performance in various
downstream NLP tasks. However, pre-training large language models demands
substantial memory and training compute. Furthermore, due to the substantial
resources required, many PLM weights are confidential. Consequently, users are
compelled to share their data with model owners for fine-tuning specific tasks.
To overcome the limitations, we introduce Plug-in External Memory Adaptation
(PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM
fine-tuning without requiring access to all the weights. PEMA integrates with
context representations from test data during inference to perform downstream
tasks. It uses external memory to store PLM-generated context representations
mapped with target tokens. Our method utilizes weight matrices of LoRA-like
bottlenecked adapter in the PLM's final layer to enhance efficiency. Our
approach also includes Gradual Unrolling, a novel interpolation strategy to
improve generation quality. We validate PEMA's effectiveness through
experiments on syntactic and real datasets for machine translation and style
transfer. Our findings show that PEMA outperforms other PEFT approaches in
memory and latency efficiency for training, and also excels in maintaining
sentence meaning and generating appropriate language and styles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate
  Professional and Non-Professional Styled Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zong, Yuyan Chen, Weiming Lu, Jian Shao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated efficacy in various linguistic
applications, including text summarization and controlled text generation.
However, studies into their capacity of switching between styles via
fine-tuning remain underexplored. This study concentrates on textual
professionalism and introduces a novel methodology, named ProSwitch, which
equips a language model with the ability to produce both professional and
non-professional responses through knowledge-guided instruction tuning.
ProSwitch unfolds across three phases: data preparation for gathering domain
knowledge and training corpus; instruction tuning for optimizing language
models with multiple levels of instruction formats; and comprehensive
evaluation for assessing the professionalism discrimination and reference-based
quality of generated text. Comparative analysis of ProSwitch against both
general and specialized language models reveals that our approach outperforms
baselines in switching between professional and non-professional text
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBQ: Cross-Block Quantization for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07950v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07950v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Ding, Xiaoyu Liu, Zhijun Tu, Yun Zhang, Wei Li, Jie Hu, Hanting Chen, Yehui Tang, Zhiwei Xiong, Baoqun Yin, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) has played a key role in compressing large
language models (LLMs) with ultra-low costs. However, existing PTQ methods only
focus on handling the outliers within one layer or one block, which ignores the
dependency of blocks and leads to severe performance degradation in low-bit
settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ
method for LLMs. CBQ employs a cross-block dependency using a homologous
reconstruction scheme, establishing long-range dependencies across multiple
blocks to minimize error accumulation. Furthermore, CBQ incorporates a
coarse-to-fine preprocessing (CFP) strategy for suppressing weight and
activation outliers, coupled with an adaptive LoRA-Rounding technique for
precise weight quantization. These innovations enable CBQ to not only handle
extreme outliers effectively but also improve overall quantization accuracy.
Extensive experiments show that CBQ achieves superior low-bit quantization
(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across
various LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model
within only 4.3 hours on a single GPU, achieving a commendable tradeoff between
performance and quantization efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting
  Jailbreaks <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14965v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14965v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, Monojit Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent explorations with commercial Large Language Models (LLMs) have shown
that non-expert users can jailbreak LLMs by simply manipulating their prompts;
resulting in degenerate output behavior, privacy and security breaches,
offensive outputs, and violations of content regulator policies. Limited
studies have been conducted to formalize and analyze these attacks and their
mitigations. We bridge this gap by proposing a formalism and a taxonomy of
known (and possible) jailbreaks. We survey existing jailbreak methods and their
effectiveness on open-source and commercial LLMs (such as GPT-based models,
OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak
detection in terms of their effectiveness against known attacks. For further
analysis, we release a dataset of model outputs across 3700 jailbreak prompts
over 4 tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024 - The 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BridgeTower: Building Bridges Between Encoders in Vision-Language
  Representation Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08657v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08657v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a deep cross-modal encoder, or feed the
last-layer uni-modal representations from the deep pre-trained uni-modal
encoders into the top cross-modal encoder. Both approaches potentially restrict
vision-language representation learning and limit model performance. In this
paper, we propose BridgeTower, which introduces multiple bridge layers that
build a connection between the top layers of uni-modal encoders and each layer
of the cross-modal encoder. This enables effective bottom-up cross-modal
alignment and fusion between visual and textual representations of different
semantic levels of pre-trained uni-modal encoders in the cross-modal encoder.
Pre-trained with only 4M images, BridgeTower achieves state-of-the-art
performance on various downstream vision-language tasks. In particular, on the
VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming
the previous state-of-the-art model METER by 1.09% with the same pre-training
data and almost negligible additional parameters and computational costs.
Notably, when further scaling the model, BridgeTower achieves an accuracy of
81.15%, surpassing models that are pre-trained on orders-of-magnitude larger
datasets. Code and checkpoints are available at
https://github.com/microsoft/BridgeTower.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023, Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue
  Systems <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04357v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04357v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue response selection aims to select an appropriate response from
several candidates based on a given user and system utterance history. Most
existing works primarily focus on post-training and fine-tuning tailored for
cross-encoders. However, there are no post-training methods tailored for dense
encoders in dialogue response selection. We argue that when the current
language model, based on dense dialogue systems (such as BERT), is employed as
a dense encoder, it separately encodes dialogue context and response, leading
to a struggle to achieve the alignment of both representations. Thus, we
propose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward
yet effective post-training technique tailored for dense encoders in dialogue
response selection. Dial-MAE uses an asymmetric encoder-decoder architecture to
compress the dialogue semantics into dense vectors, which achieves better
alignment between the features of the dialogue context and response. Our
experiments have demonstrated that Dial-MAE is highly effective, achieving
state-of-the-art performance on two commonly evaluated benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoftTiger: A Clinical Foundation Model for Healthcare Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Chen, Igor Couto, Wei Cai, Cong Fu, Bruno Dorneles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SoftTiger, a clinical large language model (CLaM) designed as a
foundation model for healthcare workflows. The narrative and unstructured
nature of clinical notes is a major obstacle for healthcare intelligentization.
We address a critical problem of structuring clinical notes into clinical data,
according to international interoperability standards. We collect and annotate
data for three subtasks, namely, international patient summary, clinical
impression and medical encounter. We then supervised fine-tuned a
state-of-the-art LLM using public and credentialed clinical data. The training
is orchestrated in a way that the target model can first support basic clinical
tasks such as abbreviation expansion and temporal information extraction, and
then learn to perform more complex downstream clinical tasks. Moreover, we
address several modeling challenges in the healthcare context, e.g., extra long
context window. Our blind pairwise evaluation shows that SoftTiger outperforms
other popular open-source models and GPT-3.5, comparable to Gemini-pro, with a
mild gap from GPT-4. We believe that LLMs may become a step-stone towards
healthcare digitalization and democratization. Therefore, we publicly release
SoftTiger models at scales of 13 billion and 70 billion parameters, as well as
datasets and code for our innovative scalable evaluation, hopefully, making a
significant contribution to the healthcare industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probing Multimodal Large Language Models for Global and Local Semantic
  Representations <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong Feng, Dongyan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of Multimodal Large Language Models (MLLMs) has greatly
accelerated the development of applications in understanding integrated texts
and images. Recent works leverage image-caption datasets to train MLLMs,
achieving state-of-the-art performance on image-to-text tasks. However, there
are few studies exploring which layers of MLLMs make the most effort to the
global image information, which plays vital roles in multimodal comprehension
and generation. In this study, we find that the intermediate layers of models
can encode more global semantic information, whose representation vectors
perform better on visual-language entailment tasks, rather than the topmost
layers. We further probe models regarding local semantic representations
through object recognition tasks. We find that the topmost layers may
excessively focus on local information, leading to a diminished ability to
encode global information. Our code and data are released via
https://github.com/kobayashikanna01/probing_MLLM_rep.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024 as a short paper (Camera Ready)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models are Free Boosters for Biomedical Imaging Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, Naira Hovakimyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we uncover the unexpected efficacy of residual-based large
language models (LLMs) as part of encoders for biomedical imaging tasks, a
domain traditionally devoid of language or textual data. The approach diverges
from established methodologies by utilizing a frozen transformer block,
extracted from pre-trained LLMs, as an innovative encoder layer for the direct
processing of visual tokens. This strategy represents a significant departure
from the standard multi-modal vision-language frameworks, which typically hinge
on language-driven prompts and inputs. We found that these LLMs could boost
performance across a spectrum of biomedical imaging applications, including
both 2D and 3D visual classification tasks, serving as plug-and-play boosters.
More interestingly, as a byproduct, we found that the proposed framework
achieved superior performance, setting new state-of-the-art results on
extensive, standardized datasets in MedMNIST-2D and 3D. Through this work, we
aim to open new avenues for employing LLMs in biomedical imaging and enriching
the understanding of their potential in this specialized domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using <span class="highlight-title">Pre-train</span>ed Language
  Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Look Before You Leap: Problem Elaboration <span class="highlight-title">Prompt</span>ing Improves
  Mathematical Reasoning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Liao, Jidong Tian, Shaohua Hu, Hao He, Yaohui Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) still grapple with complex tasks like
mathematical reasoning. Despite significant efforts invested in improving
prefix prompts or reasoning process, the crucial role of problem context might
have been neglected. Accurate recognition of inputs is fundamental for solving
mathematical tasks, as ill-formed problems could potentially mislead LLM's
reasoning. In this study, we propose a new approach named Problem Elaboration
Prompting (PEP) to enhance the mathematical capacities of LLMs. Specifically,
PEP decomposes and elucidates the problem context before reasoning, therefore
enhancing the context modeling and parsing efficiency. Experiments across
datasets and models demonstrate promising performances: (1) PEP demonstrates an
overall enhancement in various mathematical tasks. For instance, with the
GPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through
greedy decoding and self-consistency, respectively. (2) PEP can be easily
implemented and integrated with other prompting methods. (3) PEP shows
particular strength in handling distraction problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Partial Mobilization: Tracking Multilingual Information Flows Amongst
  Russian Media Outlets and Telegram 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10856v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10856v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hans W. A. Hanley, Zakir Durumeric
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In response to disinformation and propaganda from Russian online media
following the invasion of Ukraine, Russian media outlets such as Russia Today
and Sputnik News were banned throughout Europe. To maintain viewership, many of
these Russian outlets began to heavily promote their content on messaging
services like Telegram. In this work, we study how 16 Russian media outlets
interacted with and utilized 732 Telegram channels throughout 2022. Leveraging
the foundational model MPNet, DP-means clustering, and Hawkes processes, we
trace how narratives spread between news sites and Telegram channels. We show
that news outlets not only propagate existing narratives through Telegram but
that they source material from the messaging platform. For example, across the
websites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of
articles discussed content that originated/resulted from activity on Telegram.
Finally, tracking the spread of individual topics, we measure the rate at which
news outlets and Telegram channels disseminate content within the Russian media
ecosystem, finding that websites like ura.news and Telegram channels such as
@genshab are the most effective at disseminating their content.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICWSM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NLP-based detection of systematic anomalies among the narratives of
  consumer complaints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11138v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11138v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiheng Gao, Ning Sun, Xuefeng Wang, Chen Yang, Ričardas Zitikis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop an NLP-based procedure for detecting systematic nonmeritorious
consumer complaints, simply called systematic anomalies, among complaint
narratives. While classification algorithms are used to detect pronounced
anomalies, in the case of smaller and frequent systematic anomalies, the
algorithms may falter due to a variety of reasons, including technical ones as
well as natural limitations of human analysts. Therefore, as the next step
after classification, we convert the complaint narratives into quantitative
data, which are then analyzed using an algorithm for detecting systematic
anomalies. We illustrate the entire procedure using complaint narratives from
the Consumer Complaint Database of the Consumer Financial Protection Bureau.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-26T00:00:00Z">2024-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">108</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LISA: Layerwise Importance Sampling for Memory-Efficient Large Language
  Model Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The machine learning community has witnessed impressive advancements since
the first appearance of large language models (LLMs), yet their huge memory
consumption has become a major roadblock to large-scale training. Parameter
Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been
proposed to alleviate this problem, but their performance still fails to match
full parameter training in most large-scale fine-tuning settings. Attempting to
complement this deficiency, we investigate layerwise properties of LoRA on
fine-tuning tasks and observe an uncommon skewness of weight norms across
different layers. Utilizing this key observation, a surprisingly simple
training strategy is discovered, which outperforms both LoRA and full parameter
training in a wide range of settings with memory costs as low as LoRA. We name
it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA,
which applies the idea of importance sampling to different layers in LLMs and
randomly freeze most middle layers during optimization. Experimental results
show that with similar or less GPU memory consumption, LISA surpasses LoRA or
even full parameter tuning in downstream fine-tuning tasks, where LISA
consistently outperforms LoRA by over $11\%$-$37\%$ in terms of MT-Bench
scores. On large models, specifically LLaMA-2-70B, LISA achieves on-par or
better performance than LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating
its effectiveness across different domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Unreasonable Ineffectiveness of the Deeper Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We empirically study a simple layer-pruning strategy for popular families of
open-weight pretrained LLMs, finding minimal degradation of performance on
different question-answering benchmarks until after a large fraction (up to
half) of the layers are removed. To prune these models, we identify the optimal
block of layers to prune by considering similarity across layers; then, to
"heal" the damage, we perform a small amount of finetuning. In particular, we
use parameter-efficient finetuning (PEFT) methods, specifically quantization
and Low Rank Adapters (QLoRA), such that each of our experiments can be
performed on a single A100 GPU. From a practical perspective, these results
suggest that layer pruning methods can complement other PEFT strategies to
further reduce computational resources of finetuning on the one hand, and can
improve the memory and latency of inference on the other hand. From a
scientific perspective, the robustness of these LLMs to the deletion of layers
implies either that current pretraining methods are not properly leveraging the
parameters in the deeper layers of the network or that the shallow layers play
a critical role in storing knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 + 10 pages, 5 + 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring LLMs as a Source of Targeted Synthetic Textual Data to
  Minimize High Confidence Misclassifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Lippmann, Matthijs Spaan, Jie Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) models optimized for predictive performance
often make high confidence errors and suffer from vulnerability to adversarial
and out-of-distribution data. Existing work has mainly focused on mitigation of
such errors using either humans or an automated approach. In this study, we
explore the usage of large language models (LLMs) for data augmentation as a
potential solution to the issue of NLP models making wrong predictions with
high confidence during classification tasks. We compare the effectiveness of
synthetic data generated by LLMs with that of human data obtained via the same
procedure. For mitigation, humans or LLMs provide natural language
characterizations of high confidence misclassifications to generate synthetic
data, which are then used to extend the training set. We conduct an extensive
evaluation of our approach on three classification tasks and demonstrate its
effectiveness in reducing the number of high confidence misclassifications
present in the model, all while maintaining the same level of accuracy.
Moreover, we find that the cost gap between humans and LLMs surpasses an order
of magnitude, as LLMs attain human-like performance while being more scalable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChroniclingAmericaQA: A Large-scale Question Answering <span class="highlight-title">Dataset</span> based on
  Historical American Newspaper Pages <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhawna Piryani, Jamshid Mozafari, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering (QA) and Machine Reading Comprehension (MRC) tasks have
significantly advanced in recent years due to the rapid development of deep
learning techniques and, more recently, large language models. At the same
time, many benchmark datasets have become available for QA and MRC tasks.
However, most existing large-scale benchmark datasets have been created
predominantly using synchronous document collections like Wikipedia or the Web.
Archival document collections, such as historical newspapers, contain valuable
information from the past that is still not widely used to train large language
models. To further contribute to advancing QA and MRC tasks and to overcome the
limitation of previous datasets, we introduce ChroniclingAmericaQA, a
large-scale dataset with 485K question-answer pairs created based on the
historical newspaper collection Chronicling America. Our dataset is constructed
from a subset of the Chronicling America newspaper collection spanning 120
years. One of the significant challenges for utilizing digitized historical
newspaper collections is the low quality of OCR text. Therefore, to enable
realistic testing of QA models, our dataset can be used in three different
ways: answering questions from raw and noisy content, answering questions from
cleaner, corrected version of the content, as well as answering questions from
scanned images of newspaper pages. This and the fact that ChroniclingAmericaQA
spans the longest time period among available QA datasets make it quite a
unique and useful resource.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Verbing Weirds Language (Models): Evaluation of English Zero-Derivation
  in Five LLMs <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David R. Mortensen, Valentina Izrailevitch, Yunze Xiao, Hinrich Schütze, Leonie Weissweiler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lexical-syntactic flexibility, in the form of conversion (or zero-derivation)
is a hallmark of English morphology. In conversion, a word with one part of
speech is placed in a non-prototypical context, where it is coerced to behave
as if it had a different part of speech. However, while this process affects a
large part of the English lexicon, little work has been done to establish the
degree to which language models capture this type of generalization. This paper
reports the first study on the behavior of large language models with reference
to conversion. We design a task for testing lexical-syntactic flexibility --
the degree to which models can generalize over words in a construction with a
non-prototypical part of speech. This task is situated within a natural
language inference paradigm. We test the abilities of five language models --
two proprietary models (GPT-3.5 and GPT-4), three open-source models (Mistral
7B, Falcon 40B, and Llama 2 70B). We find that GPT-4 performs best on the task,
followed by GPT-3.5, but that the open source language models are also able to
perform it and that the 7B parameter Mistral displays as little difference
between its baseline performance on the natural language inference task and the
non-prototypical syntactic category task, as the massive GPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Domain Knowledge to Guide Dialog Structure Induction via Neural
  Probabilistic Soft Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Pryor, Quan Yuan, Jeremiah Liu, Mehran Kazemi, Deepak Ramachandran, Tania Bedrax-Weiss, Lise Getoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialog Structure Induction (DSI) is the task of inferring the latent dialog
structure (i.e., a set of dialog states and their temporal transitions) of a
given goal-oriented dialog. It is a critical component for modern dialog system
design and discourse analysis. Existing DSI approaches are often purely
data-driven, deploy models that infer latent states without access to domain
knowledge, underperform when the training corpus is limited/noisy, or have
difficulty when test dialogs exhibit distributional shifts from the training
domain. This work explores a neural-symbolic approach as a potential solution
to these problems. We introduce Neural Probabilistic Soft Logic Dialogue
Structure Induction (NEUPSL DSI), a principled approach that injects symbolic
knowledge into the latent space of a generative neural model. We conduct a
thorough empirical investigation on the effect of NEUPSL DSI learning on hidden
representation quality, few-shot learning, and out-of-domain generalization
performance. Over three dialog structure induction datasets and across
unsupervised and semi-supervised settings for standard and cross-domain
generalization, the injection of symbolic knowledge using NEUPSL DSI provides a
consistent boost in performance over the canonical baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArabicaQA: A Comprehensive <span class="highlight-title">Dataset</span> for Arabic Question Answering <span class="chip">SIGIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Mahmoud Kasem, Mahmoud Abdalla, Mohamed Mahmoud, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the significant gap in Arabic natural language
processing (NLP) resources by introducing ArabicaQA, the first large-scale
dataset for machine reading comprehension and open-domain question answering in
Arabic. This comprehensive dataset, consisting of 89,095 answerable and 3,701
unanswerable questions created by crowdworkers to look similar to answerable
ones, along with additional labels of open-domain questions marks a crucial
advancement in Arabic NLP resources. We also present AraDPR, the first dense
passage retrieval model trained on the Arabic Wikipedia corpus, specifically
designed to tackle the unique challenges of Arabic text retrieval. Furthermore,
our study includes extensive benchmarking of large language models (LLMs) for
Arabic question answering, critically evaluating their performance in the
Arabic language context. In conclusion, ArabicaQA, AraDPR, and the benchmarking
of LLMs in Arabic question answering offer significant advancements in the
field of Arabic NLP. The dataset and code are publicly accessible for further
research https://github.com/DataScienceUIBK/ArabicaQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at SIGIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Chenguang Huang, Martin Büchner, Abhinav Valada, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent open-vocabulary robot mapping methods enrich dense geometric maps with
pre-trained visual-language features. While these maps allow for the prediction
of point-wise saliency maps when queried for a certain language concept,
large-scale environments and abstract queries beyond the object level still
pose a considerable hurdle, ultimately limiting language-grounded robotic
navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D
scene graph mapping approach for language-grounded robot navigation. Leveraging
open-vocabulary vision foundation models, we first obtain state-of-the-art
open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene
graph hierarchy consisting of floor, room, and object concepts, each enriched
with open-vocabulary features. Our approach is able to represent multi-story
buildings and allows robotic traversal of those using a cross-floor Voronoi
graph. HOV-SG is evaluated on three distinct datasets and surpasses previous
baselines in open-vocabulary semantic accuracy on the object, room, and floor
level while producing a 75% reduction in representation size compared to dense
open-vocabulary maps. In order to prove the efficacy and generalization
capabilities of HOV-SG, we showcase successful long-horizon
language-conditioned robot navigation within real-world multi-storage
environments. We provide code and trial video data at http://hovsg.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and video are available at http://hovsg.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Language Model (GLM): A new graph-based approach to detect social
  instabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wallyson Lemes de Oliveira, Vahid Shamsaddini, Ali Ghofrani, Rahul Singh Inda, Jithendra Sai Veeramaneni, Étienne Voutaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This scientific report presents a novel methodology for the early prediction
of important political events using News datasets. The methodology leverages
natural language processing, graph theory, clique analysis, and semantic
relationships to uncover hidden predictive signals within the data. Initially,
we designed a preliminary version of the method and tested it on a few events.
This analysis revealed limitations in the initial research phase. We then
enhanced the model in two key ways: first, we added a filtration step to only
consider politically relevant news before further processing; second, we
adjusted the input features to make the alert system more sensitive to
significant spikes in the data. After finalizing the improved methodology, we
tested it on eleven events including US protests, the Ukraine war, and French
protests. Results demonstrate the superiority of our approach compared to
baseline methods. Through targeted refinements, our model can now provide
earlier and more accurate predictions of major political events based on subtle
patterns in news data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Compressed Language Models Less Subgroup Robust? <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonidas Gee, Andrea Zugarini, Novi Quadrianto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To reduce the inference cost of large language models, model compression is
increasingly used to create smaller scalable models. However, little is known
about their robustness to minority subgroups defined by the labels and
attributes of a dataset. In this paper, we investigate the effects of 18
different compression methods and settings on the subgroup robustness of BERT
language models. We show that worst-group performance does not depend on model
size alone, but also on the compression method used. Additionally, we find that
model compression does not always worsen the performance on minority subgroups.
Altogether, our analysis serves to further research into the subgroup
robustness of model compression.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding
  Model Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Hanna, Sandro Pezzelle, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent language model (LM) interpretability studies have adopted the
circuits framework, which aims to find the minimal computational subgraph, or
circuit, that explains LM behavior on a given task. Most studies determine
which edges belong in a LM's circuit by performing causal interventions on each
edge independently, but this scales poorly with model size. Edge attribution
patching (EAP), gradient-based approximation to interventions, has emerged as a
scalable but imperfect solution to this problem. In this paper, we introduce a
new method - EAP with integrated gradients (EAP-IG) - that aims to better
maintain a core property of circuits: faithfulness. A circuit is faithful if
all model edges outside the circuit can be ablated without changing the model's
performance on the task; faithfulness is what justifies studying circuits,
rather than the full model. Our experiments demonstrate that circuits found
using EAP are less faithful than those found using EAP-IG, even though both
have high node overlap with circuits found previously using causal
interventions. We conclude more generally that when using circuits to compare
the mechanisms models use to solve tasks, faithfulness, not overlap, is what
should be measured.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Text-to-Image Consistency via Automatic <span class="highlight-title">Prompt</span> Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oscar Mañas, Pietro Astolfi, Melissa Hall, Candace Ross, Jack Urbanek, Adina Williams, Aishwarya Agrawal, Adriana Romero-Soriano, Michal Drozdzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Impressive advances in text-to-image (T2I) generative models have yielded a
plethora of high performing models which are able to generate aesthetically
appealing, photorealistic images. Despite the progress, these models still
struggle to produce images that are consistent with the input prompt,
oftentimes failing to capture object quantities, relations and attributes
properly. Existing solutions to improve prompt-image consistency suffer from
the following challenges: (1) they oftentimes require model fine-tuning, (2)
they only focus on nearby prompt samples, and (3) they are affected by
unfavorable trade-offs among image quality, representation diversity, and
prompt-image consistency. In this paper, we address these challenges and
introduce a T2I optimization-by-prompting framework, OPT2I, which leverages a
large language model (LLM) to improve prompt-image consistency in T2I models.
Our framework starts from a user prompt and iteratively generates revised
prompts with the goal of maximizing a consistency score. Our extensive
validation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost
the initial consistency score by up to 24.9% in terms of DSG score while
preserving the FID and increasing the recall between generated and real data.
Our work paves the way toward building more reliable and robust T2I systems by
harnessing the power of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciNews: From Scholarly Complexities to Public Narratives -- A <span class="highlight-title">Dataset</span>
  for Scientific News Report Generation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongqi Pu, Yifan Wang, Jia Loy, Vera Demberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific news reports serve as a bridge, adeptly translating complex
research articles into reports that resonate with the broader public. The
automated generation of such narratives enhances the accessibility of scholarly
insights. In this paper, we present a new corpus to facilitate this paradigm
development. Our corpus comprises a parallel compilation of academic
publications and their corresponding scientific news reports across nine
disciplines. To demonstrate the utility and reliability of our dataset, we
conduct an extensive analysis, highlighting the divergences in readability and
brevity between scientific news narratives and academic manuscripts. We
benchmark our dataset employing state-of-the-art text generation models. The
evaluation process involves both automatic and human evaluation, which lays the
groundwork for future explorations into the automated generation of scientific
news reports. The dataset and code related to this work are available at
https://dongqi.me/projects/SciNews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024 Main Conference Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constructions Are So Difficult That Even Large Language Models Get Them
  Right for the Wrong Reasons <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijia Zhou, Leonie Weissweiler, Taiqi He, Hinrich Schütze, David R. Mortensen, Lori Levin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we make a contribution that can be understood from two
perspectives: from an NLP perspective, we introduce a small challenge dataset
for NLI with large lexical overlap, which minimises the possibility of models
discerning entailment solely based on token distinctions, and show that GPT-4
and Llama 2 fail it with strong bias. We then create further challenging
sub-tasks in an effort to explain this failure. From a Computational
Linguistics perspective, we identify a group of constructions with three
classes of adjectives which cannot be distinguished by surface features. This
enables us to probe for LLM's understanding of these constructions in various
ways, and we find that they fail in a variety of ways to distinguish between
them, suggesting that they don't adequately represent their meaning or capture
the lexical properties of phrasal heads.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can multiple-choice questions really be useful in detecting the
  abilities of LLMs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangyue Li, Liangzhi Li, Tong Xiang, Xiao Liu, Wei Deng, Noa Garcia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple-choice questions (MCQs) are widely used in the evaluation of large
language models (LLMs) due to their simplicity and efficiency. However, there
are concerns about whether MCQs can truly measure LLM's capabilities,
particularly in knowledge-intensive scenarios where long-form generation (LFG)
answers are required. The misalignment between the task and the evaluation
method demands a thoughtful analysis of MCQ's efficacy, which we undertake in
this paper by evaluating nine LLMs on four question-answering (QA) datasets in
two languages: Chinese and English. We identify a significant issue: LLMs
exhibit an order sensitivity in bilingual MCQs, favoring answers located at
specific positions, i.e., the first position. We further quantify the gap
between MCQs and long-form generation questions (LFGQs) by comparing their
direct outputs, token logits, and embeddings. Our results reveal a relatively
low correlation between answers from MCQs and LFGQs for identical questions.
Additionally, we propose two methods to quantify the consistency and confidence
of LLMs' output, which can be generalized to other QA evaluation benchmarks.
Notably, our analysis challenges the idea that the higher the consistency, the
greater the accuracy. We also find MCQs to be less reliable than LFGQs in terms
of expected calibration error. Finally, the misalignment between MCQs and LFGQs
is not only reflected in the evaluation performance but also in the embedding
space. Our code and models can be accessed at
https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UCxn: Typologically Informed Annotation of Constructions Atop Universal
  Dependencies <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonie Weissweiler, Nina Böbel, Kirian Guiller, Santiago Herrera, Wesley Scivetti, Arthur Lorenzi, Nurit Melnik, Archna Bhatia, Hinrich Schütze, Lori Levin, Amir Zeldes, Joakim Nivre, William Croft, Nathan Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Universal Dependencies (UD) project has created an invaluable collection
of treebanks with contributions in over 140 languages. However, the UD
annotations do not tell the full story. Grammatical constructions that convey
meaning through a particular combination of several morphosyntactic elements --
for example, interrogative sentences with special markers and/or word orders --
are not labeled holistically. We argue for (i) augmenting UD annotations with a
'UCxn' annotation layer for such meaning-bearing grammatical constructions, and
(ii) approaching this in a typologically informed way so that morphosyntactic
strategies can be compared across languages. As a case study, we consider five
construction families in ten languages, identifying instances of each
construction in UD treebanks through the use of morphosyntactic patterns. In
addition to findings regarding these particular constructions, our study yields
important insights on methodology for describing and identifying constructions
in language-general and language-particular ways, and lays the foundation for
future constructional enrichment of UD treebanks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Few-shot Event Detection via Hierarchical Augmentation
  Networks <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenlong Zhang, Pengfei Cao, Yubo Chen, Kang Liu, Zhiqiang Zhang, Mengshu Sun, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional continual event detection relies on abundant labeled data for
training, which is often impractical to obtain in real-world applications. In
this paper, we introduce continual few-shot event detection (CFED), a more
commonly encountered scenario when a substantial number of labeled samples are
not accessible. The CFED task is challenging as it involves memorizing previous
event types and learning new event types with few-shot samples. To mitigate
these challenges, we propose a memory-based framework: Hierarchical
Augmentation Networks (HANet). To memorize previous event types with limited
memory, we incorporate prototypical augmentation into the memory set. For the
issue of learning new event types in few-shot scenarios, we propose a
contrastive augmentation module for token representations. Despite comparing
with previous state-of-the-art methods, we also conduct comparisons with
ChatGPT. Experiment results demonstrate that our method significantly
outperforms all of these methods in multiple continual few-shot event detection
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastPerson: Enhancing Video Learning through Effective Video
  Summarization that Preserves Linguistic and Visual Contexts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Kawamura, Jun Rekimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quickly understanding lengthy lecture videos is essential for learners with
limited time and interest in various topics to improve their learning
efficiency. To this end, video summarization has been actively researched to
enable users to view only important scenes from a video. However, these studies
focus on either the visual or audio information of a video and extract
important segments in the video. Therefore, there is a risk of missing
important information when both the teacher's speech and visual information on
the blackboard or slides are important, such as in a lecture video. To tackle
this issue, we propose FastPerson, a video summarization approach that
considers both the visual and auditory information in lecture videos.
FastPerson creates summary videos by utilizing audio transcriptions along with
on-screen images and text, minimizing the risk of overlooking crucial
information for learners. Further, it provides a feature that allows learners
to switch between the summary and original videos for each chapter of the
video, enabling them to adjust the pace of learning based on their interests
and level of understanding. We conducted an evaluation with 40 participants to
assess the effectiveness of our method and confirmed that it reduced viewing
time by 53\% at the same level of comprehension as that when using traditional
video playback methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Short Text Modeling: Leveraging Large Language Models for Topic
  Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyu Chang, Rui Wang, Peng Ren, Haiping Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crafting effective topic models for brief texts, like tweets and news
headlines, is essential for capturing the swift shifts in social dynamics.
Traditional topic models, however, often fall short in accurately representing
the semantic intricacies of short texts due to their brevity and lack of
contextual data. In our study, we harness the advanced capabilities of Large
Language Models (LLMs) to introduce a novel approach termed "Topic Refinement".
This approach does not directly involve itself in the initial modeling of
topics but focuses on improving topics after they have been mined. By employing
prompt engineering, we direct LLMs to eliminate off-topic words within a given
topic, ensuring that only contextually relevant words are preserved or
substituted with ones that fit better semantically. This method emulates
human-like scrutiny and improvement of topics, thereby elevating the semantic
quality of the topics generated by various models. Our comprehensive evaluation
across three unique datasets has shown that our topic refinement approach
significantly enhances the semantic coherence of topics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not All Similarities Are Created Equal: Leveraging Data-Driven Biases to
  Inform GenAI Copyright Disputes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Hacohen, Adi Haviv, Shahar Sarfaty, Bruria Friedman, Niva Elkin-Koren, Roi Livni, Amit H Bermano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Generative Artificial Intelligence (GenAI) models, including
GitHub Copilot, OpenAI GPT, and Stable Diffusion, has revolutionized content
creation, enabling non-professionals to produce high-quality content across
various domains. This transformative technology has led to a surge of synthetic
content and sparked legal disputes over copyright infringement. To address
these challenges, this paper introduces a novel approach that leverages the
learning capacity of GenAI models for copyright legal analysis, demonstrated
with GPT2 and Stable Diffusion models. Copyright law distinguishes between
original expressions and generic ones (Sc\`enes \`a faire), protecting the
former and permitting reproduction of the latter. However, this distinction has
historically been challenging to make consistently, leading to over-protection
of copyrighted works. GenAI offers an unprecedented opportunity to enhance this
legal analysis by revealing shared patterns in preexisting works. We propose a
data-driven approach to identify the genericity of works created by GenAI,
employing "data-driven bias" to assess the genericity of expressive
compositions. This approach aids in copyright scope determination by utilizing
the capabilities of GenAI to identify and prioritize expressive elements and
rank them according to their frequency in the model's dataset. The potential
implications of measuring expressive genericity for copyright law are profound.
Such scoring could assist courts in determining copyright scope during
litigation, inform the registration practices of Copyright Offices, allowing
registration of only highly original synthetic works, and help copyright owners
signal the value of their works and facilitate fairer licensing deals. More
generally, this approach offers valuable insights to policymakers grappling
with adapting copyright law to the challenges posed by the era of GenAI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ACM CSLAW 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models for Text Classification: Is In-Context Learning Enough? <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandra Edwards, Jose Camacho-Collados
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent foundational language models have shown state-of-the-art performance
in many NLP tasks in zero- and few-shot settings. An advantage of these models
over more standard approaches based on fine-tuning is the ability to understand
instructions written in natural language (prompts), which helps them generalise
better to different tasks and domains without the need for specific training
data. This makes them suitable for addressing text classification problems for
domains with limited amounts of annotated instances. However, existing research
is limited in scale and lacks understanding of how text generation models
combined with prompting techniques compare to more established methods for text
classification such as fine-tuning masked language models. In this paper, we
address this research gap by performing a large-scale evaluation study for 16
text classification datasets covering binary, multiclass, and multilabel
problems. In particular, we compare zero- and few-shot approaches of large
language models to fine-tuning smaller language models. We also analyse the
results by prompt, classification type, domain, and number of labels. In
general, the results show how fine-tuning smaller and more efficient language
models can still outperform few-shot approaches of larger language models,
which have room for improvement when it comes to text classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intrinsic Subgraph Generation for Interpretable Graph based Visual
  Question Answering <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Tilli, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large success of deep learning based methods in Visual Question Answering
(VQA) has concurrently increased the demand for explainable methods. Most
methods in Explainable Artificial Intelligence (XAI) focus on generating
post-hoc explanations rather than taking an intrinsic approach, the latter
characterizing an interpretable model. In this work, we introduce an
interpretable approach for graph-based VQA and demonstrate competitive
performance on the GQA dataset. This approach bridges the gap between
interpretability and performance. Our model is designed to intrinsically
produce a subgraph during the question-answering process as its explanation,
providing insight into the decision making. To evaluate the quality of these
generated subgraphs, we compare them against established post-hoc
explainability methods for graph neural networks, and perform a human
evaluation. Moreover, we present quantitative metrics that correlate with the
evaluations of human assessors, acting as automatic metrics for the generated
explanatory subgraphs. Our implementation is available at
https://github.com/DigitalPhonetics/Intrinsic-Subgraph-Generation-for-VQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DANCER: Entity Description Augmented Named Entity Corrector for
  Automatic Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17645v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17645v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Cheng Wang, Hsin-Wei Wang, Bi-Cheng Yan, Chi-Han Lin, Berlin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end automatic speech recognition (E2E ASR) systems often suffer from
mistranscription of domain-specific phrases, such as named entities, sometimes
leading to catastrophic failures in downstream tasks. A family of fast and
lightweight named entity correction (NEC) models for ASR have recently been
proposed, which normally build on phonetic-level edit distance algorithms and
have shown impressive NEC performance. However, as the named entity (NE) list
grows, the problems of phonetic confusion in the NE list are exacerbated; for
example, homophone ambiguities increase substantially. In view of this, we
proposed a novel Description Augmented Named entity CorrEctoR (dubbed DANCER),
which leverages entity descriptions to provide additional information to
facilitate mitigation of phonetic confusion for NEC on ASR transcription. To
this end, an efficient entity description augmented masked language model
(EDA-MLM) comprised of a dense retrieval model is introduced, enabling MLM to
adapt swiftly to domain-specific entities for the NEC task. A series of
experiments conducted on the AISHELL-1 and Homophone datasets confirm the
effectiveness of our modeling approach. DANCER outperforms a strong baseline,
the phonetic edit-distance-based NEC model (PED-NEC), by a character error rate
(CER) reduction of about 7% relatively on AISHELL-1 for named entities. More
notably, when tested on Homophone that contain named entities of high phonetic
confusion, DANCER offers a more pronounced CER reduction of 46% relatively over
PED-NEC for named entities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REFeREE: A REference-FREE Model-Based Metric for Text Simplification <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Huang, Ekaterina Kochmar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text simplification lacks a universal standard of quality, and annotated
reference simplifications are scarce and costly. We propose to alleviate such
limitations by introducing REFeREE, a reference-free model-based metric with a
3-stage curriculum. REFeREE leverages an arbitrarily scalable pretraining stage
and can be applied to any quality standard as long as a small number of human
annotations are available. Our experiments show that our metric outperforms
existing reference-based metrics in predicting overall ratings and reaches
competitive and consistent performance in predicting specific ratings while
requiring no reference simplifications at inference time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mix-Initiative Response Generation with Dynamic Prefix Tuning <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Nie, Heyan Huang, Xian-Ling Mao, Lizi Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed initiative serves as one of the key factors in controlling conversation
directions. For a speaker, responding passively or leading proactively would
result in rather different responses. However, most dialogue systems focus on
training a holistic response generation model without any distinction among
different initiatives. It leads to the cross-contamination problem, where the
model confuses different initiatives and generates inappropriate responses.
Moreover, obtaining plenty of human annotations for initiative labels can be
expensive. To address this issue, we propose a general mix-Initiative Dynamic
Prefix Tuning framework (IDPT) to decouple different initiatives from the
generation model, which learns initiative-aware prefixes in both supervised and
unsupervised settings. Specifically, IDPT decouples initiative factors into
different prefix parameters and uses the attention mechanism to adjust the
selection of initiatives in guiding generation dynamically. The prefix
parameters can be tuned towards accurate initiative prediction as well as
mix-initiative response generation. Extensive experiments on two public
dialogue datasets show that the proposed IDPT outperforms previous baselines on
both automatic metrics and human evaluations. It also manages to generate
appropriate responses with manipulated initiatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "You are an expert annotator": Automatic Best-Worst-Scaling Annotations
  for Emotion Intensity Modeling <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Bagdon, Prathamesh Karmalker, Harsha Gurulingappa, Roman Klinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Labeling corpora constitutes a bottleneck to create models for new tasks or
domains. Large language models mitigate the issue with automatic corpus
labeling methods, particularly for categorical annotations. Some NLP tasks such
as emotion intensity prediction, however, require text regression, but there is
no work on automating annotations for continuous label assignments. Regression
is considered more challenging than classification: The fact that humans
perform worse when tasked to choose values from a rating scale lead to
comparative annotation methods, including best-worst scaling. This raises the
question if large language model-based annotation methods show similar
patterns, namely that they perform worse on rating scale annotation tasks than
on comparative annotation tasks. To study this, we automate emotion intensity
predictions and compare direct rating scale predictions, pairwise comparisons
and best-worst scaling. We find that the latter shows the highest reliability.
A transformer regressor fine-tuned on these data performs nearly on par with a
model trained on the original manual annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for publication in NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Denoising Table-Text Retrieval for Open-Domain Question Answering <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deokhyung Kang, Baikjin Jung, Yunsu Kim, Gary Geunbae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In table-text open-domain question answering, a retriever system retrieves
relevant evidence from tables and text to answer questions. Previous studies in
table-text open-domain question answering have two common challenges: firstly,
their retrievers can be affected by false-positive labels in training datasets;
secondly, they may struggle to provide appropriate evidence for questions that
require reasoning across the table. To address these issues, we propose
Denoised Table-Text Retriever (DoTTeR). Our approach involves utilizing a
denoised training dataset with fewer false positive labels by discarding
instances with lower question-relevance scores measured through a false
positive detection model. Subsequently, we integrate table-level ranking
information into the retriever to assist in finding evidence for questions that
demand reasoning across the table. To encode this ranking information, we
fine-tune a rank-aware column encoder to identify minimum and maximum values
within a column. Experimental results demonstrate that DoTTeR significantly
outperforms strong baselines on both retrieval recall and downstream QA tasks.
Our code is available at https://github.com/deokhk/DoTTeR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coimagining the Future of Voice Assistants with Cultural Sensitivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katie Seaborn, Yuto Sawa, Mizuki Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice assistants (VAs) are becoming a feature of our everyday life. Yet, the
user experience (UX) is often limited, leading to underuse, disengagement, and
abandonment. Co-designing interactions for VAs with potential end-users can be
useful. Crowdsourcing this process online and anonymously may add value.
However, most work has been done in the English-speaking West on dialogue data
sets. We must be sensitive to cultural differences in language, social
interactions, and attitudes towards technology. Our aims were to explore the
value of co-designing VAs in the non-Western context of Japan and demonstrate
the necessity of cultural sensitivity. We conducted an online elicitation study
(N = 135) where Americans (n = 64) and Japanese people (n = 71) imagined
dialogues (N = 282) and activities (N = 73) with future VAs. We discuss the
implications for coimagining interactions with future VAs, offer design
guidelines for the Japanese and English-speaking US contexts, and suggest
opportunities for cultural plurality in VA design and scholarship.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Zero-Data, Controllable, Adaptive Dialog System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dirk Väth, Lindsey Vanderlyn, Ngoc Thang Vu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Tree Search (V\"ath et al., 2023) is a recent approach to
controllable dialog systems, where domain experts shape the behavior of a
Reinforcement Learning agent through a dialog tree. The agent learns to
efficiently navigate this tree, while adapting to information needs, e.g.,
domain familiarity, of different users. However, the need for additional
training data hinders deployment in new domains. To address this, we explore
approaches to generate this data directly from dialog trees. We improve the
original approach, and show that agents trained on synthetic data can achieve
comparable dialog success to models trained on human data, both when using a
commercial Large Language Model for generation, or when using a smaller
open-source model, running on a single GPU. We further demonstrate the
scalability of our approach by collecting and testing on two new datasets:
ONBOARD, a new domain helping foreign residents moving to a new city, and the
medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and
head symptoms. Finally, we perform human testing, where no statistically
significant differences were found in either objective or subjective measures
between models trained on human and generated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-Oriented Paraphrase Analytics <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Gohsen, Matthias Hagen, Martin Potthast, Benno Stein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since paraphrasing is an ill-defined task, the term "paraphrasing" covers
text transformation tasks with different characteristics. Consequently,
existing paraphrasing studies have applied quite different (explicit and
implicit) criteria as to when a pair of texts is to be considered a paraphrase,
all of which amount to postulating a certain level of semantic or lexical
similarity. In this paper, we conduct a literature review and propose a
taxonomy to organize the 25~identified paraphrasing (sub-)tasks. Using
classifiers trained to identify the tasks that a given paraphrasing instance
fits, we find that the distributions of task-specific instances in the known
paraphrase corpora vary substantially. This means that the use of these
corpora, without the respective paraphrase conditions being clearly defined
(which is the normal case), must lead to incomparable and misleading results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ m3P: Towards Multimodal Multilingual Translation with Multimodal <span class="highlight-title">Prompt</span> <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Yang, Hongcheng Guo, Yuwei Yin, Jiaqi Bai, Bing Wang, Jiaheng Liu, Xinnian Liang, Linzheng Cahi, Liqun Yang, Zhoujun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual translation supports multiple translation directions by
projecting all languages in a shared space, but the translation quality is
undermined by the difference between languages in the text-only modality,
especially when the number of languages is large. To bridge this gap, we
introduce visual context as the universal language-independent representation
to facilitate multilingual translation. In this paper, we propose a framework
to leverage the multimodal prompt to guide the Multimodal Multilingual neural
Machine Translation (m3P), which aligns the representations of different
languages with the same meaning and generates the conditional vision-language
memory for translation. We construct a multilingual multimodal instruction
dataset (InstrMulti102) to support 102 languages. Our method aims to minimize
the representation distance of different languages by regarding the image as a
central language. Experimental results show that m3P outperforms previous
text-only baselines and multilingual multimodal methods by a large margin.
Furthermore, the probing experiments validate the effectiveness of our method
in enhancing translation under the low-resource and massively multilingual
scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RuBia: A Russian Language Bias Detection <span class="highlight-title">Dataset</span> <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Veronika Grigoreva, Anastasiia Ivanova, Ilseyar Alimova, Ekaterina Artemova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Warning: this work contains upsetting or disturbing content.
  Large language models (LLMs) tend to learn the social and cultural biases
present in the raw pre-training data. To test if an LLM's behavior is fair,
functional datasets are employed, and due to their purpose, these datasets are
highly language and culture-specific. In this paper, we address a gap in the
scope of multilingual bias evaluation by presenting a bias detection dataset
specifically designed for the Russian language, dubbed as RuBia. The RuBia
dataset is divided into 4 domains: gender, nationality, socio-economic status,
and diverse, each of the domains is further divided into multiple fine-grained
subdomains. Every example in the dataset consists of two sentences with the
first reinforcing a potentially harmful stereotype or trope and the second
contradicting it. These sentence pairs were first written by volunteers and
then validated by native-speaking crowdsourcing workers. Overall, there are
nearly 2,000 unique sentence pairs spread over 19 subdomains in RuBia. To
illustrate the dataset's purpose, we conduct a diagnostic evaluation of
state-of-the-art or near-state-of-the-art LLMs and discuss the LLMs'
predisposition to social biases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Naive Bayes-based Context Extension for Large Language Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianlin Su, Murtadha Ahmed,  Wenbo, Luo Ao, Mingren Zhu, Yunfeng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown promising in-context learning
abilities. However, conventional In-Context Learning (ICL) approaches are often
impeded by length limitations of transformer architecture, which pose
challenges when attempting to effectively integrate supervision from a
substantial number of demonstration examples. In this paper, we introduce a
novel framework, called Naive Bayes-based Context Extension (NBCE), to enable
existing LLMs to perform ICL with an increased number of demonstrations by
significantly expanding their context size. Importantly, this expansion does
not require fine-tuning or dependence on particular model architectures, all
the while preserving linear efficiency. NBCE initially splits the context into
equal-sized windows fitting the target LLM's maximum length. Then, it
introduces a voting mechanism to select the most relevant window, regarded as
the posterior context. Finally, it employs Bayes' theorem to generate the test
task. Our experimental results demonstrate that NBCE substantially enhances
performance, particularly as the number of demonstration examples increases,
consistently outperforming alternative methods. The NBCE code will be made
publicly accessible. The code NBCE is available at:
https://github.com/amurtadha/NBCE-master
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to main NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoding excellence: Mapping the demand for psychological traits of
  operations and supply chain professionals through text mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Di Luozzo, A. Fronzetti Colladon, M. M. Schiraldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current study proposes an innovative methodology for the profiling of
psychological traits of Operations Management (OM) and Supply Chain Management
(SCM) professionals. We use innovative methods and tools of text mining and
social network analysis to map the demand for relevant skills from a set of job
descriptions, with a focus on psychological characteristics. The proposed
approach aims to evaluate the market demand for specific traits by combining
relevant psychological constructs, text mining techniques, and an innovative
measure, namely, the Semantic Brand Score. We apply the proposed methodology to
a dataset of job descriptions for OM and SCM professionals, with the objective
of providing a mapping of their relevant required skills, including
psychological characteristics. In addition, the analysis is then detailed by
considering the region of the organization that issues the job description, its
organizational size, and the seniority level of the open position in order to
understand their nuances. Finally, topic modeling is used to examine key
components and their relative significance in job descriptions. By employing a
novel methodology and considering contextual factors, we provide an innovative
understanding of the attitudinal traits that differentiate professionals. This
research contributes to talent management, recruitment practices, and
professional development initiatives, since it provides new figures and
perspectives to improve the effectiveness and success of Operations Management
and Supply Chain Management professionals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Gaze-grounded Visual Question Answering <span class="highlight-title">Dataset</span> for Clarifying
  Ambiguous Japanese Questions <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Inadumi, Seiya Kawano, Akishige Yuguchi, Yasutomo Kawanishi, Koichiro Yoshino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Situated conversations, which refer to visual information as visual question
answering (VQA), often contain ambiguities caused by reliance on directive
information. This problem is exacerbated because some languages, such as
Japanese, often omit subjective or objective terms. Such ambiguities in
questions are often clarified by the contexts in conversational situations,
such as joint attention with a user or user gaze information. In this study, we
propose the Gaze-grounded VQA dataset (GazeVQA) that clarifies ambiguous
questions using gaze information by focusing on a clarification process
complemented by gaze information. We also propose a method that utilizes gaze
target estimation results to improve the accuracy of GazeVQA tasks. Our
experimental results showed that the proposed method improved the performance
in some cases of a VQA system on GazeVQA and identified some typical problems
of GazeVQA tasks that need to be improved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Are State-of-the-Art Evaluator for Grammatical
  Error Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masamune Kobayashi, Masato Mita, Mamoru Komachi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been reported to outperform existing
automatic evaluation metrics in some tasks, such as text summarization and
machine translation. However, there has been a lack of research on LLMs as
evaluators in grammatical error correction (GEC). In this study, we investigate
the performance of LLMs in GEC evaluation by employing prompts designed to
incorporate various evaluation criteria inspired by previous research. Our
extensive experimental results demonstrate that GPT-4 achieved Kendall's rank
correlation of 0.662 with human judgments, surpassing all existing methods.
Furthermore, in recent GEC evaluations, we have underscored the significance of
the LLMs scale and particularly emphasized the importance of fluency among
evaluation criteria.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ILLUMINER: Instruction-tuned Large Language Models as Few-shot Intent
  Classifier and Slot Filler <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paramita Mirza, Viju Sudhi, Soumya Ranjan Sahoo, Sinchana Ramakanth Bhat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art intent classification (IC) and slot filling (SF) methods
often rely on data-intensive deep learning models, limiting their practicality
for industry applications. Large language models on the other hand,
particularly instruction-tuned models (Instruct-LLMs), exhibit remarkable
zero-shot performance across various natural language tasks. This study
evaluates Instruct-LLMs on popular benchmark datasets for IC and SF,
emphasizing their capacity to learn from fewer examples. We introduce
ILLUMINER, an approach framing IC and SF as language generation tasks for
Instruct-LLMs, with a more efficient SF-prompting method compared to prior
work. A comprehensive comparison with multiple baselines shows that our
approach, using the FLAN-T5 11B model, outperforms the state-of-the-art joint
IC+SF method and in-context learning with GPT3.5 (175B), particularly in slot
filling by 11.1--32.2 percentage points. Additionally, our in-depth ablation
study demonstrates that parameter-efficient fine-tuning requires less than 6%
of training data to yield comparable performance with traditional full-weight
fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Logistic Regression with High-order Features for Automatic
  Grammar Rule Extraction from Treebanks <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santiago Herrera, Caio Corro, Sylvain Kahane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Descriptive grammars are highly valuable, but writing them is time-consuming
and difficult. Furthermore, while linguists typically use corpora to create
them, grammar descriptions often lack quantitative data. As for formal
grammars, they can be challenging to interpret. In this paper, we propose a new
method to extract and explore significant fine-grained grammar patterns and
potential syntactic grammar rules from treebanks, in order to create an
easy-to-understand corpus-based grammar. More specifically, we extract
descriptions and rules across different languages for two linguistic phenomena,
agreement and word order, using a large search space and paying special
attention to the ranking order of the extracted rules. For that, we use a
linear classifier to extract the most salient features that predict the
linguistic phenomena under study. We associate statistical information to each
rule, and we compare the ranking of the model's results to those of other
quantitative and statistical measures. Our method captures both well-known and
less well-known significant grammar rules in Spanish, French, and Wolof.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in LREC-Coling 2024 proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual Sentence-T5: Scalable Sentence Encoders for Multilingual
  Applications <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chihiro Yano, Akihiko Fukuchi, Shoko Fukasawa, Hideyuki Tachibana, Yotaro Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior work on multilingual sentence embedding has demonstrated that the
efficient use of natural language inference (NLI) data to build
high-performance models can outperform conventional methods. However, the
potential benefits from the recent ``exponential'' growth of language models
with billions of parameters have not yet been fully explored. In this paper, we
introduce Multilingual Sentence T5 (m-ST5), as a larger model of NLI-based
multilingual sentence embedding, by extending Sentence T5, an existing
monolingual model. By employing the low-rank adaptation (LoRA) technique, we
have achieved a successful scaling of the model's size to 5.7 billion
parameters. We conducted experiments to evaluate the performance of sentence
embedding and verified that the method outperforms the NLI-based prior
approach. Furthermore, we also have confirmed a positive correlation between
the size of the model and its performance. It was particularly noteworthy that
languages with fewer resources or those with less linguistic similarity to
English benefited more from the parameter increase. Our model is available at
https://huggingface.co/pkshatech/m-ST5.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably Secure Disambiguating Neural Linguistic Steganography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuang Qi, Kejiang Chen, Kai Zeng, Weiming Zhang, Nenghai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research in provably secure neural linguistic steganography has
overlooked a crucial aspect: the sender must detokenize stegotexts to avoid
raising suspicion from the eavesdropper. The segmentation ambiguity problem,
which arises when using language models based on subwords, leads to occasional
decoding failures in all neural language steganography implementations based on
these models. Current solutions to this issue involve altering the probability
distribution of candidate words, rendering them incompatible with provably
secure steganography. We propose a novel secure disambiguation method named
SyncPool, which effectively addresses the segmentation ambiguity problem. We
group all tokens with prefix relationships in the candidate pool before the
steganographic embedding algorithm runs to eliminate uncertainty among
ambiguous tokens. To enable the receiver to synchronize the sampling process of
the sender, a shared cryptographically-secure pseudorandom number generator
(CSPRNG) is deployed to select a token from the ambiguity pool. SyncPool does
not change the size of the candidate pool or the distribution of tokens and
thus is applicable to provably secure language steganography methods. We
provide theoretical proofs and experimentally demonstrate the applicability of
our solution to various languages and models, showing its potential to
significantly improve the reliability and security of neural linguistic
steganography systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MapGuide: A Simple yet Effective Method to Reconstruct Continuous
  Language from Brain Activities <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinpei Zhao, Jingyuan Sun, Shaonan Wang, Jing Ye, Xiaohan Zhang, Chengqing Zong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding continuous language from brain activity is a formidable yet
promising field of research. It is particularly significant for aiding people
with speech disabilities to communicate through brain signals. This field
addresses the complex task of mapping brain signals to text. The previous best
attempt reverse-engineered this process in an indirect way: it began by
learning to encode brain activity from text and then guided text generation by
aligning with predicted brain responses. In contrast, we propose a simple yet
effective method that guides text reconstruction by directly comparing them
with the predicted text embeddings mapped from brain activities. Comprehensive
experiments reveal that our method significantly outperforms the current
state-of-the-art model, showing average improvements of 77% and 54% on BLEU and
METEOR scores. We further validate the proposed modules through detailed
ablation studies and case analyses and highlight a critical correlation: the
more precisely we map brain activities to text embeddings, the better the text
reconstruction results. Such insight can simplify the task of reconstructing
language from brain activities for future work, emphasizing the importance of
improving brain-to-text-embedding mapping techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sharing the Cost of Success: A Game for Evaluating and Learning
  Collaborative Multi-Agent Instruction Giving and Following Policies <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Sadler, Sherzod Hakimov, David Schlangen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In collaborative goal-oriented settings, the participants are not only
interested in achieving a successful outcome, but do also implicitly negotiate
the effort they put into the interaction (by adapting to each other). In this
work, we propose a challenging interactive reference game that requires two
players to coordinate on vision and language observations. The learning signal
in this game is a score (given after playing) that takes into account the
achieved goal and the players' assumed efforts during the interaction. We show
that a standard Proximal Policy Optimization (PPO) setup achieves a high
success rate when bootstrapped with heuristic partner behaviors that implement
insights from the analysis of human-human interactions. And we find that a
pairing of neural partners indeed reduces the measured joint effort when
playing together repeatedly. However, we observe that in comparison to a
reasonable heuristic pairing there is still room for improvement -- which
invites further research in the direction of cost-sharing in collaborative
interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DGoT: Dynamic Graph of Thoughts for Scientific Abstract Generation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Ning, Yutong Zhao, Yitong Liu, Hongwen Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The method of training language models based on domain datasets has obtained
significant achievements in the task of generating scientific paper abstracts.
However, such models face problems of generalization and expensive training
costs. The use of large language models (LLMs) to solve the task of generating
paper abstracts saves the cost of model training. However, due to the
hallucination problem of LLM, it is often necessary to improve the reliability
of the results through multi-round query prompt approach such as Graph of
Thoughts (GoT), which also brings additional reasoning costs. In this paper, we
propose a Dynamic Graph of Thought (DGoT). It not only inherits the advantages
of the existing GoT prompt approach, but also dynamically adjust the graph
structure according to data characteristics while reducing model reasoning
cost. Experimental results show that our method's cost-effectiveness in
abstract generation tasks is only 43.7% to 56.4% of other multi-round query
prompt approaches. Our code is available at https://github.com/JayceNing/DGoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KDMCSE: Knowledge Distillation Multimodal Sentence Embeddings with
  Adaptive Angular margin Contrastive Learning <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong-Duy Nguyen, Thong Nguyen, Xiaobao Wu, Anh Tuan Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work on multimodal sentence embedding has proposed multimodal
contrastive learning and achieved promising results. However, by taking the
rest of the batch as negative samples without reviewing when forming
contrastive pairs, those studies encountered many suspicious and noisy negative
examples, significantly affecting the methods' overall performance. In this
work, we propose KDMCSE (Knowledge Distillation Multimodal contrastive learning
of Sentence Embeddings), a novel approach that enhances the discrimination and
generalizability of multimodal representation and inherits the knowledge from
the teacher model to learn the difference between positive and negative
instances and via that, can detect noisy and wrong negative samples effectively
before they are calculated in the contrastive objective. Furthermore, to
overcome the limitation of modeling the variation within negative pairs, we
introduce a new contrastive objective, AdapACSE (Adaptive Angular Margin
Supervised Contrastive Learning for Multimodal sentence embeddings), that
enhances the discriminative representation by strengthening the margin within
the angular space while capturing varying semantics within the negative.
Experimental results on widely used Semantic Textual Similarity (STS)
benchmarks demonstrate the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Exponential Smoothing into MLP: A Simple but Effective
  Sequence Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiqun Chu, Zuoquan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling long-range dependencies in sequential data is a crucial step in
sequence learning. A recently developed model, the Structured State Space (S4),
demonstrated significant effectiveness in modeling long-range sequences.
However, It is unclear whether the success of S4 can be attributed to its
intricate parameterization and HiPPO initialization or simply due to State
Space Models (SSMs). To further investigate the potential of the deep SSMs, we
start with exponential smoothing (ETS), a simple SSM, and propose a stacked
architecture by directly incorporating it into an element-wise MLP. We augment
simple ETS with additional parameters and complex field to reduce the inductive
bias. Despite increasing less than 1\% of parameters of element-wise MLP, our
models achieve comparable results to S4 on the LRA benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 tables, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust and Scalable Model Editing for Large Language Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingfa Chen, Zhengyan Zhang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Chen Chen, Kuai Li, Tao Yang, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can make predictions using parametric
knowledge--knowledge encoded in the model weights--or contextual
knowledge--knowledge presented in the context. In many scenarios, a desirable
behavior is that LLMs give precedence to contextual knowledge when it conflicts
with the parametric knowledge, and fall back to using their parametric
knowledge when the context is irrelevant. This enables updating and correcting
the model's knowledge by in-context editing instead of retraining. Previous
works have shown that LLMs are inclined to ignore contextual knowledge and fail
to reliably fall back to parametric knowledge when presented with irrelevant
context. In this work, we discover that, with proper prompting methods,
instruction-finetuned LLMs can be highly controllable by contextual knowledge
and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit
models by REading Notes) to improve the scalability and robustness of LLM
editing. To better evaluate the robustness of model editors, we collect a new
dataset, that contains irrelevant questions that are more challenging than the
ones in existing datasets. Empirical results show that our method outperforms
current state-of-the-art methods by a large margin. Unlike existing techniques,
it can integrate knowledge from multiple edits, and correctly respond to
syntactically similar but semantically unrelated inputs (and vice versa). The
source code can be found at https://github.com/thunlp/EREN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024 paper, 16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligning Large Language Models for Enhancing Psychiatric Interviews
  through Symptom Delineation and Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jae-hee So, Joonhwan Chang, Eunji Kim, Junho Na, JiYeon Choi, Jy-yong Sohn, Byung-Hoon Kim, Sang Hui Chu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have accelerated their
usage in various domains. Given the fact that psychiatric interviews are
goal-oriented and structured dialogues between the professional interviewer and
the interviewee, it is one of the most underexplored areas where LLMs can
contribute substantial value. Here, we explore the use of LLMs for enhancing
psychiatric interviews, by analyzing counseling data from North Korean
defectors with traumatic events and mental health issues. Specifically, we
investigate whether LLMs can (1) delineate the part of the conversation that
suggests psychiatric symptoms and name the symptoms, and (2) summarize
stressors and symptoms, based on the interview dialogue transcript. Here, the
transcript data was labeled by mental health experts for training and
evaluation of LLMs. Our experimental results show that appropriately prompted
LLMs can achieve high performance on both the symptom delineation task and the
summarization task. This research contributes to the nascent field of applying
LLMs to psychiatric interview and demonstrates their potential effectiveness in
aiding mental health practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LM-Combiner: A Contextual Rewriting Model for Chinese Grammatical Error
  Correction <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Baoxin Wang, Yijun Liu, Dayong Wu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over-correction is a critical problem in Chinese grammatical error correction
(CGEC) task. Recent work using model ensemble methods based on voting can
effectively mitigate over-correction and improve the precision of the GEC
system. However, these methods still require the output of several GEC systems
and inevitably lead to reduced error recall. In this light, we propose the
LM-Combiner, a rewriting model that can directly modify the over-correction of
GEC system outputs without a model ensemble. Specifically, we train the model
on an over-correction dataset constructed through the proposed K-fold cross
inference method, which allows it to directly generate filtered sentences by
combining the original and the over-corrected text. In the inference stage, we
directly take the original sentences and the output results of other systems as
input and then obtain the filtered sentences through LM-Combiner. Experiments
on the FCGEC dataset show that our proposed method effectively alleviates the
over-correction of the original system (+18.2 Precision) while ensuring the
error recall remains unchanged. Besides, we find that LM-Combiner still has a
good rewriting performance even with small parameters and few training data,
and thus can cost-effectively mitigate the over-correction of black-box GEC
systems (e.g., ChatGPT).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PCToolkit: A Unified Plug-and-Play <span class="highlight-title">Prompt</span> Compression Toolkit of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyi Li, Yihuai Lan, Lei Wang, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt compression is an innovative method for efficiently condensing input
prompts while preserving essential information. To facilitate quick-start
services, user-friendly interfaces, and compatibility with common datasets and
metrics, we present the Prompt Compression Toolkit (PCToolkit). This toolkit is
a unified plug-and-play solution for compressing prompts in Large Language
Models (LLMs), featuring cutting-edge prompt compressors, diverse datasets, and
metrics for comprehensive performance evaluation. PCToolkit boasts a modular
design, allowing for easy integration of new datasets and metrics through
portable and user-friendly interfaces. In this paper, we outline the key
components and functionalities of PCToolkit. We conducted evaluations of the
compressors within PCToolkit across various natural language tasks, including
reconstruction, summarization, mathematical problem-solving, question
answering, few-shot learning, synthetic tasks, code completion, boolean
expressions, multiple choice questions, and lies recognition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For open-source repository, see
  https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transcribing Bengali Text with Regional Dialects to IPA using District
  Guided Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S M Jishanul Islam, Sadia Ahmmed, Sahid Hossain Mustakim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate transcription of Bengali text to the International Phonetic Alphabet
(IPA) is a challenging task due to the complex phonology of the language and
context-dependent sound changes. This challenge is even more for regional
Bengali dialects due to unavailability of standardized spelling conventions for
these dialects, presence of local and foreign words popular in those regions
and phonological diversity across different regions. This paper presents an
approach to this sequence-to-sequence problem by introducing the District
Guided Tokens (DGT) technique on a new dataset spanning six districts of
Bangladesh. The key idea is to provide the model with explicit information
about the regional dialect or "district" of the input text before generating
the IPA transcription. This is achieved by prepending a district token to the
input sequence, effectively guiding the model to understand the unique phonetic
patterns associated with each district. The DGT technique is applied to
fine-tune several transformer-based models, on this new dataset. Experimental
results demonstrate the effectiveness of DGT, with the ByT5 model achieving
superior performance over word-based models like mT5, BanglaT5, and umT5. This
is attributed to ByT5's ability to handle a high percentage of
out-of-vocabulary words in the test set. The proposed approach highlights the
importance of incorporating regional dialect information into ubiquitous
natural language processing systems for languages with diverse phonological
variations. The following work was a result of the "Bhashamul" challenge, which
is dedicated to solving the problem of Bengali text with regional dialects to
IPA transcription https://www.kaggle.com/competitions/regipa/. The training and
inference notebooks are available through the competition link.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work became the champion of the Bhashamul challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity
  Recognition <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haris Riaz, Razvan-Gabriel Dumitru, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we revisit the problem of semi-supervised named entity
recognition (NER) focusing on extremely light supervision, consisting of a
lexicon containing only 10 examples per class. We introduce ELLEN, a simple,
fully modular, neuro-symbolic method that blends fine-tuned language models
with linguistic rules. These rules include insights such as ''One Sense Per
Discourse'', using a Masked Language Model as an unsupervised NER, leveraging
part-of-speech tags to identify and eliminate unlabeled entities as false
negatives, and other intuitions about classifier confidence scores in local and
global context. ELLEN achieves very strong performance on the CoNLL-2003
dataset when using the minimal supervision from the lexicon above. It also
outperforms most existing (and considerably more complex) semi-supervised NER
methods under the same supervision settings commonly used in the literature
(i.e., 5% of the training data). Further, we evaluate our CoNLL-2003 model in a
zero-shot scenario on WNUT-17 where we find that it outperforms GPT-3.5 and
achieves comparable performance to GPT-4. In a zero-shot setting, ELLEN also
achieves over 75% of the performance of a strong, fully supervised model
trained on gold data. Our code is available at:
https://github.com/hriaz17/ELLEN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chat<span class="highlight-title">GPT</span> Rates Natural Language Explanation Quality Like Humans: But on
  Which Scales? <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Huang, Haewoon Kwak, Kunwoo Park, Jisun An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As AI becomes more integral in our lives, the need for transparency and
responsibility grows. While natural language explanations (NLEs) are vital for
clarifying the reasoning behind AI decisions, evaluating them through human
judgments is complex and resource-intensive due to subjectivity and the need
for fine-grained ratings. This study explores the alignment between ChatGPT and
human assessments across multiple scales (i.e., binary, ternary, and 7-Likert
scale). We sample 300 data instances from three NLE datasets and collect 900
human annotations for both informativeness and clarity scores as the text
quality measurement. We further conduct paired comparison experiments under
different ranges of subjectivity scores, where the baseline comes from 8,346
human annotations. Our results show that ChatGPT aligns better with humans in
more coarse-grained scales. Also, paired comparisons and dynamic prompting
(i.e., providing semantically similar examples in the prompt) improve the
alignment. This research advances our understanding of large language models'
capabilities to assess the text explanation quality in different configurations
for responsible AI development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accpeted by LREC-COLING 2024 main conference, long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extracting Biomedical Entities from Noisy Audio Transcripts <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nima Ebadi, Kellen Morgan, Adrian Tan, Billy Linares, Sheri Osborn, Emma Majors, Jeremy Davis, Anthony Rios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Speech Recognition (ASR) technology is fundamental in transcribing
spoken language into text, with considerable applications in the clinical
realm, including streamlining medical transcription and integrating with
Electronic Health Record (EHR) systems. Nevertheless, challenges persist,
especially when transcriptions contain noise, leading to significant drops in
performance when Natural Language Processing (NLP) models are applied. Named
Entity Recognition (NER), an essential clinical task, is particularly affected
by such noise, often termed the ASR-NLP gap. Prior works have primarily studied
ASR's efficiency in clean recordings, leaving a research gap concerning the
performance in noisy environments. This paper introduces a novel dataset,
BioASR-NER, designed to bridge the ASR-NLP gap in the biomedical domain,
focusing on extracting adverse drug reactions and mentions of entities from the
Brief Test of Adult Cognition by Telephone (BTACT) exam. Our dataset offers a
comprehensive collection of almost 2,000 clean and noisy recordings. In
addressing the noise challenge, we present an innovative transcript-cleaning
method using GPT4, investigating both zero-shot and few-shot methodologies. Our
study further delves into an error analysis, shedding light on the types of
errors in transcription software, corrections by GPT4, and the challenges GPT4
faces. This paper aims to foster improved understanding and potential solutions
for the ASR-NLP gap, ultimately supporting enhanced healthcare documentation
practices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging Textual and Tabular Worlds for Fact Verification: A
  Lightweight, Attention-Based Model <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirin Dabbaghi Varnosfaderani, Canasai Kruengkrai, Ramin Yahyapour, Junichi Yamagishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  FEVEROUS is a benchmark and research initiative focused on fact extraction
and verification tasks involving unstructured text and structured tabular data.
In FEVEROUS, existing works often rely on extensive preprocessing and utilize
rule-based transformations of data, leading to potential context loss or
misleading encodings. This paper introduces a simple yet powerful model that
nullifies the need for modality conversion, thereby preserving the original
evidence's context. By leveraging pre-trained models on diverse text and
tabular datasets and by incorporating a lightweight attention-based mechanism,
our approach efficiently exploits latent connections between different data
types, thereby yielding comprehensive and reliable verdict predictions. The
model's modular structure adeptly manages multi-modal information, ensuring the
integrity and authenticity of the original evidence are uncompromised.
Comparative analyses reveal that our approach exhibits competitive performance,
aligning itself closely with top-tier models on the FEVEROUS benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for a presentation at LREC-COLING 2024 - The 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain-of-Action: Faithful and Multimodal Question Answering through
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17359v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17359v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Pan, Haozheng Luo, Manling Li, Han Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a Chain-of-Action (CoA) framework for multimodal and
retrieval-augmented Question-Answering (QA). Compared to the literature, CoA
overcomes two major challenges of current QA applications: (i) unfaithful
hallucination that is inconsistent with real-time or domain facts and (ii) weak
reasoning performance over compositional information. Our key contribution is a
novel reasoning-retrieval mechanism that decomposes a complex question into a
reasoning chain via systematic prompting and pre-designed actions.
Methodologically, we propose three types of domain-adaptable `Plug-and-Play'
actions for retrieving real-time information from heterogeneous sources. We
also propose a multi-reference faith score (MRFS) to verify and resolve
conflicts in the answers. Empirically, we exploit both public benchmarks and a
Web3 case study to demonstrate the capability of CoA over other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal
  Propagation Analysis for Large Language Models <span class="chip">ICLR
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Kyunggeun Lee, Jun Ma, Harris Teague
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large generative models, such as large language models (LLMs) and diffusion
models have as revolutionized the fields of NLP and computer vision
respectively. However, their slow inference, high computation and memory
requirement makes it challenging to deploy them on edge devices. In this study,
we propose a light-weight quantization aware fine tuning technique using
knowledge distillation (KD-QAT) to improve the performance of 4-bit weight
quantized LLMs using commonly available datasets to realize a popular language
use case, on device chat applications. To improve this paradigm of finetuning,
as main contributions, we provide insights into stability of KD-QAT by
empirically studying the gradient propagation during training to better
understand the vulnerabilities of KD-QAT based approaches to low-bit
quantization errors. Based on our insights, we propose ov-freeze, a simple
technique to stabilize the KD-QAT process. Finally, we experiment with the
popular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that
ov-freeze results in near float-point precision performance, i.e., less than
0.7% loss of accuracy on Commonsense Reasoning benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Practical ML for Low Resource Settings Workshop at ICLR
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as Financial Data Annotators: A Study on
  Effectiveness and Efficiency <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toyin Aguda, Suchetha Siddagangappa, Elena Kochkina, Simerjot Kaur, Dongsheng Wang, Charese Smiley, Sameena Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collecting labeled datasets in finance is challenging due to scarcity of
domain experts and higher cost of employing them. While Large Language Models
(LLMs) have demonstrated remarkable performance in data annotation tasks on
general domain datasets, their effectiveness on domain specific datasets
remains underexplored. To address this gap, we investigate the potential of
LLMs as efficient data annotators for extracting relations in financial
documents. We compare the annotations produced by three LLMs (GPT-4, PaLM 2,
and MPT Instruct) against expert annotators and crowdworkers. We demonstrate
that the current state-of-the-art LLMs can be sufficient alternatives to
non-expert crowdworkers. We analyze models using various prompts and parameter
settings and find that customizing the prompts for each relation group by
providing specific examples belonging to those groups is paramount.
Furthermore, we introduce a reliability index (LLM-RelIndex) used to identify
outputs that may require expert attention. Finally, we perform an extensive
time, cost and error analysis and provide recommendations for the collection
and usage of automated annotations in domain-specific settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Produce Responses Perceived to be Empathic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoon Kyung Lee, Jina Suh, Hongli Zhan, Junyi Jessy Li, Desmond C. Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated surprising performance on many
tasks, including writing supportive messages that display empathy. Here, we had
these models generate empathic messages in response to posts describing common
life experiences, such as workplace situations, parenting, relationships, and
other anxiety- and anger-eliciting situations. Across two studies (N=192, 202),
we showed human raters a variety of responses written by several models (GPT4
Turbo, Llama2, and Mistral), and had people rate these responses on how
empathic they seemed to be. We found that LLM-generated responses were
consistently rated as more empathic than human-written responses. Linguistic
analyses also show that these models write in distinct, predictable ``styles",
in terms of their use of punctuation, emojis, and certain words. These results
highlight the potential of using LLMs to enhance human peer support in contexts
where empathy is important.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Juru: Legal Brazilian Large Language Model from Reputable Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roseval Malaquias Junior, Ramon Pires, Roseli Romero, Rodrigo Nogueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high computational cost associated with pretraining large language models
limits their research. Two strategies have emerged to address this issue:
domain specialization and pretraining with high-quality data. To explore these
strategies, we specialized the Sabi\'a-2 Small model with 1.9 billion unique
tokens from reputable Brazilian legal sources and conducted few-shot
evaluations on legal and general knowledge exams. Our model, Juru, demonstrates
the benefits of domain specialization with a reduced amount of pretraining
data. However, this specialization comes at the expense of degrading
performance in other knowledge areas within the same language. This study
contributes to the growing body of scientific evidence showing that pretraining
data selection may enhance the performance of large language models, enabling
the exploration of these models at a lower cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based
  on Twitter Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijeta Deshpande, Minhwa Lee, Zonghai Yao, Zihao Zhang, Jason Brian Gibbons, Hong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research on Twitter (now X) data has provided positive evidence of its
utility in developing supplementary health surveillance systems. In this study,
we present a new framework to surveil public health, focusing on mental health
(MH) outcomes. We hypothesize that locally posted tweets are indicative of
local MH outcomes and collect tweets posted from 765 neighborhoods (census
block groups) in the USA. We pair these tweets from each neighborhood with the
corresponding MH outcome reported by the Center for Disease Control (CDC) to
create a benchmark dataset, LocalTweets. With LocalTweets, we present the first
population-level evaluation task for Twitter-based MH surveillance systems. We
then develop an efficient and effective method, LocalHealth, for predicting MH
outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the
highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\%
improvement in F1-score over the GPT3.5 in zero-shot setting. We also utilize
LocalHealth to extrapolate CDC's estimates to proxy unreported neighborhoods,
achieving an F1-score of 0.7291. Our work suggests that Twitter data can be
effectively leveraged to simulate neighborhood-level MH outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple and Scalable Strategies to Continually <span class="highlight-title">Pre-train</span> Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08763v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08763v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, Irina Rish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are routinely pre-trained on billions of tokens,
only to start the process over again once new data becomes available. A much
more efficient solution is to continually pre-train these models, saving
significant compute compared to re-training. However, the distribution shift
induced by new data typically results in degraded performance on previous data
or poor adaptation to the new data. In this work, we show that a simple and
scalable combination of learning rate (LR) re-warming, LR re-decaying, and
replay of previous data is sufficient to match the performance of fully
re-training from scratch on all available data, as measured by the final loss
and the average score on several language model (LM) evaluation benchmarks.
Specifically, we show this for a weak but realistic distribution shift between
two commonly used LLM pre-training datasets (English$\rightarrow$English) and a
stronger distribution shift (English$\rightarrow$German) at the $405$M
parameter model scale with large dataset sizes (hundreds of billions of
tokens). Selecting the weak but realistic shift for larger-scale experiments,
we also find that our continual learning strategies match the re-training
baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be
successfully updated via simple and scalable continual learning strategies,
matching the re-training baseline using only a fraction of the compute.
Finally, inspired by previous work, we propose alternatives to the cosine
learning rate schedule that help circumvent forgetting induced by LR re-warming
and that are not bound to a fixed token budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Offer an Alternative to the Traditional Approach
  of Topic Modelling <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Mu, Chun Dong, Kalina Bontcheva, Xingyi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modelling, as a well-established unsupervised technique, has found
extensive use in automatically detecting significant topics within a corpus of
documents. However, classic topic modelling approaches (e.g., LDA) have certain
drawbacks, such as the lack of semantic understanding and the presence of
overlapping topics. In this work, we investigate the untapped potential of
large language models (LLMs) as an alternative for uncovering the underlying
topics within extensive text corpora. To this end, we introduce a framework
that prompts LLMs to generate topics from a given set of documents and
establish evaluation protocols to assess the clustering efficacy of LLMs. Our
findings indicate that LLMs with appropriate prompts can stand out as a viable
alternative, capable of generating relevant topic titles and adhering to human
guidelines to refine and merge topics. Through in-depth experiments and
evaluation, we summarise the advantages and constraints of employing LLMs in
topic extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI and Generative AI for Research Discovery and Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Glickman, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI and generative AI tools, including chatbots like ChatGPT that rely on
large language models (LLMs), have burst onto the scene this year, creating
incredible opportunities to increase work productivity and improve our lives.
Statisticians and data scientists have begun experiencing the benefits from the
availability of these tools in numerous ways, such as the generation of
programming code from text prompts to analyze data or fit statistical models.
One area that these tools can make a substantial impact is in research
discovery and summarization. Standalone tools and plugins to chatbots are being
developed that allow researchers to more quickly find relevant literature than
pre-2023 search tools. Furthermore, generative AI tools have improved to the
point where they can summarize and extract the key points from research
articles in succinct language. Finally, chatbots based on highly parameterized
LLMs can be used to simulate abductive reasoning, which provides researchers
the ability to make connections among related technical topics, which can also
be used for research discovery. We review the developments in AI and generative
AI for research discovery and summarization, and propose directions where these
types of tools are likely to head in the future that may be of interest to
statistician and data scientists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generator-Retriever-Generator Approach for Open-Domain Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.11278v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.11278v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-domain question answering (QA) tasks usually require the retrieval of
relevant information from a large corpus to generate accurate answers. We
propose a novel approach called Generator-Retriever-Generator (GRG) that
combines document retrieval techniques with a large language model (LLM), by
first prompting the model to generate contextual documents based on a given
question. In parallel, a dual-encoder network retrieves documents that are
relevant to the question from an external corpus. The generated and retrieved
documents are then passed to the second LLM, which generates the final answer.
By combining document retrieval and LLM generation, our approach addresses the
challenges of open-domain QA, such as generating informative and contextually
relevant answers. GRG outperforms the state-of-the-art generate-then-read and
retrieve-then-read pipelines (GENREAD and RFiD) improving their performance by
at least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets,
respectively. We provide code, datasets, and checkpoints at
https://github.com/abdoelsayed2016/GRG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for AI policy act, if designed by the governments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMuRD: Annotated Arabic-English Receipt <span class="highlight-title">Dataset</span> for Key Information
  Extraction and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Mahmoud Abdalla, Mohamed Elkasaby, Yasser Elbendary, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extraction of key information from receipts is a complex task that
involves the recognition and extraction of text from scanned receipts. This
process is crucial as it enables the retrieval of essential content and
organizing it into structured documents for easy access and analysis. In this
paper, we present AMuRD, a novel multilingual human-annotated dataset
specifically designed for information extraction from receipts. This dataset
comprises $47,720$ samples and addresses the key challenges in information
extraction and item classification - the two critical aspects of data analysis
in the retail industry. Each sample includes annotations for item names and
attributes such as price, brand, and more. This detailed annotation facilitates
a comprehensive understanding of each item on the receipt. Furthermore, the
dataset provides classification into $44$ distinct product categories. This
classification feature allows for a more organized and efficient analysis of
the items, enhancing the usability of the dataset for various applications. In
our study, we evaluated various language model architectures, e.g., by
fine-tuning LLaMA models on the AMuRD dataset. Our approach yielded exceptional
results, with an F1 score of 97.43\% and accuracy of 94.99\% in information
extraction and classification, and an even higher F1 score of 98.51\% and
accuracy of 97.06\% observed in specific tasks. The dataset and code are
publicly accessible for further
researchhttps://github.com/Update-For-Integrated-Business-AI/AMuRD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training <span class="highlight-title">BERT</span> Models to Carry Over a Coding System Developed on One
  Corpus to Another <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dalma Galambos, Pál Zsámboki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes how we train BERT models to carry over a coding system
developed on the paragraphs of a Hungarian literary journal to another. The aim
of the coding system is to track trends in the perception of literary
translation around the political transformation in 1989 in Hungary. To evaluate
not only task performance but also the consistence of the annotation, moreover,
to get better predictions from an ensemble, we use 10-fold crossvalidation.
Extensive hyperparameter tuning is used to obtain the best possible results and
fair comparisons. To handle label imbalance, we use loss functions and metrics
robust to it. Evaluation of the effect of domain shift is carried out by
sampling a test set from the target domain. We establish the sample size by
estimating the bootstrapped confidence interval via simulations. This way, we
show that our models can carry over one annotation system to the target domain.
Comparisons are drawn to provide insights such as learning multilabel
correlations and confidence penalty improve resistance to domain shift, and
domain adaptation on OCR-ed text on another domain improves performance almost
to the same extent as that on the corpus under study. See our code at
https://codeberg.org/zsamboki/bert-annotator-ensemble.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version, to be presented at the 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient <span class="highlight-title">Pre-train</span>ing for Localized Instruction Generation of Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Batra, Davide Moltisanti, Laura Sevilla-Lara, Marcus Rohrbach, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural videos show step-by-step demonstrations of tasks like recipe
preparation. Understanding such videos is challenging, involving the precise
localization of steps and the generation of textual instructions. Manually
annotating steps and writing instructions is costly, which limits the size of
current datasets and hinders effective learning. Leveraging large but noisy
video-transcript datasets for pre-training can boost performance, but demands
significant computational resources. Furthermore, transcripts contain
irrelevant content and exhibit style variation compared to instructions written
by human annotators. To mitigate both issues, we propose a technique,
Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters
irrelevant transcripts and (ii) Swap enhances the quality of the text
instruction by automatically replacing the transcripts with human-written
instructions from a text-only recipe dataset. The curated dataset, three orders
of magnitude smaller than current web-scale datasets, enables efficient
training of large-scale models with competitive performance. We complement our
Sieve-\&-Swap approach with a Procedure Transformer (ProcX) for end-to-end step
localization and instruction generation for procedural videos. When this model
is pre-trained on our curated dataset, it achieves state-of-the-art performance
in zero-shot and finetuning settings on YouCook2 and Tasty, while using a
fraction of the computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has some missing experiments and elaborative technical
  details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blinded by Generated Contexts: How Language Models Merge Generated and
  Retrieved Contexts for Open-Domain QA? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11911v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11911v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While auxiliary information has become a key to enhancing Large Language
Models (LLMs), relatively little is known about how LLMs merge these contexts,
specifically contexts generated by LLMs and those retrieved from external
sources. To investigate this, we formulate a systematic framework to identify
whether LLMs' responses, derived from the integration of generated and
retrieved contexts, are attributed to either generated or retrieved contexts.
To easily trace the origin of the response, we construct datasets with
conflicting contexts, i.e., each question is paired with both generated and
retrieved contexts, yet only one of them contains the correct answer. Our
experiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to
favor generated contexts, even when they provide incorrect information. We
further identify two key factors contributing to this bias: i) contexts
generated by LLMs typically show greater similarity to the questions,
increasing their likelihood of being selected; ii) the segmentation process
used in retrieved contexts disrupts their completeness, thereby hindering their
full utilization in LLMs. Our analysis enhances the understanding of how LLMs
merge diverse contexts, offering valuable insights for advancing current
augmentation methods for LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Entrainment in Spontaneous Code-switched Speech <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07703v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07703v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debasmita Bhattacharya, Siying Ding, Alayna Nguyen, Julia Hirschberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well-known that speakers who entrain to one another have more
successful conversations than those who do not. Previous research has shown
that interlocutors entrain on linguistic features in both written and spoken
monolingual domains. More recent work on code-switched communication has also
shown preliminary evidence of entrainment on certain aspects of code-switching
(CSW). However, such studies of entrainment in code-switched domains have been
extremely few and restricted to human-machine textual interactions. Our work
studies code-switched spontaneous speech between humans, finding that (1)
patterns of written and spoken entrainment in monolingual settings largely
generalize to code-switched settings, and (2) some patterns of entrainment on
code-switching in dialogue agent-generated text generalize to spontaneous
code-switched speech. Our findings give rise to important implications for the
potentially "universal" nature of entrainment as a communication phenomenon,
and potential applications in inclusive and interactive speech technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Edits: camera-ready manuscript for NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decode Neural signal as Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqian Yang, Yiqun Duan, Qiang Zhang, Renjing Xu, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding language from brain dynamics is an important open direction in the
realm of brain-computer interface (BCI), especially considering the rapid
growth of large language models. Compared to invasive-based signals which
require electrode implantation surgery, non-invasive neural signals (e.g. EEG,
MEG) have attracted increasing attention considering their safety and
generality. However, the exploration is not adequate in three aspects: 1)
previous methods mainly focus on EEG but none of the previous works address
this problem on MEG with better signal quality; 2) prior works have
predominantly used ``teacher-forcing" during generative decoding, which is
impractical; 3) prior works are mostly ``BART-based" not fully auto-regressive,
which performs better in other sequence tasks. In this paper, we explore the
brain-to-text translation of MEG signals in a speech-decoding formation. Here
we are the first to investigate a cross-attention-based ``whisper" model for
generating text directly from MEG signals without teacher forcing. Our model
achieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining \&
teacher-forcing on two major datasets (\textit{GWilliams} and
\textit{Schoffelen}). This paper conducts a comprehensive review to understand
how speech decoding formation performs on the neural decoding tasks, including
pretraining initialization, training \& evaluation set splitting, augmentation,
and scaling law.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Semantic Reconstruction to Mitigate Hallucinations in
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minchan Kim, Minyeong Kim, Junik Bae, Suhwan Choi, Sungkyung Kim, Buru Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in vision-language models pose a significant challenge to
their reliability, particularly in the generation of long captions. Current
methods fall short of accurately identifying and mitigating these
hallucinations. To address this issue, we introduce ESREAL, a novel
unsupervised learning framework designed to suppress the generation of
hallucinations through accurate localization and penalization of hallucinated
tokens. Initially, ESREAL creates a reconstructed image based on the generated
caption and aligns its corresponding regions with those of the original image.
This semantic reconstruction aids in identifying both the presence and type of
token-level hallucinations within the generated caption. Subsequently, ESREAL
computes token-level hallucination scores by assessing the semantic similarity
of aligned regions based on the type of hallucination. Finally, ESREAL employs
a proximal policy optimization algorithm, where it selectively penalizes
hallucinated tokens according to their token-level hallucination scores. Our
framework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2
by 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved
solely through signals derived from the image itself, without the need for any
image-text pairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Scientific Discovery with Generative Knowledge Extraction,
  Graph-Based Representation, and Multimodal Intelligent Graph Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11996v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11996v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus J. Buehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging generative Artificial Intelligence (AI), we have transformed a
dataset comprising 1,000 scientific papers into an ontological knowledge graph.
Through an in-depth structural analysis, we have calculated node degrees,
identified communities and connectivities, and evaluated clustering
coefficients and betweenness centrality of pivotal nodes, uncovering
fascinating knowledge architectures. The graph has an inherently scale-free
nature, is highly connected, and can be used for graph reasoning by taking
advantage of transitive and isomorphic properties that reveal unprecedented
interdisciplinary relationships that can be used to answer queries, identify
gaps in knowledge, propose never-before-seen material designs, and predict
material behaviors. We compute deep node embeddings for combinatorial node
similarity ranking for use in a path sampling strategy links dissimilar
concepts that have previously not been related. One comparison revealed
structural parallels between biological materials and Beethoven's 9th Symphony,
highlighting shared patterns of complexity through isomorphic mapping. In
another example, the algorithm proposed a hierarchical mycelium-based composite
based on integrating path sampling with principles extracted from Kandinsky's
'Composition VII' painting. The resulting material integrates an innovative set
of concepts that include a balance of chaos/order, adjustable porosity,
mechanical strength, and complex patterned chemical functionalization. We
uncover other isomorphisms across science, technology and art, revealing a
nuanced ontology of immanence that reveal a context-dependent heterarchical
interplay of constituents. Graph-based generative AI achieves a far higher
degree of novelty, explorative capacity, and technical detail, than
conventional approaches and establishes a widely useful framework for
innovation by revealing hidden connections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Pitfalls of Knowledge Editing for Large Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02129v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02129v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there's
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code and data are available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched
  with Linguistic and Genre Annotation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12721v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12721v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikola Ljubešić, Taja Kuzman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a collection of highly comparable web corpora of
Slovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian,
covering thereby the whole spectrum of official languages in the South Slavic
language space. The collection of these corpora comprises a total of 13 billion
tokens of texts from 26 million documents. The comparability of the corpora is
ensured by a comparable crawling setup and the usage of identical crawling and
post-processing technology. All the corpora were linguistically annotated with
the state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and
enriched with document-level genre information via the Transformer-based
multilingual X-GENRE classifier, which further enhances comparability at the
level of linguistic annotation and metadata enrichment. The genre-focused
analysis of the resulting corpora shows a rather consistent distribution of
genres throughout the seven corpora, with variations in the most prominent
genre categories being well-explained by the economic strength of each language
community. A comparison of the distribution of genre categories across the
corpora indicates that web corpora from less developed countries primarily
consist of news articles. Conversely, web corpora from economically more
developed countries exhibit a smaller proportion of news content, with a
greater presence of promotional and opinionated texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the LREC-COLING 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing the Emergent Cognitive Synergy in Large Language Models: A
  Task-Solving Agent through Multi-Persona Self-Collaboration <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05300v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05300v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human intelligence thrives on cognitive synergy, where collaboration among
different minds yield superior outcomes compared to isolated individuals. In
this work, we propose Solo Performance Prompting (SPP), which transforms a
single LLM into a cognitive synergist by engaging in multi-turn
self-collaboration with multiple personas. A cognitive synergist is an
intelligent agent that collaboratively combines multiple minds' strengths and
knowledge to enhance problem-solving in complex tasks. By dynamically
identifying and simulating different personas based on task inputs, SPP
unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis
shows that assigning multiple fine-grained personas in LLMs improves
problem-solving abilities compared to using a single or fixed number of
personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,
Codenames Collaborative, and Logic Grid Puzzle, encompassing both
knowledge-intensive and reasoning-intensive types. Unlike previous works, such
as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,
experimental results demonstrate that SPP effectively reduces factual
hallucination, and maintains strong reasoning capabilities. Additionally,
comparative experiments show that cognitive synergy only emerges in GPT-4 and
does not appear in less capable models, such as GPT-3.5-turbo and
Llama2-13b-chat, which draws an interesting analogy to human development. Code,
data, and prompts can be found at:
https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a main conference paper at NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Representational Disparities Between Multilingual and
  Bilingual Translation Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14230v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14230v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neha Verma, Kenton Murray, Kevin Duh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual machine translation has proven immensely useful for both
parameter efficiency and overall performance across many language pairs via
complete multilingual parameter sharing. However, some language pairs in
multilingual models can see worse performance than in bilingual models,
especially in the one-to-many translation setting. Motivated by their empirical
differences, we examine the geometric differences in representations from
bilingual models versus those from one-to-many multilingual models.
Specifically, we compute the isotropy of these representations using intrinsic
dimensionality and IsoScore, in order to measure how the representations
utilize the dimensions in their underlying vector space. Using the same
evaluation data in both models, we find that for a given language pair, its
multilingual model decoder representations are consistently less isotropic and
occupy fewer dimensions than comparable bilingual model decoder
representations. Additionally, we show that much of the anisotropy in
multilingual decoder representations can be attributed to modeling
language-specific information, therefore limiting remaining representational
capacity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FENICE: Factuality Evaluation of summarization based on Natural language
  Inference and Claim Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Scirè, Karim Ghonim, Roberto Navigli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text summarization, particularly with the advent of
Large Language Models (LLMs), have shown remarkable performance. However, a
notable challenge persists as a substantial number of automatically-generated
summaries exhibit factual inconsistencies, such as hallucinations. In response
to this issue, various approaches for the evaluation of consistency for
summarization have emerged. Yet, these newly-introduced metrics face several
limitations, including lack of interpretability, focus on short document
summaries (e.g., news articles), and computational impracticality, especially
for LLM-based metrics. To address these shortcomings, we propose Factuality
Evaluation of summarization based on Natural language Inference and Claim
Extraction (FENICE), a more interpretable and efficient factuality-oriented
metric. FENICE leverages an NLI-based alignment between information in the
source document and a set of atomic facts, referred to as claims, extracted
from the summary. Our metric sets a new state of the art on AGGREFACT, the
de-facto benchmark for factuality evaluation. Moreover, we extend our
evaluation to a more challenging setting by conducting a human annotation
process of long-form summarization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using <span class="highlight-title">Pre-train</span>ed Language
  Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EthioLLM: Multilingual Large Language Models for Ethiopian Languages
  with Task Evaluation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13737v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13737v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atnafu Lambebo Tonja, Israel Abebe Azime, Tadesse Destaw Belay, Mesay Gemeda Yigezu, Moges Ahmed Mehamed, Abinew Ali Ayele, Ebrahim Chekol Jibril, Michael Melese Woldeyohannis, Olga Kolesnikova, Philipp Slusallek, Dietrich Klakow, Shengwu Xiong, Seid Muhie Yimam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have gained popularity recently due to their
outstanding performance in various downstream Natural Language Processing (NLP)
tasks. However, low-resource languages are still lagging behind current
state-of-the-art (SOTA) developments in the field of NLP due to insufficient
resources to train LLMs. Ethiopian languages exhibit remarkable linguistic
diversity, encompassing a wide array of scripts, and are imbued with profound
religious and cultural significance. This paper introduces EthioLLM --
multilingual large language models for five Ethiopian languages (Amharic,
Ge'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- a
new benchmark dataset for various downstream NLP tasks. We evaluate the
performance of these models across five downstream NLP tasks. We open-source
our multilingual language models, new benchmark datasets for various downstream
tasks, and task-specific fine-tuned language models and discuss the performance
of the models. Our dataset and models are available at the
https://huggingface.co/EthioNLP repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-Coling 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Design Space for Intelligent and Interactive Writing Assistants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David Zhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti Dutta, Jin L. C. Guo, Md Naimul Hoque, Yewon Kim, Simon Knight, Seyed Parsa Neshaei, Agnia Sergeyuk, Antonette Shibani, Disha Shrivastava, Lila Shroff, Jessi Stark, Sarah Sterman, Sitong Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy Pea, Eugenia H. Rho, Shannon Zejiang Shen, Pao Siangliulue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In our era of rapid technological advancement, the research landscape for
writing assistants has become increasingly fragmented across various research
communities. We seek to address this challenge by proposing a design space as a
structured way to examine and explore the multidimensional space of intelligent
and interactive writing assistants. Through a large community collaboration, we
explore five aspects of writing assistants: task, user, technology,
interaction, and ecosystem. Within each aspect, we define dimensions (i.e.,
fundamental components of an aspect) and codes (i.e., potential options for
each dimension) by systematically reviewing 115 papers. Our design space aims
to offer researchers and designers a practical tool to navigate, comprehend,
and compare the various possibilities of writing assistants, and aid in the
envisioning and design of new writing assistants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BAN-PL: a Novel Polish <span class="highlight-title">Dataset</span> of Banned Harmful and Offensive Content
  from Wykop.pl web service <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10592v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10592v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Kołos, Inez Okulska, Kinga Głąbińska, Agnieszka Karlińska, Emilia Wiśnios, Paweł Ellerik, Andrzej Prałat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the Internet is flooded with hate, it is one of the main tasks for NLP
experts to master automated online content moderation. However, advancements in
this field require improved access to publicly available accurate and
non-synthetic datasets of social media content. For the Polish language, such
resources are very limited. In this paper, we address this gap by presenting a
new open dataset of offensive social media content for the Polish language. The
dataset comprises content from Wykop.pl, a popular online service often
referred to as the "Polish Reddit", reported by users and banned in the
internal moderation process. It contains a total of 691,662 posts and comments,
evenly divided into two categories: "harmful" and "neutral" ("non-harmful").
The anonymized subset of the BAN-PL dataset consisting on 24,000 pieces (12,000
for each class), along with preprocessing scripts have been made publicly
available. Furthermore the paper offers valuable insights into real-life
content moderation processes and delves into an analysis of linguistic features
and content characteristics of the dataset. Moreover, a comprehensive
anonymization procedure has been meticulously described and applied. The
prevalent biases encountered in similar datasets, including post-moderation and
pre-selection biases, are also discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for LREC-COLING 2024 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyacinth6B: A large language model for Traditional Chinese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13334v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13334v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Wei Song, Yin-Te Tsai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research's primary motivation of this study is to address the high
hardware and computational demands typically associated with LLMs.Therefore,our
goal is to find a balance between model lightness and performance,striving to
maximize performance while using a comparatively lightweight model. Hyacinth6B
was developed with this objective in mind,aiming to fully leverage the core
capabilities of LLMs without incurring substantial resource costs, effectively
pushing the boundaries of smaller model's performance. The training approach
involves parameter efficient finetuning using the LoRA method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Model for Multi-objective Evolutionary Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12541v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12541v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fei Liu, Xi Lin, Zhenkun Wang, Shunyu Yao, Xialiang Tong, Mingxuan Yuan, Qingfu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiobjective evolutionary algorithms (MOEAs) are major methods for solving
multiobjective optimization problems (MOPs). Many MOEAs have been proposed in
the past decades, of which the search operators need a carefully handcrafted
design with domain knowledge. Recently, some attempts have been made to replace
the manually designed operators in MOEAs with learning-based operators (e.g.,
neural network models). However, much effort is still required for designing
and training such models, and the learned operators might not generalize well
on new problems. To tackle the above challenges, this work investigates a novel
approach that leverages the powerful large language model (LLM) to design MOEA
operators. With proper prompt engineering, we successfully let a general LLM
serve as a black-box search operator for decomposition-based MOEA (MOEA/D) in a
zero-shot manner. In addition, by learning from the LLM behavior, we further
design an explicit white-box operator with randomness and propose a new version
of decomposition-based MOEA, termed MOEA/D-LO. Experimental studies on
different test benchmarks show that our proposed method can achieve competitive
performance with widely used MOEAs. It is also promising to see the operator
only learned from a few instances can have robust generalization performance on
unseen problems with quite different patterns and settings. The results reveal
the potential benefits of using pre-trained LLMs in the design of MOEAs.To
foster reproducibility and accessibility, the source code is
https://github.com/FeiLiu36/LLM4MOEA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COPR: Continual Learning Human Preference through Optimal Policy
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15694v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15694v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhang, Lin Gui, Yuanzhao Zhai, Hui Wang, Yu Lei, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The technique of Reinforcement Learning from Human Feedback (RLHF) is a
commonly employed method to improve pre-trained Language Models (LM), enhancing
their ability to conform to human preferences. Nevertheless, the current
RLHF-based LMs necessitate full retraining each time novel queries or feedback
are introduced, which becomes a challenging task because human preferences can
vary between different domains or tasks. Retraining LMs poses practical
difficulties in many real-world situations due to the significant time and
computational resources required, along with concerns related to data privacy.
To address this limitation, we propose a new method called Continual Optimal
Policy Regularization (COPR), in which we compute the distribution of optimal
policy bypassing the partition function and then regularize the current policy
based on the historically optimal distribution to mitigate Catastrophic
Forgetting (CF). COPR involves a single learning phase and doesn't necessitate
complex reinforcement learning. Importantly, it shares the capability with RLHF
to learn from unlabeled data by maintaining a scoring module, similar to reward
model, making it flexible for continually learning without human feedback. Our
experimental results show that COPR outperforms strong Continuous Learning (CL)
baselines when it comes to consistently aligning with human preferences on
incremental tasks and domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Topic Segmentation and Outline Generation in Chinese Texts:
  The Paragraph-level Topic Representation, Corpus, and Benchmark <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Jiang, Weihao Liu, Xiaomin Chu, Peifeng Li, Qiaoming Zhu, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic segmentation and outline generation strive to divide a document into
coherent topic sections and generate corresponding subheadings, unveiling the
discourse topic structure of a document. Compared with sentence-level topic
structure, the paragraph-level topic structure can quickly grasp and understand
the overall context of the document from a higher level, benefitting many
downstream tasks such as summarization, discourse parsing, and information
retrieval. However, the lack of large-scale, high-quality Chinese
paragraph-level topic structure corpora restrained relative research and
applications. To fill this gap, we build the Chinese paragraph-level topic
representation, corpus, and benchmark in this paper. Firstly, we propose a
hierarchical paragraph-level topic structure representation with three layers
to guide the corpus construction. Then, we employ a two-stage man-machine
collaborative annotation method to construct the largest Chinese
Paragraph-level Topic Structure corpus (CPTS), achieving high quality. We also
build several strong baselines, including ChatGPT, to validate the
computability of CPTS on two fundamental tasks (topic segmentation and outline
generation) and preliminarily verified its usefulness for the downstream task
(discourse parsing).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spanish Resource Grammar version 2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olga Zamaraeva, Lorena S. Allegue, Carlos Gómez-Rodríguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the latest version of the Spanish Resource Grammar (SRG), a
grammar of Spanish implemented in the HPSG formalism. Such grammars encode a
complex set of hypotheses about syntax making them a resource for empirical
testing of linguistic theory. They also encode a strict notion of
grammaticality which makes them a resource for natural language processing
applications in computer-assisted language learning. This version of the SRG
uses the recent version of the Freeling morphological analyzer and is released
along with an automatically created, manually verified treebank of 2,291
sentences. We explain the treebanking process, emphasizing how it is different
from treebanking with manual annotation and how it contributes to
empirically-driven development of syntactic theory. The treebanks' high level
of consistency and detail makes them a resource for training high-quality
semantic parsers and generally systems that benefit from precise and detailed
semantics. Finally, we present the grammar's coverage and overgeneration on 100
sentences from a learner corpus, a new research line related to developing
methodologies for robust empirical evaluation of hypotheses in second language
acquisition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Motion Generation from Fine-grained Textual Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunhang Li, Yansong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of text2motion is to generate human motion sequences from given
textual descriptions, where the model explores diverse mappings from natural
language instructions to human body movements. While most existing works are
confined to coarse-grained motion descriptions, e.g., "A man squats.",
fine-grained descriptions specifying movements of relevant body parts are
barely explored. Models trained with coarse-grained texts may not be able to
learn mappings from fine-grained motion-related words to motion primitives,
resulting in the failure to generate motions from unseen descriptions. In this
paper, we build a large-scale language-motion dataset specializing in
fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with
step-by-step instructions with pseudo-code compulsory checks. Accordingly, we
design a new text2motion model, FineMotionDiffuse, making full use of
fine-grained textual information. Our quantitative evaluation shows that
FineMotionDiffuse trained on FineHumanML3D improves FID by a large margin of
0.38, compared with competitive baselines. According to the qualitative
evaluation and case study, our model outperforms MotionDiffuse in generating
spatially or chronologically composite motions, by learning the implicit
mappings from fine-grained descriptions to the corresponding basic motions. We
release our data at https://github.com/KunhangL/finemotiondiffuse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tandem <span class="highlight-title">Transformer</span>s for Inference Efficient LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08644v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08644v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya P S, Pranav Ajit Nair, Yashas Samaga, Toby Boyd, Sanjiv Kumar, Prateek Jain, Praneeth Netrapalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The autoregressive nature of conventional large language models (LLMs)
inherently limits inference speed, as tokens are generated sequentially. While
speculative and parallel decoding techniques attempt to mitigate this, they
face limitations: either relying on less accurate smaller models for generation
or failing to fully leverage the base LLM's representations.
  We introduce a novel architecture, Tandem transformers, to address these
issues. This architecture uniquely combines (1) a small autoregressive model
and (2) a large model operating in block mode (processing multiple tokens
simultaneously). The small model's predictive accuracy is substantially
enhanced by granting it attention to the large model's richer representations.
On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko
demonstrates a 3.3% improvement in next-token prediction accuracy over a
standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter
model with comparable downstream performance. We further incorporate the tandem
model within the speculative decoding (SPEED) framework where the large model
validates tokens from the small model. This ensures that the Tandem of
PaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster
than using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream
task accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multimodal Approach to Device-Directed Speech Detection with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Wagner, Alexander Churchill, Siddharth Sigtia, Panayiotis Georgiou, Matt Mirsamadi, Aarshee Mishra, Erik Marchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactions with virtual assistants typically start with a predefined
trigger phrase followed by the user command. To make interactions with the
assistant more intuitive, we explore whether it is feasible to drop the
requirement that users must begin each command with a trigger phrase. We
explore this task in three ways: First, we train classifiers using only
acoustic information obtained from the audio waveform. Second, we take the
decoder outputs of an automatic speech recognition (ASR) system, such as 1-best
hypotheses, as input features to a large language model (LLM). Finally, we
explore a multimodal system that combines acoustic and lexical features, as
well as ASR decoder signals in an LLM. Using multimodal information yields
relative equal-error-rate improvements over text-only and audio-only models of
up to 39% and 61%. Increasing the size of the LLM and training with low-rank
adaption leads to further relative EER reductions of up to 18% on our dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2312.03632</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deciphering the Impact of <span class="highlight-title">Pretrain</span>ing Data on Large Language Models
  through Machine Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhao, Li Du, Xiao Ding, Kai Xiong, Zhouhao Sun, Jun Shi, Ting Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through pretraining on a corpus with various sources, Large Language Models
(LLMs) have gained impressive performance. However, the impact of each
component of the pretraining corpus remains opaque. As a result, the
organization of the pretraining corpus is still empirical and may deviate from
the optimal. To address this issue, we systematically analyze the impact of 48
datasets from 5 major categories of pretraining data of LLMs and measure their
impacts on LLMs using benchmarks about nine major categories of model
capabilities. Our analyses provide empirical results about the contribution of
multiple corpora on the performances of LLMs, along with their joint impact
patterns, including complementary, orthogonal, and correlational relationships.
We also identify a set of ``high-impact data'' such as Books that is
significantly related to a set of model capabilities. These findings provide
insights into the organization of data to support more efficient pretraining of
LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-throughput Biomedical Relation Extraction for Semi-Structured Web
  Articles Empowered by Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08274v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08274v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songchi Zhou, Sheng Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: To develop a high-throughput biomedical relation extraction system
that takes advantage of the large language models'(LLMs) reading comprehension
ability and biomedical world knowledge in a scalable and evidential manner.
Methods: We formulate the relation extraction task as binary classifications
for large language models. Specifically, LLMs make the decision based on the
external corpus and its world knowledge, giving the reason for the judgment for
factual verification. This method is tailored for semi-structured web articles,
wherein we designate the main title as the tail entity and explicitly
incorporate it into the context, and the potential head entities are matched
based on a biomedical thesaurus. Moreover, lengthy contents are sliced into
text chunks, embedded, and retrieved with additional embedding models. Results:
Using an open-source LLM, we extracted 248659 relation triplets of three
distinct relation types from three reputable biomedical websites. To assess the
efficacy of the basic pipeline employed for biomedical relation extraction, we
curated a benchmark dataset annotated by a medical expert. Evaluation results
indicate that the pipeline exhibits performance comparable to that of GPT-4.
Case studies further illuminate challenges faced by contemporary LLMs in the
context of biomedical relation extraction for semi-structured web articles.
Conclusion: The proposed method has demonstrated its effectiveness in
leveraging the strengths of LLMs for high-throughput biomedical relation
extraction. Its adaptability is evident, as it can be seamlessly extended to
diverse semi-structured biomedical websites, facilitating the extraction of
various types of biomedical relations with ease.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STEntConv: Predicting Disagreement with Stance Detection and a Signed
  Graph Convolutional Network <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabelle Lorge, Li Zhang, Xiaowen Dong, Janet B. Pierrehumbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of social media platforms has led to an increase in polarised online
discussions, especially on political and socio-cultural topics such as
elections and climate change. We propose a simple and novel unsupervised method
to predict whether the authors of two posts agree or disagree, leveraging user
stances about named entities obtained from their posts. We present STEntConv, a
model which builds a graph of users and named entities weighted by stance and
trains a Signed Graph Convolutional Network (SGCN) to detect disagreement
between comment and reply posts. We run experiments and ablation studies and
show that including this information improves disagreement detection
performance on a dataset of Reddit posts for a range of controversial subreddit
topics, without the need for platform-specific features or user history.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the 2024 Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PWESuite: Phonetic Word Embeddings and Tasks They Facilitate <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02541v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02541v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vilém Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson, Nathaniel Robinson, Mrinmaya Sachan, David Mortensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mapping words into a fixed-dimensional vector space is the backbone of modern
NLP. While most word embedding methods successfully encode semantic
information, they overlook phonetic information that is crucial for many tasks.
We develop three methods that use articulatory features to build phonetically
informed word embeddings. To address the inconsistent evaluation of existing
phonetic word embedding methods, we also contribute a task suite to fairly
evaluate past, current, and future methods. We evaluate both (1) intrinsic
aspects of phonetic word embeddings, such as word retrieval and correlation
with sound similarity, and (2) extrinsic performance on tasks such as rhyme and
cognate detection and sound analogies. We hope our task suite will promote
reproducibility and inspire future phonetic embedding research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mastering Text, Code and Math Simultaneously via Fusing Highly
  Specialized Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08281v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08281v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Ding, Yulin Chen, Ganqu Cui, Xingtai Lv, Weilin Zhao, Ruobing Xie, Bowen Zhou, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underlying data distributions of natural language, programming code, and
mathematical symbols vary vastly, presenting a complex challenge for large
language models (LLMs) that strive to achieve high performance across all three
domains simultaneously. Achieving a very high level of proficiency for an LLM
within a specific domain often requires extensive training with relevant
corpora, which is typically accompanied by a sacrifice in performance in other
domains. In this paper, we propose to fuse models that are already
highly-specialized directly. The proposed fusing framework, UltraFuser,
consists of three distinct specialists that are already sufficiently trained on
language, coding, and mathematics. A token-level gating mechanism is introduced
to blend the specialists' outputs. A two-stage training strategy accompanied by
balanced sampling is designed to ensure stability. To effectively train the
fused model, we further construct a high-quality supervised instruction tuning
dataset, UltraChat 2, which includes text, code, and mathematical content. This
dataset comprises approximately 300,000 instructions and covers a wide range of
topics in each domain. Experiments show that our model could simultaneously
achieve mastery of the three crucial domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Document Embeddings via Self-Contrastive Bregman Divergence
  Learning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Saggau, Mina Rezaei, Bernd Bischl, Ilias Chalkidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning quality document embeddings is a fundamental problem in natural
language processing (NLP), information retrieval (IR), recommendation systems,
and search engines. Despite recent advances in the development of
transformer-based models that produce sentence embeddings with self-contrastive
learning, the encoding of long documents (Ks of words) is still challenging
with respect to both efficiency and quality considerations. Therefore, we train
Longfomer-based document encoders using a state-of-the-art unsupervised
contrastive learning method (SimCSE). Further on, we complement the baseline
method -- siamese neural network -- with additional convex neural networks
based on functional Bregman divergence aiming to enhance the quality of the
output document representations. We show that overall the combination of a
self-contrastive siamese network and our proposed neural Bregman network
outperforms the baselines in two linear classification settings on three long
document topic classification tasks from the legal and biomedical domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, short paper at Findings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Sexual Content at the Sentence Level in First Millennium Latin
  Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14974v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14974v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thibault Clérice
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose to evaluate the use of deep learning methods for
semantic classification at the sentence level to accelerate the process of
corpus building in the field of humanities and linguistics, a traditional and
time-consuming task. We introduce a novel corpus comprising around 2500
sentences spanning from 300 BCE to 900 CE including sexual semantics (medical,
erotica, etc.). We evaluate various sentence classification approaches and
different input embedding layers, and show that all consistently outperform
simple token-based searches. We explore the integration of idiolectal and
sociolectal metadata embeddings (centuries, author, type of writing), but find
that it leads to overfitting. Our results demonstrate the effectiveness of this
approach, achieving high precision and true positive rates (TPR) of
respectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset
size on the model performances (420 instead of 2013), and show that, while our
models perform worse, they still offer a high enough precision and TPR, even
without MLM, respectively 69% and 51%. Given the result, we provide an analysis
of the attention mechanism as a supporting added value for humanists in order
to produce more data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-order Joint Constituency and Dependency Parsing <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanggan Gu, Yang Hou, Zhefeng Wang, Xinyu Duan, Zhenghua Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work revisits the topic of jointly parsing constituency and dependency
trees, i.e., to produce compatible constituency and dependency trees
simultaneously for input sentences, which is attractive considering that the
two types of trees are complementary in representing syntax. The original work
of Zhou and Zhao (2019) performs joint parsing only at the inference phase.
They train two separate parsers under the multi-task learning framework (i.e.,
one shared encoder and two independent decoders). They design an ad-hoc dynamic
programming-based decoding algorithm of $O(n^5)$ time complexity for finding
optimal compatible tree pairs. Compared to their work, we make progress in
three aspects: (1) adopting a much more efficient decoding algorithm of
$O(n^4)$ time complexity, (2) exploring joint modeling at the training phase,
instead of only at the inference phase, (3) proposing high-order scoring
components to promote constituent-dependency interaction. We conduct
experiments and analysis on seven languages, covering both rich-resource and
low-resource scenarios. Results and analysis show that joint modeling leads to
a modest overall performance boost over separate modeling, but substantially
improves the complete matching ratio of whole trees, thanks to the explicit
modeling of tree compatibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking
  on Russia-Ukraine Conflict 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16662v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16662v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yirong Zeng, Xiao Ding, Yi Zhao, Xiangyu Li, Jie Zhang, Chao Yao, Ting Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fact-checking is the task of verifying the factuality of a given claim by
examining the available evidence. High-quality evidence plays a vital role in
enhancing fact-checking systems and facilitating the generation of explanations
that are understandable to humans. However, the provision of both sufficient
and relevant evidence for explainable fact-checking systems poses a challenge.
To tackle this challenge, we propose a method based on a Large Language Model
to automatically retrieve and summarize evidence from the Web. Furthermore, we
construct RU22Fact, a novel multilingual explainable fact-checking dataset on
the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world
claims, optimized evidence, and referenced explanation. To establish a baseline
for our dataset, we also develop an end-to-end explainable fact-checking system
to verify claims and generate explanations. Experimental results demonstrate
the prospect of optimized evidence in increasing fact-checking performance and
also indicate the possibility of further progress in the end-to-end claim
verification and explanation generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, accepted by lrec-coling2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Born With a Silver Spoon? Investigating Socioeconomic Bias in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14633v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14633v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Smriti Singh, Shuvam Keshari, Vinija Jain, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Socioeconomic bias in society exacerbates disparities, influencing access to
opportunities and resources based on individuals' economic and social
backgrounds. This pervasive issue perpetuates systemic inequalities, hindering
the pursuit of inclusive progress as a society. In this paper, we investigate
the presence of socioeconomic bias, if any, in large language models. To this
end, we introduce a novel dataset SilverSpoon, consisting of 3000 samples that
illustrate hypothetical scenarios that involve underprivileged people
performing ethically ambiguous actions due to their circumstances, and ask
whether the action is ethically justified. Further, this dataset has a
dual-labeling scheme and has been annotated by people belonging to both ends of
the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of
socioeconomic bias expressed in large language models and the variation of this
degree as a function of model size. We also perform qualitative analysis to
analyze the nature of this bias. Our analysis reveals that while humans
disagree on which situations require empathy toward the underprivileged, most
large language models are unable to empathize with the socioeconomically
underprivileged regardless of the situation. To foster further research in this
domain, we make SilverSpoon and our evaluation harness publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Performance of Long-Document Ranking Models through
  Comprehensive Evaluation and Leaderboarding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonid Boytsov, David Akinpelu, Tianyi Lin, Fangwei Gao, Yutian Zhao, Jeffrey Huang, Eric Nyberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We evaluated 20+ Transformer models for ranking of long documents (including
recent LongP models trained with FlashAttention) and compared them with simple
FirstP baselines (applying the same model to input truncated to the first 512
tokens). We used MS MARCO Documents v1 as a primary training set and evaluated
models in the zero-shot scenario as well as after fine-tuning on other
collections.
  In our initial experiments with standard collections we found that
long-document models underperformed FirstP or outperformed it by at most 5% on
average in terms of MRR or NDCG. We then conjectured that this was not due to
models inability to process long context but rather due to a positional bias of
relevant passages, which tended to be among the first 512 document tokens. We
found evidence that this bias was, indeed, present in at least two test sets,
which motivated us to create a new collection MS MARCO FarRelevant where the
relevant passages were not present among the first 512 tokens.
  Unlike standard collections where we observed both little benefit from
incorporating longer contexts and limited variability in model performance
(within a few %), experiments on MS MARCO FarRelevant uncovered dramatic
differences among models. FirstP models performed roughly at the
random-baseline level in both zero-shot and fine-tuning scenarios. Simple
aggregation models (e.g., MaxP) had good zero-shot accuracy but benefited
little from fine-tuning. Most other models had poor zero-shot performance
(sometimes at a random baseline level) but outstripped MaxP by as much 13-28\%
after finetuning. Thus, positional bias not only diminishes benefits of
processing longer document contexts but also leads to model overfitting to this
bias and performing poorly in a zero-shot setting when a distribution of
relevant passages changes substantially.
  We make our software and MS MARCO FarRelevant available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topic Detection and Tracking with Time-Aware Document Embeddings <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.06166v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.06166v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Jiang, Doug Beeferman, Weiquan Mao, Deb Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The time at which a message is communicated is a vital piece of metadata in
many real-world natural language processing tasks such as Topic Detection and
Tracking (TDT). TDT systems aim to cluster a corpus of news articles by event,
and in that context, stories that describe the same event are likely to have
been written at around the same time. Prior work on time modeling for TDT takes
this into account, but does not well capture how time interacts with the
semantic nature of the event. For example, stories about a tropical storm are
likely to be written within a short time interval, while stories about a movie
release may appear over weeks or months. In our work, we design a neural method
that fuses temporal and textual information into a single representation of
news documents for event detection. We fine-tune these time-aware document
embeddings with a triplet loss architecture, integrate the model into
downstream TDT systems, and evaluate the systems on two benchmark TDT data sets
in English. In the retrospective setting, we apply clustering algorithms to the
time-aware embeddings and show substantial improvements over baselines on the
News2013 data set. In the online streaming setting, we add our document encoder
to an existing state-of-the-art TDT pipeline and demonstrate that it can
benefit the overall performance. We conduct ablation studies on the time
representation and fusion algorithm strategies, showing that our proposed model
outperforms alternative strategies. Finally, we probe the model to examine how
it handles recurring events more effectively than previous TDT systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06463v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06463v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, Zhaopeng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety lies at the core of the development of Large Language Models (LLMs).
There is ample work on aligning LLMs with human ethics and preferences,
including data filtering in pretraining, supervised fine-tuning, reinforcement
learning from human feedback, and red teaming, etc. In this study, we discover
that chat in cipher can bypass the safety alignment techniques of LLMs, which
are mainly conducted in natural languages. We propose a novel framework
CipherChat to systematically examine the generalizability of safety alignment
to non-natural languages -- ciphers. CipherChat enables humans to chat with
LLMs through cipher prompts topped with system role descriptions and few-shot
enciphered demonstrations. We use CipherChat to assess state-of-the-art LLMs,
including ChatGPT and GPT-4 for different representative human ciphers across
11 safety domains in both English and Chinese. Experimental results show that
certain ciphers succeed almost 100% of the time to bypass the safety alignment
of GPT-4 in several safety domains, demonstrating the necessity of developing
safety alignment for non-natural languages. Notably, we identify that LLMs seem
to have a ''secret cipher'', and propose a novel SelfCipher that uses only role
play and several demonstrations in natural language to evoke this capability.
SelfCipher surprisingly outperforms existing human ciphers in almost all cases.
Our code and data will be released at https://github.com/RobustNLP/CipherChat.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024. 21 pages, 3 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Take Care of Your <span class="highlight-title">Prompt</span> Bias! Investigating and Mitigating <span class="highlight-title">Prompt</span> Bias
  in Factual Knowledge Extraction <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Xu, Keqin Peng, Liang Ding, Dacheng Tao, Xiliang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research shows that pre-trained language models (PLMs) suffer from
"prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce
biases toward specific labels. Prompt bias presents a significant challenge in
assessing the factual knowledge within PLMs. Therefore, this paper aims to
improve the reliability of existing benchmarks by thoroughly investigating and
mitigating prompt bias. We show that: 1) all prompts in the experiments exhibit
non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt
displaying significantly higher levels of bias; 2) prompt bias can amplify
benchmark accuracy unreasonably by overfitting the test datasets, especially on
imbalanced datasets like LAMA. Based on these findings, we propose a
representation-based approach to mitigate the prompt bias during inference
time. Specifically, we first estimate the biased representation using
prompt-only querying, and then remove it from the model's internal
representations to generate the debiased representations, which are used to
produce the final debiased outputs. Experiments across various prompts, PLMs,
and benchmarks show that our approach can not only correct the overfitted
performance caused by prompt bias, but also significantly improve the prompt
retrieval capability (up to 10% absolute performance gain). These results
indicate that our approach effectively alleviates prompt bias in knowledge
evaluation, thereby enhancing the reliability of benchmark assessments.
Hopefully, our plug-and-play approach can be a golden standard to strengthen
PLMs toward reliable knowledge bases. Code and data are released in
https://github.com/FelliYang/PromptBias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sabiá-2: A New Generation of Portuguese Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thales Sales Almeida, Hugo Abonizio, Rodrigo Nogueira, Ramon Pires
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Sabi\'a-2, a family of large language models trained on
Portuguese texts. The models are evaluated on a diverse range of exams,
including entry-level tests for Brazilian universities, professional
certification exams, and graduate-level exams for various disciplines such as
accounting, economics, engineering, law and medicine. Our results reveal that
our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's
performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64
exams. Notably, specialization has a significant impact on a model's
performance without the need to increase its size, allowing us to offer
Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.
Finally, we identified that math and coding are key abilities that need
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HIVE: Harnessing Human Feedback for Instructional Visual Editing <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating human feedback has been shown to be crucial to align text
generated by large language models to human preferences. We hypothesize that
state-of-the-art instructional image editing models, where outputs are
generated based on an input image and an editing instruction, could similarly
benefit from human feedback, as their outputs may not adhere to the correct
instructions and preferences of users. In this paper, we present a novel
framework to harness human feedback for instructional visual editing (HIVE).
Specifically, we collect human feedback on the edited images and learn a reward
function to capture the underlying user preferences. We then introduce scalable
diffusion model fine-tuning methods that can incorporate human preferences
based on the estimated reward. Besides, to mitigate the bias brought by the
limitation of data, we contribute a new 1M training dataset, a 3.6K reward
dataset for rewards learning, and a 1K evaluation dataset to boost the
performance of instructional image editing. We conduct extensive empirical
experiments quantitatively and qualitatively, showing that HIVE is favored over
previous state-of-the-art instructional image editing approaches by a large
margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In CVPR, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Few-Shot Learning Focused <span class="highlight-title">Survey</span> on Recent Named Entity Recognition
  and Relation Classification Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sakher Khalil Alqaaidi, Elika Bozorgi, Afsaneh Shams, Krzysztof Kochut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition (NER) and Relation Classification (RC) are important
steps in extracting information from unstructured text and formatting it into a
machine-readable format. We present a survey of recent deep learning models
that address named entity recognition and relation classification, with focus
on few-shot learning performance. Our survey is helpful for researchers in
knowing the recent techniques in text mining and extracting structured
information from raw text.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Good, but not always Fair: An Evaluation of Gender Bias for three
  commercial Machine Translation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silvia Alma Piazzolla, Beatrice Savoldi, Luisa Bentivogli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Translation (MT) continues to make significant strides in quality and
is increasingly adopted on a larger scale. Consequently, analyses have been
redirected to more nuanced aspects, intricate phenomena, as well as potential
risks that may arise from the widespread use of MT tools. Along this line, this
paper offers a meticulous assessment of three commercial MT systems - Google
Translate, DeepL, and Modern MT - with a specific focus on gender translation
and bias. For three language pairs (English/Spanish, English/Italian, and
English/French), we scrutinize the behavior of such systems at several levels
of granularity and on a variety of naturally occurring gender phenomena in
translation. Our study takes stock of the current state of online MT tools, by
revealing significant discrepancies in the gender translation of the three
systems, with each system displaying varying degrees of bias despite their
overall translation quality.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-25T00:00:00Z">2024-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">52</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProCQA: A Large-scale Community-based Programming Question Answering
  <span class="highlight-title">Dataset</span> for Code Search <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehan Li, Jianfei Zhang, Chuantao Yin, Yuanxin Ouyang, Wenge Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-based code question answering seeks to match user queries in
natural language to relevant code snippets. Previous approaches typically rely
on pretraining models using crafted bi-modal and uni-modal datasets to align
text and code representations. In this paper, we introduce ProCQA, a
large-scale programming question answering dataset extracted from the
StackOverflow community, offering naturally structured mixed-modal QA pairs. To
validate its effectiveness, we propose a modality-agnostic contrastive
pre-training approach to improve the alignment of text and code representations
of current code language models. Compared to previous models that primarily
employ bimodal and unimodal pairs extracted from CodeSearchNet for
pre-training, our model exhibits significant performance improvements across a
wide range of code retrieval benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ToXCL: A Unified Framework for Toxic Speech Detection and Explanation <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nhat M. Hoang, Xuan Long Do, Duc Anh Do, Duc Anh Vu, Luu Anh Tuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of online toxic speech is a pertinent problem posing
threats to demographic groups. While explicit toxic speech contains offensive
lexical signals, implicit one consists of coded or indirect language.
Therefore, it is crucial for models not only to detect implicit toxic speech
but also to explain its toxicity. This draws a unique need for unified
frameworks that can effectively detect and explain implicit toxic speech. Prior
works mainly formulated the task of toxic speech detection and explanation as a
text generation problem. Nonetheless, models trained using this strategy can be
prone to suffer from the consequent error propagation problem. Moreover, our
experiments reveal that the detection results of such models are much lower
than those that focus only on the detection task. To bridge these gaps, we
introduce ToXCL, a unified framework for the detection and explanation of
implicit toxic speech. Our model consists of three modules: a (i) Target Group
Generator to generate the targeted demographic group(s) of a given post; an
(ii) Encoder-Decoder Model in which the encoder focuses on detecting implicit
toxic speech and is boosted by a (iii) Teacher Classifier via knowledge
distillation, and the decoder generates the necessary explanation. ToXCL
achieves new state-of-the-art effectiveness, and outperforms baselines
significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Who is bragging more online? A large scale analysis of bragging in
  social media <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mali Jin, Daniel Preoţiuc-Pietro, A. Seza Doğruöz, Nikolaos Aletras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bragging is the act of uttering statements that are likely to be positively
viewed by others and it is extensively employed in human communication with the
aim to build a positive self-image of oneself. Social media is a natural
platform for users to employ bragging in order to gain admiration, respect,
attention and followers from their audiences. Yet, little is known about the
scale of bragging online and its characteristics. This paper employs
computational sociolinguistics methods to conduct the first large scale study
of bragging behavior on Twitter (U.S.) by focusing on its overall prevalence,
temporal dynamics and impact of demographic factors. Our study shows that the
prevalence of bragging decreases over time within the same population of users.
In addition, younger, more educated and popular users in the U.S. are more
likely to brag. Finally, we conduct an extensive linguistics analysis to unveil
specific bragging themes associated with different user traits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RU22Fact: Optimizing Evidence for Multilingual Explainable Fact-Checking
  on Russia-Ukraine Conflict 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yirong Zeng, Xiao Ding, Yi Zhao, Xiangyu Li, Jie Zhang, Chao Yao, Ting Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fact-checking is the task of verifying the factuality of a given claim by
examining the available evidence. High-quality evidence plays a vital role in
enhancing fact-checking systems and facilitating the generation of explanations
that are understandable to humans. However, the provision of both sufficient
and relevant evidence for explainable fact-checking systems poses a challenge.
To tackle this challenge, we propose a method based on a Large Language Model
to automatically retrieve and summarize evidence from the Web. Furthermore, we
construct RU22Fact, a novel multilingual explainable fact-checking dataset on
the Russia-Ukraine conflict in 2022 of 16K samples, each containing real-world
claims, optimized evidence, and referenced explanation. To establish a baseline
for our dataset, we also develop an end-to-end explainable fact-checking system
to verify claims and generate explanations. Experimental results demonstrate
the prospect of optimized evidence in increasing fact-checking performance and
also indicate the possibility of further progress in the end-to-end claim
verification and explanation generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, accepted by lrec-coling2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grammatical vs Spelling Error Correction: An Investigation into the
  Responsiveness of <span class="highlight-title">Transformer</span>-based Language Models using BART and MarianMT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Raju, Peeta Basa Pati, SA Gandheesh, Gayatri Sanjana Sannala, Suriya KS
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text continues to remain a relevant form of representation for information.
Text documents are created either in digital native platforms or through the
conversion of other media files such as images and speech. While the digital
native text is invariably obtained through physical or virtual keyboards,
technologies such as OCR and speech recognition are utilized to transform the
images and speech signals into text content. All these variety of mechanisms of
text generation also introduce errors into the captured text.
  This project aims at analyzing different kinds of error that occurs in text
documents. The work employs two of the advanced deep neural network-based
language models, namely, BART and MarianMT, to rectify the anomalies present in
the text. Transfer learning of these models with available dataset is performed
to finetune their capacity for error correction. A comparative study is
conducted to investigate the effectiveness of these models in handling each of
the defined error categories. It is observed that while both models can bring
down the erroneous sentences by 20+%, BART can handle spelling errors far
better (24.6%) than grammatical errors (8.8%).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comparative analysis of embedding models for patent similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grazia Sveva Ascione, Valerio Sterzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper makes two contributions to the field of text-based patent
similarity. First, it compares the performance of different kinds of
patent-specific pretrained embedding models, namely static word embeddings
(such as word2vec and doc2vec models) and contextual word embeddings (such as
transformers based models), on the task of patent similarity calculation.
Second, it compares specifically the performance of Sentence Transformers
(SBERT) architectures with different training phases on the patent similarity
task. To assess the models' performance, we use information about patent
interferences, a phenomenon in which two or more patent claims belonging to
different patent applications are proven to be overlapping by patent examiners.
Therefore, we use these interferences cases as a proxy for maximum similarity
between two patents, treating them as ground-truth to evaluate the performance
of the different embedding models. Our results point out that, first, Patent
SBERT-adapt-ub, the domain adaptation of the pretrained Sentence Transformer
architecture proposed in this research, outperforms the current
state-of-the-art in patent similarity. Second, they show that, in some cases,
large static models performances are still comparable to contextual ones when
trained on extensive data; thus, we believe that the superiority in the
performance of contextual embeddings may not be related to the actual
architecture but rather to the way the training phase is performed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantically Enriched Cross-Lingual Sentence Embeddings for
  Crisis-related Social Media Texts <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabindra Lamsal, Maria Rodriguez Read, Shanika Karunasekera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tasks such as semantic search and clustering on crisis-related social media
texts enhance our comprehension of crisis discourse, aiding decision-making and
targeted interventions. Pre-trained language models have advanced performance
in crisis informatics, but their contextual embeddings lack semantic
meaningfulness. Although the CrisisTransformers family includes a sentence
encoder to address the semanticity issue, it remains monolingual, processing
only English texts. Furthermore, employing separate models for different
languages leads to embeddings in distinct vector spaces, introducing challenges
when comparing semantic similarities between multi-lingual texts. Therefore, we
propose multi-lingual sentence encoders (CT-XLMR-SE and CT-mBERT-SE) that embed
crisis-related social media texts for over 50 languages, such that texts with
similar meanings are in close proximity within the same vector space,
irrespective of language diversity. Results in sentence encoding and sentence
matching tasks are promising, suggesting these models could serve as robust
baselines when embedding multi-lingual crisis-related social media texts. The
models are publicly available at: https://huggingface.co/crisistransformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ISCRAM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conversational Grounding: Annotation and Analysis of Grounding Acts and
  Grounding Units 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biswesh Mohapatra, Seemab Hassan, Laurent Romary, Justine Cassell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Successful conversations often rest on common understanding, where all
parties are on the same page about the information being shared. This process,
known as conversational grounding, is crucial for building trustworthy dialog
systems that can accurately keep track of and recall the shared information.
The proficiencies of an agent in grounding the conveyed information
significantly contribute to building a reliable dialog system. Despite recent
advancements in dialog systems, there exists a noticeable deficit in their
grounding capabilities. Traum provided a framework for conversational grounding
introducing Grounding Acts and Grounding Units, but substantial progress,
especially in the realm of Large Language Models, remains lacking. To bridge
this gap, we present the annotation of two dialog corpora employing Grounding
Acts, Grounding Units, and a measure of their degree of grounding. We discuss
our key findings during the annotation and also provide a baseline model to
test the performance of current Language Models in categorizing the grounding
acts of the dialogs. Our work aims to provide a useful resource for further
research in making conversations with machines better understood and more
reliable in natural day-to-day collaborative dialogs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TrustAI at SemEval-2024 Task 8: A Comprehensive Analysis of Multi-domain
  Machine Generated Text Detection Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashok Urlana, Aditya Saibewar, Bala Mallikarjunarao Garlapati, Charaka Vinayak Kumar, Ajeet Kumar Singh, Srinivasa Rao Chalamala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Large Language Models (LLMs) exhibit remarkable ability to generate
fluent content across a wide spectrum of user queries. However, this capability
has raised concerns regarding misinformation and personal information leakage.
In this paper, we present our methods for the SemEval2024 Task8, aiming to
detect machine-generated text across various domains in both mono-lingual and
multi-lingual contexts. Our study comprehensively analyzes various methods to
detect machine-generated text, including statistical, neural, and pre-trained
model approaches. We also detail our experimental setup and perform a in-depth
error analysis to evaluate the effectiveness of these methods. Our methods
obtain an accuracy of 86.9\% on the test set of subtask-A mono and 83.7\% for
subtask-B. Furthermore, we also highlight the challenges and essential factors
for consideration in future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 Figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models (or Humans) Distill Text? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Audinet de Pieuchon, Adel Daoud, Connor Thomas Jerzak, Moa Johansson, Richard Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the potential of large language models (LLMs) to distill text:
to remove the textual traces of an undesired forbidden variable. We employ a
range of LLMs with varying architectures and training approaches to distill
text by identifying and removing information about the target variable while
preserving other relevant signals. Our findings shed light on the strengths and
limitations of LLMs in addressing the distillation and provide insights into
the strategies for leveraging these models in computational social science
investigations involving text data. In particular, we show that in the strong
test of removing sentiment, the statistical association between the processed
text and sentiment is still clearly detectable to machine learning classifiers
post-LLM-distillation. Furthermore, we find that human annotators also struggle
to distill sentiment while preserving other semantic content. This suggests
there may be limited separability between concept variables in some text
contexts, highlighting limitations of methods relying on text-level
transformations and also raising questions about the robustness of distillation
methods that achieve statistical independence in representation space if this
is difficult for human coders operating on raw text to attain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NSINA: A News Corpus for Sinhala <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansi Hettiarachchi, Damith Premasiri, Lasitha Uyangodage, Tharindu Ranasinghe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of large language models (LLMs) has advanced natural
language processing (NLP), but their effectiveness is largely dependent on
pre-training resources. This is especially evident in low-resource languages,
such as Sinhala, which face two primary challenges: the lack of substantial
training data and limited benchmarking datasets. In response, this study
introduces NSINA, a comprehensive news corpus of over 500,000 articles from
popular Sinhala news websites, along with three NLP tasks: news media
identification, news category prediction, and news headline generation. The
release of NSINA aims to provide a solution to challenges in adapting LLMs to
Sinhala, offering valuable resources and benchmarks for improving NLP in the
Sinhala language. NSINA is the largest news corpus for Sinhala, available up to
date.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PE: A Poincare Explanation Method for Fast Text Hierarchy Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Chen, Xiaofeng He, Hongzhao Li, Hongyu Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The black-box nature of deep learning models in NLP hinders their widespread
application. The research focus has shifted to Hierarchical Attribution (HA)
for its ability to model feature interactions. Recent works model
non-contiguous combinations with a time-costly greedy search in Eculidean
spaces, neglecting underlying linguistic information in feature
representations. In this work, we introduce a novel method, namely Poincar\'e
Explanation (PE), for modeling feature interactions using hyperbolic spaces in
an $O(n^2logn)$ time complexity. Inspired by Poincar\'e model, we propose a
framework to project the embeddings into hyperbolic spaces, which exhibit
better inductive biases for syntax and semantic hierarchical structures.
Eventually, we prove that the hierarchical clustering process in the projected
space could be viewed as building a minimum spanning tree and propose a time
efficient algorithm. Experimental results demonstrate the effectiveness of our
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Information Extraction in Few-Shot Relation Classification
  through Contrastive Representation Learning <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Borchert, Jochen De Weerdt, Marie-Francine Moens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentiating relationships between entity pairs with limited labeled
instances poses a significant challenge in few-shot relation classification.
Representations of textual data extract rich information spanning the domain,
entities, and relations. In this paper, we introduce a novel approach to
enhance information extraction combining multiple sentence representations and
contrastive learning. While representations in relation classification are
commonly extracted using entity marker tokens, we argue that substantial
information within the internal model representations remains untapped. To
address this, we propose aligning multiple sentence representations, such as
the [CLS] token, the [MASK] token used in prompting, and entity marker tokens.
Our method employs contrastive learning to extract complementary discriminative
information from these individual representations. This is particularly
relevant in low-resource settings where information is scarce. Leveraging
multiple sentence representations is especially effective in distilling
discriminative information for relation classification when additional
information, like relation descriptions, are not available. We validate the
adaptability of our approach, maintaining robust performance in scenarios that
include relation descriptions, and showcasing its flexibility to adapt to
different resource constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hallucination Detection in Foundation Models for Decision-Making: A
  Flexible Definition and <span class="highlight-title">Review</span> of the State of the Art 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neeloy Chakraborty, Melkior Ornik, Katherine Driggs-Campbell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to
agricultural field robots, and from health care assistants to the entertainment
industry. The majority of these systems are developed with modular
sub-components for decision-making, planning, and control that may be
hand-engineered or learning-based. While these existing approaches have been
shown to perform well under the situations they were specifically designed for,
they can perform especially poorly in rare, out-of-distribution scenarios that
will undoubtedly arise at test-time. The rise of foundation models trained on
multiple tasks with impressively large datasets from a variety of fields has
led researchers to believe that these models may provide common sense reasoning
that existing planners are missing. Researchers posit that this common sense
reasoning will bridge the gap between algorithm development and deployment to
out-of-distribution tasks, like how humans adapt to unexpected scenarios. Large
language models have already penetrated the robotics and autonomous systems
domains as researchers are scrambling to showcase their potential use cases in
deployment. While this application direction is very promising empirically,
foundation models are known to hallucinate and generate decisions that may
sound reasonable, but are in fact poor. We argue there is a need to step back
and simultaneously design systems that can quantify the certainty of a model's
decision, and detect when it may be hallucinating. In this work, we discuss the
current use cases of foundation models for decision-making tasks, provide a
general definition for hallucinations with examples, discuss existing
approaches to hallucination detection and mitigation with a focus on decision
problems, and explore areas for further research in this exciting field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visually Guided Generative Text-Layout <span class="highlight-title">Pre-train</span>ing for Document
  Intelligence <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiming Mao, Haoli Bai, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu, Kam-Fai Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior study shows that pre-training techniques can boost the performance of
visual document understanding (VDU), which typically requires models to gain
abilities to perceive and reason both document texts and layouts (e.g.,
locations of texts and table-cells). To this end, we propose visually guided
generative text-layout pre-training, named ViTLP. Given a document image, the
model optimizes hierarchical language and layout modeling objectives to
generate the interleaved text and layout sequence. In addition, to address the
limitation of processing long documents by Transformers, we introduce a
straightforward yet effective multi-segment generative pre-training scheme,
facilitating ViTLP to process word-intensive documents of any length. ViTLP can
function as a native OCR model to localize and recognize texts of document
images. Besides, ViTLP can be effectively applied to various downstream VDU
tasks. Extensive experiments show that ViTLP achieves competitive performance
over existing baselines on benchmark VDU tasks, including information
extraction, document classification, and document question answering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024 main conference. The first version of this
  paper was submitted to OpenReview
  (https://openreview.net/forum?id=ARtBIBAmNR) in June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs Are Few-Shot In-Context Low-Resource Language Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Cahyawijaya, Holy Lovenia, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) empowers large language models (LLMs) to perform
diverse tasks in underrepresented languages using only short in-context
information, offering a crucial avenue for narrowing the gap between
high-resource and low-resource languages. Nonetheless, there is only a handful
of works explored ICL for low-resource languages with most of them focusing on
relatively high-resource languages, such as French and Spanish. In this work,
we extensively study ICL and its cross-lingual variation (X-ICL) on 25
low-resource and 7 relatively higher-resource languages. Our study not only
assesses the effectiveness of ICL with LLMs in low-resource languages but also
identifies the shortcomings of in-context label alignment, and introduces a
more effective alternative: query alignment. Moreover, we provide valuable
insights into various facets of ICL for low-resource languages. Our study
concludes the significance of few-shot in-context information on enhancing the
low-resource understanding quality of LLMs through semantically relevant
information by closing the language gap in the target language and aligning the
semantics between the targeted low-resource and the high-resource language that
the model is proficient in. Our work highlights the importance of advancing ICL
research, particularly for low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LARA: Linguistic-Adaptive Retrieval-Augmented LLMs for Multi-Turn Intent
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Junhua, Tan Yong Keat, Fu Bin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following the significant achievements of large language models (LLMs),
researchers have employed in-context learning for text classification tasks.
However, these studies focused on monolingual, single-turn classification
tasks. In this paper, we introduce LARA (Linguistic-Adaptive
Retrieval-Augmented Language Models), designed to enhance accuracy in
multi-turn classification tasks across six languages, accommodating numerous
intents in chatbot interactions. Multi-turn intent classification is notably
challenging due to the complexity and evolving nature of conversational
contexts. LARA tackles these issues by combining a fine-tuned smaller model
with a retrieval-augmented mechanism, integrated within the architecture of
LLMs. This integration allows LARA to dynamically utilize past dialogues and
relevant intents, thereby improving the understanding of the context.
Furthermore, our adaptive retrieval techniques bolster the cross-lingual
capabilities of LLMs without extensive retraining and fine-tune. Comprehensive
experiments demonstrate that LARA achieves state-of-the-art performance on
multi-turn intent classification tasks, enhancing the average accuracy by 3.67%
compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Construction of a Large-Scale Corpus for Geoparsing Using
  Wikipedia Hyperlinks <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyaki Ohno, Hirotaka Kameko, Keisuke Shirai, Taichi Nishimura, Shinsuke Mori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geoparsing is the task of estimating the latitude and longitude (coordinates)
of location expressions in texts. Geoparsing must deal with the ambiguity of
the expressions that indicate multiple locations with the same notation. For
evaluating geoparsing systems, several corpora have been proposed in previous
work. However, these corpora are small-scale and suffer from the coverage of
location expressions on general domains. In this paper, we propose Wikipedia
Hyperlink-based Location Linking (WHLL), a novel method to construct a
large-scale corpus for geoparsing from Wikipedia articles. WHLL leverages
hyperlinks in Wikipedia to annotate multiple location expressions with
coordinates. With this method, we constructed the WHLL corpus, a new
large-scale corpus for geoparsing. The WHLL corpus consists of 1.3M articles,
each containing about 7.8 unique location expressions. 45.6% of location
expressions are ambiguous and refer to more than one location with the same
notation. In each article, location expressions of the article title and those
hyperlinks to other articles are assigned with coordinates. By utilizing
hyperlinks, we can accurately assign location expressions with coordinates even
with ambiguous location expressions in the texts. Experimental results show
that there remains room for improvement by disambiguating location expressions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Named Entity Recognition via Superposition Concept
  Discrimination <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Chen, Hongyu Lin, Xianpei Han, Yaojie Lu, Shanshan Jiang, Bin Dong, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot NER aims to identify entities of target types with only limited
number of illustrative instances. Unfortunately, few-shot NER is severely
challenged by the intrinsic precise generalization problem, i.e., it is hard to
accurately determine the desired target type due to the ambiguity stemming from
information deficiency. In this paper, we propose Superposition Concept
Discriminator (SuperCD), which resolves the above challenge via an active
learning paradigm. Specifically, a concept extractor is first introduced to
identify superposition concepts from illustrative instances, with each concept
corresponding to a possible generalization boundary. Then a superposition
instance retriever is applied to retrieve corresponding instances of these
superposition concepts from large-scale text corpus. Finally, annotators are
asked to annotate the retrieved instances and these annotated instances
together with original illustrative instances are used to learn FS-NER models.
To this end, we learn a universal concept extractor and superposition instance
retriever using a large-scale openly available knowledge bases. Experiments
show that SuperCD can effectively identify superposition concepts from
illustrative instances, retrieve superposition instances from large-scale
corpus, and significantly improve the few-shot NER performance with minimal
additional efforts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study on How Attention Scores in the <span class="highlight-title">BERT</span> Model are Aware of Lexical
  Categories in Syntactic and Semantic Tasks on the GLUE Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjun Jang, Sungjoo Byun, Hyopil Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines whether the attention scores between tokens in the BERT
model significantly vary based on lexical categories during the fine-tuning
process for downstream tasks. Drawing inspiration from the notion that in human
language processing, syntactic and semantic information is parsed differently,
we categorize tokens in sentences according to their lexical categories and
focus on changes in attention scores among these categories. Our hypothesis
posits that in downstream tasks that prioritize semantic information, attention
scores centered on content words are enhanced, while in cases emphasizing
syntactic information, attention scores centered on function words are
intensified. Through experimentation conducted on six tasks from the GLUE
benchmark dataset, we substantiate our hypothesis regarding the fine-tuning
process. Furthermore, our additional investigations reveal the presence of BERT
layers that consistently assign more bias to specific lexical categories,
irrespective of the task, highlighting the existence of task-agnostic lexical
category preferences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Automatic Evaluation for LLMs' Clinical Capabilities: Metric,
  Data, and Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Liu, Xiaoyan Yang, Fangzhou Li, Chenfei Chi, Yue Shen, Shiwei Lyu Ming Zhang, Xiaowei Ma, Xiangguo Lyu, Liya Ma, Zhiqiang Zhang, Wei Xue, Yiran Huang, Jinjie Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are gaining increasing interests to improve
clinical efficiency for medical diagnosis, owing to their unprecedented
performance in modelling natural language. Ensuring the safe and reliable
clinical applications, the evaluation of LLMs indeed becomes critical for
better mitigating the potential risks, e.g., hallucinations. However, current
evaluation methods heavily rely on labor-intensive human participation to
achieve human-preferred judgements. To overcome this challenge, we propose an
automatic evaluation paradigm tailored to assess the LLMs' capabilities in
delivering clinical services, e.g., disease diagnosis and treatment. The
evaluation paradigm contains three basic elements: metric, data, and algorithm.
Specifically, inspired by professional clinical practice pathways, we formulate
a LLM-specific clinical pathway (LCP) to define the clinical capabilities that
a doctor agent should possess. Then, Standardized Patients (SPs) from the
medical education are introduced as the guideline for collecting medical data
for evaluation, which can well ensure the completeness of the evaluation
procedure. Leveraging these steps, we develop a multi-agent framework to
simulate the interactive environment between SPs and a doctor agent, which is
equipped with a Retrieval-Augmented Evaluation (RAE) to determine whether the
behaviors of a doctor agent are in accordance with LCP. The above paradigm can
be extended to any similar clinical scenarios to automatically evaluate the
LLMs' medical capabilities. Applying such paradigm, we construct an evaluation
benchmark in the field of urology, including a LCP, a SPs dataset, and an
automated RAE. Extensive experiments are conducted to demonstrate the
effectiveness of the proposed approach, providing more insights for LLMs' safe
and reliable deployments in clinical practice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KIT-19: A Comprehensive Korean Instruction Toolkit on 19 Tasks for
  Fine-Tuning Korean Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongjun Jang, Sungjoo Byun, Hyemi Jo, Hyopil Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction Tuning on Large Language Models is an essential process for model
to function well and achieve high performance in specific tasks. Accordingly,
in mainstream languages such as English, instruction-based datasets are being
constructed and made publicly available. In the case of Korean, publicly
available models and datasets all rely on using the output of ChatGPT or
translating datasets built in English. In this paper, We introduce
\textit{KIT-19} as an instruction dataset for the development of LLM in Korean.
\textit{KIT-19} is a dataset created in an instruction format, comprising 19
existing open-source datasets for Korean NLP tasks. In this paper, we train a
Korean Pretrained LLM using \textit{KIT-19} to demonstrate its effectiveness.
The experimental results show that the model trained on \textit{KIT-19}
significantly outperforms existing Korean LLMs. Based on the its quality and
empirical results, this paper proposes that \textit{KIT-19} has the potential
to make a substantial contribution to the future improvement of Korean LLMs'
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CodeS: Natural Language to Code Repository via Multi-Layer Sketch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan, Zhiguang Yang, Yongji Wang, Qianxiang Wang, Lizhen Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive performance of large language models (LLMs) on code-related
tasks has shown the potential of fully automated software development. In light
of this, we introduce a new software engineering task, namely Natural Language
to code Repository (NL2Repo). This task aims to generate an entire code
repository from its natural language requirements. To address this task, we
propose a simple yet effective framework CodeS, which decomposes NL2Repo into
multiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three
modules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first
generates a repository's directory structure for given requirements;
FileSketcher then generates a file sketch for each file in the generated
structure; SketchFiller finally fills in the details for each function in the
generated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry
out evaluations through both automated benchmarking and manual feedback
analysis. For benchmark-based evaluation, we craft a repository-oriented
benchmark, SketchEval, and design an evaluation metric, SketchBLEU. For
feedback-based evaluation, we develop a VSCode plugin for CodeS and engage 30
participants in conducting empirical studies. Extensive experiments prove the
effectiveness and practicality of CodeS on the NL2Repo task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/NL2Code/CodeS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ If CLIP Could Talk: Understanding Vision-Language Model Representations
  Through Their Preferred Concept Descriptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Esfandiarpoor, Cristina Menghini, Stephen H. Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works often assume that Vision-Language Model (VLM) representations
are based on visual attributes like shape. However, it is unclear to what
extent VLMs prioritize this information to represent concepts. We propose
Extract and Explore (EX2), a novel approach to characterize important textual
features for VLMs. EX2 uses reinforcement learning to align a large language
model with VLM preferences and generates descriptions that incorporate the
important features for the VLM. Then, we inspect the descriptions to identify
the features that contribute to VLM representations. We find that spurious
descriptions have a major role in VLM representations despite providing no
helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly,
among informative descriptions, VLMs rely significantly on non-visual
attributes like habitat to represent visual concepts. Also, our analysis
reveals that different VLMs prioritize different attributes in their
representations. Overall, we show that VLMs do not simply match images to scene
descriptions and that non-visual or even spurious descriptions significantly
influence their representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/BatsResearch/ex2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Large Language Models with Runtime Behavior of Program
  Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, Xin Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models for code (i.e., code LLMs) have shown strong code
understanding and generation capabilities. To evaluate the capabilities of code
LLMs in various aspects, many benchmarks have been proposed (e.g., HumanEval
and ClassEval). Code reasoning is one of the most essential abilities of code
LLMs, but existing benchmarks for code reasoning are not sufficient. Typically,
they focus on predicting the input and output of a program, ignoring the
evaluation of the intermediate behavior during program execution, as well as
the logical consistency (e.g., the model should not give the correct output if
the prediction of execution path is wrong) when performing the reasoning. To
address these problems, in this paper, we propose a framework, namely REval,
for evaluating code reasoning abilities and consistency of code LLMs with
program execution. We utilize existing code benchmarks and adapt them to new
benchmarks within our framework. A large-scale empirical study is conducted and
most LLMs show unsatisfactory performance on both Runtime Behavior Reasoning
(i.e., an average accuracy of 44.4%) and Incremental Consistency Evaluation
(i.e., an average IC score of 10.3). Evaluation results of current code LLMs
reflect the urgent need for the community to strengthen the code reasoning
capability of code LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InstUPR : Instruction-based Unsupervised Passage Reranking with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao-Wei Huang, Yun-Nung Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces InstUPR, an unsupervised passage reranking method based
on large language models (LLMs). Different from existing approaches that rely
on extensive training with query-document pairs or retrieval-specific
instructions, our method leverages the instruction-following capabilities of
instruction-tuned LLMs for passage reranking without any additional
fine-tuning. To achieve this, we introduce a soft score aggregation technique
and employ pairwise reranking for unsupervised passage reranking. Experiments
on the BEIR benchmark demonstrate that InstUPR outperforms unsupervised
baselines as well as an instruction-tuned reranker, highlighting its
effectiveness and superiority. Source code to reproduce all experiments is
open-sourced at https://github.com/MiuLab/InstUPR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. This manuscript was originally written and submitted in
  June 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\textit{Link<span class="highlight-title">Prompt</span>}$: Natural and Universal Adversarial Attacks on
  <span class="highlight-title">Prompt</span>-based Language Models <span class="chip">NAACL2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Xu, Wenjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based learning is a new language model training paradigm that adapts
the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes
the performance benchmarks across various natural language processing (NLP)
tasks. Instead of using a fixed prompt template to fine-tune the model, some
research demonstrates the effectiveness of searching for the prompt via
optimization. Such prompt optimization process of prompt-based learning on PLMs
also gives insight into generating adversarial prompts to mislead the model,
raising concerns about the adversarial vulnerability of this paradigm. Recent
studies have shown that universal adversarial triggers (UATs) can be generated
to alter not only the predictions of the target PLMs but also the prediction of
corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based
learning paradigm. However, UATs found in previous works are often unreadable
tokens or characters and can be easily distinguished from natural texts with
adaptive defenses. In this work, we consider the naturalness of the UATs and
develop $\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs
by a gradient-based beam search algorithm that not only effectively attacks the
target PLMs and PFMs but also maintains the naturalness among the trigger
tokens. Extensive results demonstrate the effectiveness of
$\textit{LinkPrompt}$, as well as the transferability of UATs generated by
\textit{LinkPrompt} to open-sourced Large Language Model (LLM) Llama2 and
API-accessed LLM GPT-3.5-turbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is There a One-Model-Fits-All Approach to Information Extraction?
  Revisiting Task Definition Biases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Huang, Qianyu He, Zhixu Li, Jiaqing Liang, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Definition bias is a negative phenomenon that can mislead models. Definition
bias in information extraction appears not only across datasets from different
domains but also within datasets sharing the same domain. We identify two types
of definition bias in IE: bias among information extraction datasets and bias
between information extraction datasets and instruction tuning datasets. To
systematically investigate definition bias, we conduct three probing
experiments to quantitatively analyze it and discover the limitations of
unified information extraction and large language models in solving definition
bias. To mitigate definition bias in information extraction, we propose a
multi-stage framework consisting of definition bias measurement, bias-aware
fine-tuning, and task-specific bias mitigation. Experimental results
demonstrate the effectiveness of our framework in addressing definition bias.
Resources of this paper can be found at
https://github.com/EZ-hwh/definition-bias
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skews in the Phenomenon Space Hinder Generalization in Text-to-Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingshan Chang, Yasi Zhang, Zhiyuan Fang, Yingnian Wu, Yonatan Bisk, Feng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The literature on text-to-image generation is plagued by issues of faithfully
composing entities with relations. But there lacks a formal understanding of
how entity-relation compositions can be effectively learned. Moreover, the
underlying phenomenon space that meaningfully reflects the problem structure is
not well-defined, leading to an arms race for larger quantities of data in the
hope that generalization emerges out of large-scale pretraining. We hypothesize
that the underlying phenomenological coverage has not been proportionally
scaled up, leading to a skew of the presented phenomenon which harms
generalization. We introduce statistical metrics that quantify both the
linguistic and visual skew of a dataset for relational learning, and show that
generalization failures of text-to-image generation are a direct result of
incomplete or unbalanced phenomenological coverage. We first perform
experiments in a synthetic domain and demonstrate that systematically
controlled metrics are strongly predictive of generalization performance. Then
we move to natural images and show that simple distribution perturbations in
light of our theories boost generalization without enlarging the absolute data
size. This work informs an important direction towards quality-enhancing the
data diversity or balance orthogonal to scaling up the absolute size. Our
discussions point out important open questions on 1) Evaluation of generated
entity-relation compositions, and 2) Better models for reasoning with abstract
relations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators
  for Reasoning-Based Chart VQA <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhuowan, Jasani Bhavan, Tang Peng, Ghadar Shabnam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding data visualizations like charts and plots requires reasoning
about both visual elements and numerics. Although strong in extractive
questions, current chart visual question answering (chart VQA) models suffer on
complex reasoning questions. In this work, we address the lack of reasoning
ability by data augmentation. We leverage Large Language Models (LLMs), which
have shown to have strong reasoning ability, as an automatic data annotator
that generates question-answer annotations for chart images. The key innovation
in our method lies in the Synthesize Step-by-Step strategy: our LLM-based data
generator learns to decompose the complex question into step-by-step
sub-questions (rationales), which are then used to derive the final answer
using external tools, i.e. Python. This step-wise generation procedure is
trained on synthetic data generated using a template-based QA generation
pipeline. Experimental results highlight the significance of the proposed
step-by-step generation. By training with the LLM-augmented data (LAMENDA), we
significantly enhance the chart VQA models, achieving the state-of-the-art
accuracy on the ChartQA and PlotQA datasets. In particular, our approach
improves the accuracy of the previous state-of-the-art approach from 38% to 54%
on the human-written questions in the ChartQA dataset, which needs strong
reasoning. We hope our work underscores the potential of synthetic data and
encourages further exploration of data augmentation using LLMs for
reasoning-heavy tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Facet Generation with LLM Editing <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joosung Lee, Jinhong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In information retrieval, facet identification of a user query is an
important task. If a search service can recognize the facets of a user's query,
it has the potential to offer users a much broader range of search results.
Previous studies can enhance facet prediction by leveraging retrieved documents
and related queries obtained through a search engine. However, there are
challenges in extending it to other applications when a search engine operates
as part of the model. First, search engines are constantly updated. Therefore,
additional information may change during training and test, which may reduce
performance. The second challenge is that public search engines cannot search
for internal documents. Therefore, a separate search system needs to be built
to incorporate documents from private domains within the company. We propose
two strategies that focus on a framework that can predict facets by taking only
queries as input without a search engine. The first strategy is multi-task
learning to predict SERP. By leveraging SERP as a target instead of a source,
the proposed model deeply understands queries without relying on external
modules. The second strategy is to enhance the facets by combining Large
Language Model (LLM) and the small model. Overall performance improves when
small model and LLM are combined rather than facet generation individually.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pointer-Generator Networks for Low-Resource Machine Translation: Don't
  Copy That! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10963v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10963v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niyati Bafna, Philipp Koehn, David Yarowsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Transformer-based neural machine translation (NMT) is very effective in
high-resource settings, many languages lack the necessary large parallel
corpora to benefit from it. In the context of low-resource (LR) MT between two
closely-related languages, a natural intuition is to seek benefits from
structural "shortcuts", such as copying subwords from the source to the target,
given that such language pairs often share a considerable number of identical
words, cognates, and borrowings. We test Pointer-Generator Networks for this
purpose for six language pairs over a variety of resource ranges, and find weak
improvements for most settings. However, analysis shows that the model does not
show greater improvements for closely-related vs. more distant language pairs,
or for lower resource ranges, and that the models do not exhibit the expected
usage of the mechanism for shared subwords. Our discussion of the reasons for
this behaviour highlights several general challenges for LR NMT, such as modern
tokenization strategies, noisy real-world conditions, and linguistic
complexities. We call for better scrutiny of linguistically motivated
improvements to NMT given the blackbox nature of Transformer models, as well as
for a focus on the above problems in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Second Look on BASS -- Boosting Abstractive Summarization with Unified
  Semantic Graphs -- A Replication Study <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osman Alperen Koraş, Jörg Schlötterer, Christin Seifert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a detailed replication study of the BASS framework, an abstractive
summarization system based on the notion of Unified Semantic Graphs. Our
investigation includes challenges in replicating key components and an ablation
study to systematically isolate error sources rooted in replicating novel
components. Our findings reveal discrepancies in performance compared to the
original work. We highlight the significance of paying careful attention even
to reasonably omitted details for replicating advanced frameworks like BASS,
and emphasize key practices for writing replicable papers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This preprint has not undergone peer review or any post-submission
  improvements or corrections. The Version of Record of this contribution is
  published in Advances in Information Retrieval, 46th European Conference on
  Information Retrieval, ECIR 2024. 16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongHeads: Multi-Head Attention is Secretly a Long Context Processor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved impressive performance in numerous
domains but often struggle to process lengthy inputs effectively and
efficiently due to limited length generalization and attention's quadratic
computational demands. Many sought to mitigate this by restricting the
attention window within the pre-trained length. However, these methods
introduce new issues such as ignoring the middle context and requiring
additional training. To address these problems, we propose LongHeads, a
training-free framework that enhances LLM's long context ability by unlocking
multi-head attention's untapped potential. Instead of allowing each head to
attend to the full sentence, which struggles with generalizing to longer
sequences due to out-of-distribution (OOD) issues, we allow each head to
process in-distribution length by selecting and attending to important context
chunks. To this end, we propose a chunk selection strategy that relies on the
inherent correlation between the query and the key representations, efficiently
distributing context chunks to different heads. In this way, each head ensures
it can effectively process attended tokens within the trained length, while
different heads in different layers can collectively process longer contexts.
LongHeads works efficiently in linear time, fits seamlessly with many LLMs that
use relative positional encoding. LongHeads achieves 100% accuracy at the 128k
length on passkey retrieval task, verifying LongHeads's efficacy in extending
the usable context window for existing models. We release our code at
https://github.com/LuLuLuyi/LongHeads .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Relationship between Skill Neurons and Robustness in <span class="highlight-title">Prompt</span>
  Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12263v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12263v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Ackermann, Xenia Ohmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt Tuning is a popular parameter-efficient finetuning method for
pre-trained large language models (PLMs). Based on experiments with RoBERTa, it
has been suggested that Prompt Tuning activates specific neurons in the
transformer's feed-forward networks, that are highly predictive and selective
for the given task. In this paper, we study the robustness of Prompt Tuning in
relation to these "skill neurons", using RoBERTa and T5. We show that prompts
tuned for a specific task are transferable to tasks of the same type but are
not very robust to adversarial data. While prompts tuned for RoBERTa yield
below-chance performance on adversarial data, prompts tuned for T5 are slightly
more robust and retain above-chance performance in two out of three cases. At
the same time, we replicate the finding that skill neurons exist in RoBERTa and
further show that skill neurons also exist in T5. Interestingly, the skill
neurons of T5 determined on non-adversarial data are also among the most
predictive neurons on the adversarial data, which is not the case for RoBERTa.
We conclude that higher adversarial robustness may be related to a model's
ability to consistently activate the relevant skill neurons on adversarial
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chitchat as Interference: Adding User Backstories to Task-Oriented
  Dialogues <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armand Stricker, Patrick Paroubek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During task-oriented dialogues (TODs), human users naturally introduce
chitchat that is beyond the immediate scope of the task, interfering with the
flow of the conversation. To address this issue without the need for expensive
manual data creation, we use few-shot prompting with Llama-2-70B to enhance the
MultiWOZ dataset with user backstories, a typical example of chitchat
interference in TODs. We assess the impact of this addition by testing two
models: one trained solely on TODs and another trained on TODs with a
preliminary chitchat interaction. Our analysis demonstrates that our enhanced
dataset poses a challenge for these systems. Moreover, we demonstrate that our
dataset can be effectively used for training purposes, enabling a system to
consistently acknowledge the user's backstory while also successfully moving
the task forward in the same turn, as confirmed by human evaluation. These
findings highlight the benefits of generating novel chitchat-TOD scenarios to
test TOD systems more thoroughly and improve their resilience to natural user
interferences
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted @ LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiLe Loss: a New Loss for Mitigating the Bias of Learning Difficulties
  in Generative Language Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19531v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19531v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenpeng Su, Xing Wu, Xue Bai, Zijia Lin, Hui Chen, Guiguang Ding, Wei Zhou, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative language models are usually pretrained on large text corpus via
predicting the next token (i.e., sub-word/word/phrase) given the previous ones.
Recent works have demonstrated the impressive performance of large generative
language models on downstream tasks. However, existing generative language
models generally neglect an inherent challenge in text corpus during training,
i.e., the imbalance between frequent tokens and infrequent ones. It can lead a
language model to be dominated by common and easy-to-learn tokens, thereby
overlooking the infrequent and difficult-to-learn ones. To alleviate that, we
propose a MiLe Loss function for mitigating the bias of learning difficulties
with tokens. During training, it can dynamically assess the learning difficulty
of a to-be-learned token, according to the information entropy of the
corresponding predicted probability distribution over the vocabulary. Then it
scales the training loss adaptively, trying to lead the model to focus more on
the difficult-to-learn tokens. On the Pile dataset, we train generative
language models at different scales of 468M, 1.2B, and 6.7B parameters.
Experiments reveal that models incorporating the proposed MiLe Loss can gain
consistent performance improvement on downstream benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Align-to-Distill: Trainable Attention Alignment for Knowledge
  Distillation in Neural Machine Translation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.01479v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.01479v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heegon Jin, Seonil Son, Jemin Park, Youngseok Kim, Hyungjong Noh, Yeonsoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of scalable deep models and large datasets has improved the
performance of Neural Machine Translation. Knowledge Distillation (KD) enhances
efficiency by transferring knowledge from a teacher model to a more compact
student model. However, KD approaches to Transformer architecture often rely on
heuristics, particularly when deciding which teacher layers to distill from. In
this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to
address the feature mapping problem by adaptively aligning student attention
heads with their teacher counterparts during training. The Attention Alignment
Module in A2D performs a dense head-by-head comparison between student and
teacher attention heads across layers, turning the combinatorial mapping
heuristics into a learning problem. Our experiments show the efficacy of A2D,
demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De->Dsb
and WMT-2014 En->De, respectively, compared to Transformer baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ To share or not to share: What risks would laypeople accept to give
  sensitive data to differentially-private NLP systems? <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06708v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06708v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Weiss, Frauke Kreuter, Ivan Habernal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although the NLP community has adopted central differential privacy as a
go-to framework for privacy-preserving model training or data sharing, the
choice and interpretation of the key parameter, privacy budget $\varepsilon$
that governs the strength of privacy protection, remains largely arbitrary. We
argue that determining the $\varepsilon$ value should not be solely in the
hands of researchers or system developers, but must also take into account the
actual people who share their potentially sensitive data. In other words: Would
you share your instant messages for $\varepsilon$ of 10? We address this
research gap by designing, implementing, and conducting a behavioral experiment
(311 lay participants) to study the behavior of people in uncertain
decision-making situations with respect to privacy-threatening situations.
Framing the risk perception in terms of two realistic NLP scenarios and using a
vignette behavioral study help us determine what $\varepsilon$ thresholds would
lead lay people to be willing to share sensitive textual data - to our
knowledge, the first study of its kind.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024; final camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HealthFC: Verifying Health Claims with Evidence-Based Medical
  Fact-Checking <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08503v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08503v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juraj Vladika, Phillip Schneider, Florian Matthes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the digital age, seeking health advice on the Internet has become a common
practice. At the same time, determining the trustworthiness of online medical
content is increasingly challenging. Fact-checking has emerged as an approach
to assess the veracity of factual claims using evidence from credible knowledge
sources. To help advance automated Natural Language Processing (NLP) solutions
for this task, in this paper we introduce a novel dataset HealthFC. It consists
of 750 health-related claims in German and English, labeled for veracity by
medical experts and backed with evidence from systematic reviews and clinical
trials. We provide an analysis of the dataset, highlighting its characteristics
and challenges. The dataset can be used for NLP tasks related to automated
fact-checking, such as evidence retrieval, claim verification, or explanation
generation. For testing purposes, we provide baseline systems based on
different approaches, examine their performance, and discuss the findings. We
show that the dataset is a challenging test bed with a high potential for
future use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ With Greater Text Comes Greater Necessity: Inference-Time Training Helps
  Long Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Y. Wang, D. Ma, D. Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long text generation, such as novel writing and discourse-level translation
with extremely long contexts, presents significant challenges to current
language models. Existing methods mainly focus on extending the model's context
window through strategies like length extrapolation. However, these approaches
demand substantial hardware resources during the training and/or inference
phases. Our proposed method, Temp-Lora, introduces an alternative concept.
Instead of relying on the KV cache to store all context information, we embeds
this information directly into a temporary Lora module. In the process of long
text generation, this module is progressively trained with text generated
previously. This approach not only efficiently preserves contextual knowledge
but also prevents any permanent alteration to the model's parameters given that
the module is discarded post-generation. Extensive experiments on the PG19
language modeling benchmark and the GuoFeng discourse-level translation
benchmark validate the effectiveness of Temp-Lora. Our results show that: 1)
Temp-Lora substantially enhances generation quality for long text, as indicated
by a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%
decrease in PPL along with a 113.2% increase in BLEU score on a subset of
GuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text
generation methods, and 3) Temp-Lora can greatly reduce computational costs by
shortening the context window. For example, we can ensure a moderate
improvement in generation quality (a decrease of 3.8% in PPL) while enabling a
51.5% memory usage reduction and a 60.0% decrease in latency for inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue
  Systems <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04357v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04357v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue response selection aims to select an appropriate response from
several candidates based on a given user and system utterance history. Most
existing works primarily focus on post-training and fine-tuning tailored for
cross-encoders. However, there are no post-training methods tailored for dense
encoders in dialogue response selection. We argue that when the current
language model, based on dense dialogue systems (such as BERT), is employed as
a dense encoder, it separately encodes dialogue context and response, leading
to a struggle to achieve the alignment of both representations. Thus, we
propose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward
yet effective post-training technique tailored for dense encoders in dialogue
response selection. Dial-MAE uses an asymmetric encoder-decoder architecture to
compress the dialogue semantics into dense vectors, which achieves better
alignment between the features of the dialogue context and response. Our
experiments have demonstrated that Dial-MAE is highly effective, achieving
state-of-the-art performance on two commonly evaluated benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Effective Distillation of Table-based Reasoning Ability from LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13182v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13182v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohao Yang, Chen Tang, Kun Zhao, Chenghao Xiao, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable performance across
a wide range of natural language processing tasks. However, their enormous
parameter size and extremely high requirements for compute power pose
challenges for their practical deployment. Recent research has revealed that
specific capabilities of LLMs, such as numerical reasoning, can be transferred
to smaller models through distillation. Some studies explore the potential of
leveraging LLMs to perform table-based reasoning. However, there has been no
prior work focusing on table reasoning skills in smaller models specifically
tailored for scientific table-to-text generation tasks. In this paper, we
propose a novel table-based reasoning distillation approach, with the aim of
distilling LLMs into tailored smaller models. Our experimental results have
shown that a 220 million parameter model (Flan-T5-base) fine-tuned using
distilled data, not only achieves a significant improvement compared to
traditionally fine-tuned baselines, but also surpasses specific LLMs on a
scientific table-to-text generation dataset. Our code is available at
https://github.com/Bernard-Yang/DistillTableCoT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HallusionBench: An Advanced Diagnostic Suite for Entangled Language
  Hallucination and Visual Illusion in Large Vision-Language Models <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14566v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14566v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce HallusionBench, a comprehensive benchmark designed for the
evaluation of image-context reasoning. This benchmark presents significant
challenges to advanced large visual-language models (LVLMs), such as
GPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing
nuanced understanding and interpretation of visual data. The benchmark
comprises 346 images paired with 1129 questions, all meticulously crafted by
human experts. We introduce a novel structure for these visual questions
designed to establish control groups. This structure enables us to conduct a
quantitative analysis of the models' response tendencies, logical consistency,
and various failure modes. In our evaluation on HallusionBench, we benchmarked
15 different models, highlighting a 31.42% question-pair accuracy achieved by
the state-of-the-art GPT-4V. Notably, all other evaluated models achieve
accuracy below 16%. Moreover, our analysis not only highlights the observed
failure modes, including language hallucination and visual illusion, but also
deepens an understanding of these pitfalls. Our comprehensive case studies
within HallusionBench shed light on the challenges of hallucination and
illusion in LVLMs. Based on these insights, we suggest potential pathways for
their future improvement. The benchmark and codebase can be accessed at
https://github.com/tianyi-lab/HallusionBench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Confidence Estimation and Calibration in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08298v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08298v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Geng, Fengyu Cai, Yuxia Wang, Heinz Koeppl, Preslav Nakov, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks in various domains. Despite their impressive performance,
they can be unreliable due to factual errors in their generations. Assessing
their confidence and calibrating them across different tasks can help mitigate
risks and enable LLMs to produce better generations. There has been a lot of
recent research aiming to address this, but there has been no comprehensive
overview to organize it and outline the main lessons learned. The present
survey aims to bridge this gap. In particular, we outline the challenges and we
summarize recent technical advancements for LLM confidence estimation and
calibration. We further discuss their applications and suggest promising
directions for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 1 page, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06199v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06199v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen, Yaxin Peng, Zhicai Ou, Feifei Feng, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have showcased impressive skills in
tasks related to visual understanding and reasoning. Yet, their widespread
application faces obstacles due to the high computational demands during both
the training and inference phases, restricting their use to a limited audience
within the research and user communities. In this paper, we investigate the
design aspects of Multimodal Small Language Models (MSLMs) and propose an
efficient multimodal assistant named Mipha, which is designed to create synergy
among various aspects: visual representation, language models, and optimization
strategies. We show that without increasing the volume of training data, our
Mipha-3B outperforms the state-of-the-art large MLLMs, especially
LLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide
insights and guidelines for developing strong MSLMs that rival the capabilities
of MLLMs. Our code is available at https://github.com/zhuyiche/llava-phi.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Chat<span class="highlight-title">GPT</span> and its Impact on Society 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Asraful Haque, Shuai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence has been around for a while, but suddenly it has
received more attention than ever before. Thanks to innovations from companies
like Google, Microsoft, Meta, and other major brands in technology. OpenAI,
though, has triggered the button with its ground-breaking invention ChatGPT.
ChatGPT is a Large Language Model (LLM) based on Transformer architecture that
has the ability to generate human-like responses in a conversational context.
It uses deep learning algorithms to generate natural language responses to
input text. Its large number of parameters, contextual generation, and
open-domain training make it a versatile and effective tool for a wide range of
applications, from chatbots to customer service to language translation. It has
the potential to revolutionize various industries and transform the way we
interact with technology. However, the use of ChatGPT has also raised several
concerns, including ethical, social, and employment challenges, which must be
carefully considered to ensure the responsible use of this technology. The
article provides an overview of ChatGPT, delving into its architecture and
training process. It highlights the potential impacts of ChatGPT on the
society. In this paper, we suggest some approaches involving technology,
regulation, education, and ethics in an effort to maximize ChatGPT's benefits
while minimizing its negative impacts. This study is expected to contribute to
a greater understanding of ChatGPT and aid in predicting the potential changes
it may bring about.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SEA: Sparse Linear Attention with Estimated Attention Mask 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01777v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01777v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heejun Lee, Jina Kim, Jeffrey Willette, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformer architecture has driven breakthroughs in recent years on
tasks which require modeling pairwise relationships between sequential
elements, as is the case in natural language understanding. However, long
seqeuences pose a problem due to the quadratic complexity of the attention
operation. Previous research has aimed to lower the complexity by sparsifying
or linearly approximating the attention matrix. Yet, these approaches cannot
straightforwardly distill knowledge from a teacher's attention matrix and often
require complete retraining from scratch. Furthermore, previous sparse and
linear approaches lose interpretability if they cannot produce full attention
matrices. To address these challenges, we propose SEA: Sparse linear attention
with an Estimated Attention mask. SEA estimates the attention matrix with
linear complexity via kernel-based linear attention, then subsequently creates
a sparse attention matrix with a top-k selection to perform a sparse attention
operation. For language modeling tasks (Wikitext2), previous linear and sparse
attention methods show roughly two-fold worse perplexity scores over the
quadratic OPT-1.3B baseline, while SEA achieves better perplexity than
OPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable
attention matrix. We believe that our work will have a large practical impact,
as it opens the possibility of running large transformers on resource-limited
devices with less memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 main pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Situated Natural Language Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zining Zhu, Haoming Jiang, Jingfeng Yang, Sreyashi Nag, Chao Zhang, Jie Huang, Yifan Gao, Frank Rudzicz, Bing Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language is among the most accessible tools for explaining decisions
to humans, and large pretrained language models (PLMs) have demonstrated
impressive abilities to generate coherent natural language explanations (NLE).
The existing NLE research perspectives do not take the audience into account.
An NLE can have high textual quality, but it might not accommodate audiences'
needs and preference. To address this limitation, we propose an alternative
perspective, \textit{situated} NLE. On the evaluation side, we set up automated
evaluation scores. These scores describe the properties of NLEs in lexical,
semantic, and pragmatic categories. On the generation side, we identify three
prompt engineering techniques and assess their applicability on the situations.
Situated NLE provides a perspective and facilitates further research on the
generation and evaluation of explanations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Transfer Attack to Image Watermarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermark has been widely deployed by industry to detect AI-generated images.
The robustness of such watermark-based detector against evasion attacks in the
white-box and black-box settings is well understood in the literature. However,
the robustness in the no-box setting is much less understood. In particular,
multiple studies claimed that image watermark is robust in such setting. In
this work, we propose a new transfer evasion attack to image watermark in the
no-box setting. Our transfer attack adds a perturbation to a watermarked image
to evade multiple surrogate watermarking models trained by the attacker itself,
and the perturbed watermarked image also evades the target watermarking model.
Our major contribution is to show that, both theoretically and empirically,
watermark-based AI-generated image detector is not robust to evasion attacks
even if the attacker does not have access to the watermarking model nor the
detection API.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Large Language Model based Autonomous Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11432v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11432v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents have long been a prominent research focus in both academic
and industry communities. Previous research in this field often focuses on
training agents with limited knowledge within isolated environments, which
diverges significantly from human learning processes, and thus makes the agents
hard to achieve human-like decisions. Recently, through the acquisition of vast
amounts of web knowledge, large language models (LLMs) have demonstrated
remarkable potential in achieving human-level intelligence. This has sparked an
upsurge in studies investigating LLM-based autonomous agents. In this paper, we
present a comprehensive survey of these studies, delivering a systematic review
of the field of LLM-based autonomous agents from a holistic perspective. More
specifically, we first discuss the construction of LLM-based autonomous agents,
for which we propose a unified framework that encompasses a majority of the
previous work. Then, we present a comprehensive overview of the diverse
applications of LLM-based autonomous agents in the fields of social science,
natural science, and engineering. Finally, we delve into the evaluation
strategies commonly used for LLM-based autonomous agents. Based on the previous
studies, we also present several challenges and future directions in this
field. To keep track of this field and continuously update our survey, we
maintain a repository of relevant references at
https://github.com/Paitesanshi/LLM-Agent-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 5 figures, 3 tables, has been accepted by frontiers of
  computer science (FCS), doi={10.1007/s11704-024-40231-1}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via
  Vision-Language Foundation Models <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Kuang, Hai Lin, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object navigation (ObjectNav) requires an agent to navigate through unseen
environments to find queried objects. Many previous methods attempted to solve
this task by relying on supervised or reinforcement learning, where they are
trained on limited household datasets with close-set objects. However, two key
challenges are unsolved: understanding free-form natural language instructions
that demand open-set objects, and generalizing to new environments in a
zero-shot manner. Aiming to solve the two challenges, in this paper, we propose
OpenFMNav, an Open-set Foundation Model based framework for zero-shot object
Navigation. We first unleash the reasoning abilities of large language models
(LLMs) to extract proposed objects from natural language instructions that meet
the user's demand. We then leverage the generalizability of large vision
language models (VLMs) to actively discover and detect candidate objects from
the scene, building a Versatile Semantic Score Map (VSSM). Then, by conducting
common sense reasoning on VSSM, our method can perform effective
language-guided exploration and exploitation of the scene and finally reach the
goal. By leveraging the reasoning and generalizing abilities of foundation
models, our method can understand free-form human instructions and perform
effective open-set zero-shot navigation in diverse environments. Extensive
experiments on the HM3D ObjectNav benchmark show that our method surpasses all
the strong baselines on all metrics, proving our method's effectiveness.
Furthermore, we perform real robot demonstrations to validate our method's
open-set-ness and generalizability to real-world environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024 Findings</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-24T00:00:00Z">2024-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">41</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models in Biomedical and Health Informatics: A
  Bibliometric <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huizi Yu, Lizhou Fan, Lingyao Li, Jiayan Zhou, Zihui Ma, Lu Xian, Wenyue Hua, Sijia He, Mingyu Jin, Yongfeng Zhang, Ashvin Gandhi, Xin Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have rapidly become important tools in
Biomedical and Health Informatics (BHI), enabling new ways to analyze data,
treat patients, and conduct research. This bibliometric review aims to provide
a panoramic view of how LLMs have been used in BHI by examining research
articles and collaboration networks from 2022 to 2023. It further explores how
LLMs can improve Natural Language Processing (NLP) applications in various BHI
areas like medical diagnosis, patient engagement, electronic health record
management, and personalized medicine. To do this, our bibliometric review
identifies key trends, maps out research networks, and highlights major
developments in this fast-moving field. Lastly, it discusses the ethical
concerns and practical challenges of using LLMs in BHI, such as data privacy
and reliable medical recommendations. Looking ahead, we consider how LLMs could
further transform biomedical research as well as healthcare delivery and
patient outcomes. This comprehensive review serves as a resource for
stakeholders in healthcare, including researchers, clinicians, and
policymakers, to understand the current state and future potential of LLMs in
BHI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 7 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LexDrafter: Terminology Drafting for Legislative Documents using
  Retrieval Augmented Generation <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Chouhan, Michael Gertz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increase in legislative documents at the EU, the number of new terms
and their definitions is increasing as well. As per the Joint Practical Guide
of the European Parliament, the Council and the Commission, terms used in legal
documents shall be consistent, and identical concepts shall be expressed
without departing from their meaning in ordinary, legal, or technical language.
Thus, while drafting a new legislative document, having a framework that
provides insights about existing definitions and helps define new terms based
on a document's context will support such harmonized legal definitions across
different regulations and thus avoid ambiguities. In this paper, we present
LexDrafter, a framework that assists in drafting Definitions articles for
legislative documents using retrieval augmented generation (RAG) and existing
term definitions present in different legislative documents. For this,
definition elements are built by extracting definitions from existing
documents. Using definition elements and RAG, a Definitions article can be
suggested on demand for a legislative document that is being drafted. We
demonstrate and evaluate the functionality of LexDrafter using a collection of
EU documents from the energy domain. The code for LexDrafter framework is
available at https://github.com/achouhan93/LexDrafter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Connecting the Dots: Inferring Patent Phrase Similarity with Retrieved
  Phrase Graphs <span class="chip">NAACL 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyi Peng, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the patent phrase similarity inference task, which measures the
semantic similarity between two patent phrases. As patent documents employ
legal and highly technical language, existing semantic textual similarity
methods that use localized contextual information do not perform satisfactorily
in inferring patent phrase similarity. To address this, we introduce a
graph-augmented approach to amplify the global contextual information of the
patent phrases. For each patent phrase, we construct a phrase graph that links
to its focal patents and a list of patents that are either cited by or cite
these focal patents. The augmented phrase embedding is then derived from
combining its localized contextual embedding with its global embedding within
the phrase graph. We further propose a self-supervised learning objective that
capitalizes on the retrieved topology to refine both the contextualized
embedding and the graph parameters in an end-to-end manner. Experimental
results from a unique patent phrase similarity dataset demonstrate that our
approach significantly enhances the representation of patent phrases, resulting
in marked improvements in similarity inference in a self-supervised fashion.
Substantial improvements are also observed in the supervised setting,
underscoring the potential benefits of leveraging retrieved phrase graph
augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Offer an Alternative to the Traditional Approach
  of Topic Modelling <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16248v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16248v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Mu, Chun Dong, Kalina Bontcheva, Xingyi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modelling, as a well-established unsupervised technique, has found
extensive use in automatically detecting significant topics within a corpus of
documents. However, classic topic modelling approaches (e.g., LDA) have certain
drawbacks, such as the lack of semantic understanding and the presence of
overlapping topics. In this work, we investigate the untapped potential of
large language models (LLMs) as an alternative for uncovering the underlying
topics within extensive text corpora. To this end, we introduce a framework
that prompts LLMs to generate topics from a given set of documents and
establish evaluation protocols to assess the clustering efficacy of LLMs. Our
findings indicate that LLMs with appropriate prompts can stand out as a viable
alternative, capable of generating relevant topic titles and adhering to human
guidelines to refine and merge topics. Through in-depth experiments and
evaluation, we summarise the advantages and constraints of employing LLMs in
topic extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Sequence-to-Sequence Models for Abstractive Text Summarization
  Using Meta Heuristic Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Saxena, Ashutosh Ranjan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As human society transitions into the information age, reduction in our
attention span is a contingency, and people who spend time reading lengthy news
articles are decreasing rapidly and the need for succinct information is higher
than ever before. Therefore, it is essential to provide a quick overview of
important news by concisely summarizing the top news article and the most
intuitive headline. When humans try to make summaries, they extract the
essential information from the source and add useful phrases and grammatical
annotations from the original extract. Humans have a unique ability to create
abstractions. However, automatic summarization is a complicated problem to
solve. The use of sequence-to-sequence (seq2seq) models for neural abstractive
text summarization has been ascending as far as prevalence. Numerous innovative
strategies have been proposed to develop the current seq2seq models further,
permitting them to handle different issues like saliency, familiarity, and
human lucidness and create excellent synopses. In this article, we aimed toward
enhancing the present architectures and models for abstractive text
summarization. The modifications have been aimed at fine-tuning
hyper-parameters, attempting specific encoder-decoder combinations. We examined
many experiments on an extensively used CNN/DailyMail dataset to check the
effectiveness of various models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SQL-Encoder: Improving NL2SQL In-Context Learning Through a
  Context-Aware Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Pourreza, Davood Rafiei, Yuxi Feng, Raymond Li, Zhenan Fan, Weiwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting structural similarity between queries is essential for selecting
examples in in-context learning models. However, assessing structural
similarity based solely on the natural language expressions of queries, without
considering SQL queries, presents a significant challenge. This paper explores
the significance of this similarity metric and proposes a model for accurately
estimating it. To achieve this, we leverage a dataset comprising 170k question
pairs, meticulously curated to train a similarity prediction model. Our
comprehensive evaluation demonstrates that the proposed model adeptly captures
the structural similarity between questions, as evidenced by improvements in
Kendall-Tau distance and precision@k metrics. Notably, our model outperforms
strong competitive embedding models from OpenAI and Cohere. Furthermore,
compared to these competitive models, our proposed encoder enhances the
downstream performance of NL2SQL models in 1-shot in-context learning scenarios
by 1-2\% for GPT-3.5-turbo, 4-8\% for CodeLlama-7B, and 2-3\% for
CodeLlama-13B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALoRA: Allocating Low-Rank Adaptation for Fine-tuning Large Language
  Models <span class="chip">NAACL-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zequan Liu, Jiawen Lyn, Wei Zhu, Xing Tian, Yvette Graham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) is widely studied for its
effectiveness and efficiency in the era of large language models. Low-rank
adaptation (LoRA) has demonstrated commendable performance as a popular and
representative method. However, it is implemented with a fixed intrinsic rank
that might not be the ideal setting for the downstream tasks. Recognizing the
need for more flexible downstream task adaptation, we extend the methodology of
LoRA to an innovative approach we call allocating low-rank adaptation (ALoRA)
that enables dynamic adjustments to the intrinsic rank during the adaptation
process. First, we propose a novel method, AB-LoRA, that can effectively
estimate the importance score of each LoRA rank. Second, guided by AB-LoRA, we
gradually prune abundant and negatively impacting LoRA ranks and allocate the
pruned LoRA budgets to important Transformer modules needing higher ranks. We
have conducted experiments on various tasks, and the experimental results
demonstrate that our ALoRA method can outperform the recent baselines with
comparable tunable parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subspace Defense: Discarding Adversarial Perturbations by Learning a
  Subspace for Clean Signals <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zheng, Yuhao Zhou, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are notoriously vulnerable to adversarial attacks
that place carefully crafted perturbations on normal examples to fool DNNs. To
better understand such attacks, a characterization of the features carried by
adversarial examples is needed. In this paper, we tackle this challenge by
inspecting the subspaces of sample features through spectral analysis. We first
empirically show that the features of either clean signals or adversarial
perturbations are redundant and span in low-dimensional linear subspaces
respectively with minimal overlap, and the classical low-dimensional subspace
projection can suppress perturbation features out of the subspace of clean
signals. This makes it possible for DNNs to learn a subspace where only
features of clean signals exist while those of perturbations are discarded,
which can facilitate the distinction of adversarial examples. To prevent the
residual perturbations that is inevitable in subspace learning, we propose an
independence criterion to disentangle clean signals from perturbations.
Experimental results show that the proposed strategy enables the model to
inherently suppress adversaries, which not only boosts model robustness but
also motivates new directions of effective adversarial defense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Semantic Reconstruction to Mitigate Hallucinations in
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minchan Kim, Minyeong Kim, Junik Bae, Suhwan Choi, Sungkyung Kim, Buru Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations in vision-language models pose a significant challenge to
their reliability, particularly in the generation of long captions. Current
methods fall short of accurately identifying and mitigating these
hallucinations. To address this issue, we introduce ESREAL, a novel
unsupervised learning framework designed to suppress the generation of
hallucinations through accurate localization and penalization of hallucinated
tokens. Initially, ESREAL creates a reconstructed image based on the generated
caption and aligns its corresponding regions with those of the original image.
This semantic reconstruction aids in identifying both the presence and type of
token-level hallucinations within the generated caption. Subsequently, ESREAL
computes token-level hallucination scores by assessing the semantic similarity
of aligned regions based on the type of hallucination. Finally, ESREAL employs
a proximal policy optimization algorithm, where it selectively penalizes
hallucinated tokens according to their token-level hallucination scores. Our
framework notably reduces hallucinations in LLaVA, InstructBLIP, and mPLUG-Owl2
by 32.81%, 27.08%, and 7.46% on the CHAIR metric. This improvement is achieved
solely through signals derived from the image itself, without the need for any
image-text pairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Korean Bio-Medical Corpus (KBMC) for Medical Named Entity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungjoo Byun, Jiseung Hong, Sumin Park, Dongjun Jang, Jean Seo, Minseok Kim, Chaeyoung Oh, Hyopil Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition (NER) plays a pivotal role in medical Natural
Language Processing (NLP). Yet, there has not been an open-source medical NER
dataset specifically for the Korean language. To address this, we utilized
ChatGPT to assist in constructing the KBMC (Korean Bio-Medical Corpus), which
we are now presenting to the public. With the KBMC dataset, we noticed an
impressive 20% increase in medical NER performance compared to models trained
on general Korean NER datasets. This research underscores the significant
benefits and importance of using specialized tools and datasets, like ChatGPT,
to enhance language processing in specialized fields such as healthcare.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Happens to a <span class="highlight-title">Dataset</span> Transformed by a Projection-based Concept
  Removal Method? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the behavior of methods that use linear projections to remove
information about a concept from a language representation, and we consider the
question of what happens to a dataset transformed by such a method. A
theoretical analysis and experiments on real-world and synthetic data show that
these methods inject strong statistical dependencies into the transformed
datasets. After applying such a method, the representation space is highly
structured: in the transformed space, an instance tends to be located near
instances of the opposite label. As a consequence, the original labeling can in
some cases be reconstructed by applying an anti-clustering method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Little Leak Will Sink a Great Ship: <span class="highlight-title">Survey</span> of Transparency for Large
  Language Models from Start to Finish 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kaneko, Timothy Baldwin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are trained on massive web-crawled corpora. This
poses risks of leakage, including personal information, copyrighted texts, and
benchmark datasets. Such leakage leads to undermining human trust in AI due to
potential unauthorized generation of content or overestimation of performance.
We establish the following three criteria concerning the leakage issues: (1)
leakage rate: the proportion of leaked data in training data, (2) output rate:
the ease of generating leaked data, and (3) detection rate: the detection
performance of leaked versus non-leaked data. Despite the leakage rate being
the origin of data leakage issues, it is not understood how it affects the
output rate and detection rate. In this paper, we conduct an experimental
survey to elucidate the relationship between the leakage rate and both the
output rate and detection rate for personal information, copyrighted texts, and
benchmark data. Additionally, we propose a self-detection approach that uses
few-shot learning in which LLMs detect whether instances are present or absent
in their training data, in contrast to previous methods that do not employ
explicit learning. To explore the ease of generating leaked information, we
create a dataset of prompts designed to elicit personal information,
copyrighted text, and benchmarks from LLMs. Our experiments reveal that LLMs
produce leaked information in most cases despite less such data in their
training set. This indicates even small amounts of leaked data can greatly
affect outputs. Our self-detection method showed superior performance compared
to existing detection methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Lexical Ambiguity Detection and Word Sense Disambiguation <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miuru Abeysiriwardana, Deshan Sumanathilaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores techniques that focus on understanding and resolving
ambiguity in language within the field of natural language processing (NLP),
highlighting the complexity of linguistic phenomena such as polysemy and
homonymy and their implications for computational models. Focusing extensively
on Word Sense Disambiguation (WSD), it outlines diverse approaches ranging from
deep learning techniques to leveraging lexical resources and knowledge graphs
like WordNet. The paper introduces cutting-edge methodologies like word sense
extension (WSE) and neuromyotonic approaches, enhancing disambiguation accuracy
by predicting new word senses. It examines specific applications in biomedical
disambiguation and language specific optimisation and discusses the
significance of cognitive metaphors in discourse analysis. The research
identifies persistent challenges in the field, such as the scarcity of sense
annotated corpora and the complexity of informal clinical texts. It concludes
by suggesting future directions, including using large language models, visual
WSD, and multilingual WSD systems, emphasising the ongoing evolution in
addressing lexical complexities in NLP. This thinking perspective highlights
the advancement in this field to enable computers to understand language more
accurately.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, 3 tables, Accepted by 20th IEEE International
  Colloquium on Signal Processing & its Applications (CSPA 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WangchanLion and WangchanX MRC Eval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wannaphong Phatthiyaphaibun, Surapon Nonesung, Patomporn Payoungkhamdee, Peerat Limkonchotiwat, Can Udomcharoenchaikit, Ekapol Chuangsuwanich, Sarana Nutanong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This technical report describes the development of WangchanLion, an
instruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in
the Thai language. Our model is based on SEA-LION and a collection of
instruction following datasets. To promote open research and reproducibility,
we publically release all training data, code, and the final model weights
under the Apache-2 license. To assess the contextual understanding capability,
we conducted extensive experimental studies using two Thai MRC datasets, XQuAD
and Iapp_wiki_qa_squad. Experimental results demonstrate the model's ability to
comprehend the context and produce an answer faithful to the reference one in
0-shot and 1-shot settings. In addition, our evaluation goes beyond the
traditional MRC. We propose a new evaluation scheme assessing the answer's
correctness, helpfulness, conciseness, and contextuality. Evaluation results
provide insight into how we can improve our model in the future. Our code is
public at https://github.com/vistec-AI/WangchanLion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Label <span class="highlight-title">Dataset</span> of French Fake News: Human and Machine Insights <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Icard, François Maine, Morgane Casanova, Géraud Faye, Julien Chanson, Guillaume Gadek, Ghislain Atemezing, François Bancilhon, Paul Égré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a corpus of 100 documents, OBSINFOX, selected from 17 sources of
French press considered unreliable by expert agencies, annotated using 11
labels by 8 annotators. By collecting more labels than usual, by more
annotators than is typically done, we can identify features that humans
consider as characteristic of fake news, and compare them to the predictions of
automated classifiers. We present a topic and genre analysis using Gate Cloud,
indicative of the prevalence of satire-like text in the corpus. We then use the
subjectivity analyzer VAGO, and a neural version of it, to clarify the link
between ascriptions of the label Subjective and ascriptions of the label Fake
News. The annotated dataset is available online at the following url:
https://github.com/obs-info/obsinfox
  Keywords: Fake News, Multi-Labels, Subjectivity, Vagueness, Detail, Opinion,
Exaggeration, French Press
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper to appear in the Proceedings of the 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs as Compiler for Arabic Programming Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serry Sibaee, Omar Najar, Lahouri Ghouti, Anis Koubaa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we introduce APL (Arabic Programming Language) that uses Large
language models (LLM) as semi-compiler to covert Arabic text code to python
code then run the code. Designing a full pipeline from the structure of the APL
text then a prompt (using prompt engineering) then running the prodcued python
code using PyRunner. This project has a three parts first python library, a
playground with simple interface and this research paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Argument Quality Assessment in the Age of Instruction-Following Large
  Language Models <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Henning Wachsmuth, Gabriella Lapesa, Elena Cabrio, Anne Lauscher, Joonsuk Park, Eva Maria Vecchi, Serena Villata, Timon Ziegenbein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The computational treatment of arguments on controversial issues has been
subject to extensive NLP research, due to its envisioned impact on opinion
formation, decision making, writing education, and the like. A critical task in
any such application is the assessment of an argument's quality - but it is
also particularly challenging. In this position paper, we start from a brief
survey of argument quality research, where we identify the diversity of quality
notions and the subjectiveness of their perception as the main hurdles towards
substantial progress on argument quality assessment. We argue that the
capabilities of instruction-following large language models (LLMs) to leverage
knowledge across contexts enable a much more reliable assessment. Rather than
just fine-tuning LLMs towards leaderboard chasing on assessment tasks, they
need to be instructed systematically with argumentation theories and scenarios
as well as with ways to solve argument-related problems. We discuss the
real-world opportunities and ethical issues emerging thereby.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qibo: A Large Language Model for Traditional Chinese Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heyi Zhang, Xin Wang, Zhaopeng Meng, Yongzhe Jia, Dawei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of Artificial Intelligence, Large Language Models (LLMs) have
demonstrated significant advances in user intent understanding and response in
a number of specialized domains, including medicine, law, and finance. However,
in the unique domain of traditional Chinese medicine (TCM), the performance
enhancement of LLMs is challenged by the essential differences between its
theories and modern medicine, as well as the lack of specialized corpus
resources. In this paper, we aim to construct and organize a professional
corpus in the field of TCM, to endow the large model with professional
knowledge that is characteristic of TCM theory, and to successfully develop the
Qibo model based on LLaMA, which is the first LLM in the field of TCM to
undergo a complete training process from pre-training to Supervised Fine-Tuning
(SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for
evaluating the performance of LLMs, which is a specialized tool for evaluating
the performance of LLMs in the TCM domain. This tool will provide an important
basis for quantifying and comparing the understanding and application
capabilities of different models in the field of traditional Chinese medicine,
and provide guidance for future research directions and practical applications
of intelligent assistants for traditional Chinese medicine. Finally, we
conducted sufficient experiments to prove that Qibo has good performance in the
field of traditional Chinese medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monotonic Paraphrasing Improves Generalization of Language Model
  <span class="highlight-title">Prompt</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Liu, Fei Wang, Nan Xu, Tianyi Yan, Tao Meng, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance of large language models (LLMs) may vary with different prompts
or instructions of even the same task. One commonly recognized factor for this
phenomenon is the model's familiarity with the given prompt or instruction,
which is typically estimated by its perplexity. However, finding the prompt
with the lowest perplexity is challenging, given the enormous space of possible
prompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara),
an end-to-end decoding strategy that paraphrases given prompts or instructions
into their lower perplexity counterparts based on an ensemble of a paraphrase
LM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or
instruction executor) that constrains the generation for lower perplexity. The
ensemble decoding process can efficiently paraphrase the original prompt
without altering its semantic meaning, while monotonically decreasing the
perplexity of each generation as calculated by the target LM. We explore in
detail both greedy and search-based decoding as two alternative decoding
schemes of MonoPara. Notably, MonoPara does not require any training and can
monotonically lower the perplexity of the paraphrased prompt or instruction,
leading to improved performance of zero-shot LM prompting as evaluated on a
wide selection of tasks. In addition, MonoPara is also shown to effectively
improve LMs' generalization on perturbed and unseen task instructions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Node Classification via Semantic-Structural Attention-Enhanced Graph
  Convolutional Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph data, also known as complex network data, is omnipresent across various
domains and applications. Prior graph neural network models primarily focused
on extracting task-specific structural features through supervised learning
objectives, but they fell short in capturing the inherent semantic and
structural features of the entire graph. In this paper, we introduce the
semantic-structural attention-enhanced graph convolutional network (SSA-GCN),
which not only models the graph structure but also extracts generalized
unsupervised features to enhance vertex classification performance. The
SSA-GCN's key contributions lie in three aspects: firstly, it derives semantic
information through unsupervised feature extraction from a knowledge graph
perspective; secondly, it obtains structural information through unsupervised
feature extraction from a complex network perspective; and finally, it
integrates these features through a cross-attention mechanism. By leveraging
these features, we augment the graph convolutional network, thereby enhancing
the model's generalization capabilities. Our experiments on the Cora and
CiteSeer datasets demonstrate the performance improvements achieved by our
proposed method. Furthermore, our approach also exhibits excellent accuracy
under privacy settings, making it a robust and effective solution for graph
data analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral
  Therapy-based Mental Health Question Answering <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbin Na
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in artificial intelligence highlight the potential of
language models in psychological health support. While models trained on data
from mental health service platform have achieved preliminary success,
challenges persist in areas such as data scarcity, quality, and ensuring a
solid foundation in psychological techniques. To address these challenges, this
study introduces a novel approach to enhance the precision and efficacy of
psychological support through large language models. Specifically, we design a
specific prompt derived from principles of Cognitive Behavioral Therapy (CBT)
and have generated the CBT QA dataset, specifically for Chinese psychological
health Q&A based on CBT structured intervention strategies. Unlike previous
methods, our dataset emphasizes professional and structured response. Utilizing
this dataset, we fine-tuned the large language model, giving birth to CBT-LLM,
the large-scale language model specifically designed for Cognitive Behavioral
Therapy techniques. Empirical evaluations demonstrate that CBT-LLM excels in
generating structured, professional, and highly relevant responses in
psychological health support tasks, showcasing its practicality and quality.
The model is available on Hugging Face:
https://huggingface.co/Hongbin37/CBT-LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BIMCV-R: A Landmark <span class="highlight-title">Dataset</span> for 3D CT Text-Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinda Chen, Che Liu, Xiaoyu Liu, Rossella Arcucci, Zhiwei Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning integration of 3D medical imaging into healthcare has led to a
substantial increase in the workload of medical professionals. To assist
clinicians in their diagnostic processes and alleviate their workload, the
development of a robust system for retrieving similar case studies presents a
viable solution. While the concept holds great promise, the field of 3D medical
text-image retrieval is currently limited by the absence of robust evaluation
benchmarks and curated datasets. To remedy this, our study presents a
groundbreaking dataset, BIMCV-R (This dataset will be released upon
acceptance.), which includes an extensive collection of 8,069 3D CT volumes,
encompassing over 2 million slices, paired with their respective radiological
reports. Expanding upon the foundational work of our dataset, we craft a
retrieval strategy, MedFinder. This approach employs a dual-stream network
architecture, harnessing the potential of large language models to advance the
field of medical image retrieval beyond existing text-image retrieval
solutions. It marks our preliminary step towards developing a system capable of
facilitating text-to-image, image-to-text, and keyword-based retrieval tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VQPy: An Object-Oriented Approach to Modern Video Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01623v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01623v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shan Yu, Zhenting Zhu, Yu Chen, Hanchen Xu, Pengzhan Zhao, Yang Wang, Arthi Padmanabhan, Hugo Latapie, Harry Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video analytics is widely used in contemporary systems and services. At the
forefront of video analytics are video queries that users develop to find
objects of particular interest. Building upon the insight that video objects
(e.g., human, animals, cars, etc.), the center of video analytics, are similar
in spirit to objects modeled by traditional object-oriented languages, we
propose to develop an object-oriented approach to video analytics. This
approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant
with constructs that make it easy for users to express video objects and their
interactions$\unicode{x2015}$as well as an extensible backend that can
automatically construct and optimize pipelines based on video objects. We have
implemented and open-sourced VQPy, which has been productized in Cisco as part
of its DeepVision framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MLSys'24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unmasking and Improving Data Credibility: A Study with <span class="highlight-title">Dataset</span>s for
  Training Harmless Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11202v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11202v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaowei Zhu, Jialu Wang, Hao Cheng, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have shown promise in various tasks but can be affected by
undesired data during training, fine-tuning, or alignment. For example, if some
unsafe conversations are wrongly annotated as safe ones, the model fine-tuned
on these samples may be harmful. Therefore, the correctness of annotations,
i.e., the credibility of the dataset, is important. This study focuses on the
credibility of real-world datasets, including the popular benchmarks Jigsaw
Civil Comments, Anthropic Harmless & Red Team, PKU BeaverTails & SafeRLHF, that
can be used for training a harmless language model. Given the cost and
difficulty of cleaning these datasets by humans, we introduce a systematic
framework for evaluating the credibility of datasets, identifying label errors,
and evaluating the influence of noisy labels in the curated language data,
specifically focusing on unsafe comments and conversation classification. With
the framework, we find and fix an average of 6.16% label errors in 11 datasets
constructed from the above benchmarks. The data credibility and downstream
learning performance can be remarkably improved by directly fixing label
errors, indicating the significance of cleaning existing real-world datasets.
We provide an open-source tool, Docta, for data cleaning at
https://github.com/Docta-ai/docta.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlocking the Potential of Chat<span class="highlight-title">GPT</span>: A Comprehensive Exploration of its
  Applications, Advantages, Limitations, and Future Directions in Natural
  Language Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02017v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02017v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walid Hariri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have revolutionized the field of artificial
intelligence and have been used in various applications. Among these models,
ChatGPT (Chat Generative Pre-trained Transformer) has been developed by OpenAI,
it stands out as a powerful tool that has been widely adopted. ChatGPT has been
successfully applied in numerous areas, including chatbots, content generation,
language translation, personalized recommendations, and even medical diagnosis
and treatment. Its success in these applications can be attributed to its
ability to generate human-like responses, understand natural language, and
adapt to different contexts. Its versatility and accuracy make it a powerful
tool for natural language processing (NLP). However, there are also limitations
to ChatGPT, such as its tendency to produce biased responses and its potential
to perpetuate harmful language patterns. This article provides a comprehensive
overview of ChatGPT, its applications, advantages, and limitations.
Additionally, the paper emphasizes the importance of ethical considerations
when using this robust tool in real-world scenarios. Finally, This paper
contributes to ongoing discussions surrounding artificial intelligence and its
impact on vision and NLP domains by providing insights into prompt engineering
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating <span class="highlight-title">Prompt</span> Complexity for Zero-Shot Classification: A Study of
  Large Language Models in Computational Social Science <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14310v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14310v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Mu, Ben P. Wu, William Thorne, Ambrose Robinson, Nikolaos Aletras, Carolina Scarton, Kalina Bontcheva, Xingyi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-tuned Large Language Models (LLMs) have exhibited impressive
language understanding and the capacity to generate responses that follow
specific prompts. However, due to the computational demands associated with
training these models, their applications often adopt a zero-shot setting. In
this paper, we evaluate the zero-shot performance of two publicly accessible
LLMs, ChatGPT and OpenAssistant, in the context of six Computational Social
Science classification tasks, while also investigating the effects of various
prompting strategies. Our experiments investigate the impact of prompt
complexity, including the effect of incorporating label definitions into the
prompt; use of synonyms for label names; and the influence of integrating past
memories during foundation model training. The findings indicate that in a
zero-shot setting, current LLMs are unable to match the performance of smaller,
fine-tuned baseline transformer models (such as BERT-large). Additionally, we
find that different prompting strategies can significantly affect
classification accuracy, with variations in accuracy and F1 scores exceeding
10\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C-TPT: Calibrated Test-Time <span class="highlight-title">Prompt</span> Tuning for Vision-Language Models via
  Text Feature Dispersion <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark Hasegawa-Johnson, Yingzhen Li, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In deep learning, test-time adaptation has gained attention as a method for
model fine-tuning without the need for labeled data. A prime exemplification is
the recently proposed test-time prompt tuning for large-scale vision-language
models such as CLIP. Unfortunately, these prompts have been mainly developed to
improve accuracy, overlooking the importance of calibration, which is a crucial
aspect for quantifying prediction uncertainty. However, traditional calibration
methods rely on substantial amounts of labeled data, making them impractical
for test-time scenarios. To this end, this paper explores calibration during
test-time prompt tuning by leveraging the inherent properties of CLIP. Through
a series of observations, we find that the prompt choice significantly affects
the calibration in CLIP, where the prompts leading to higher text feature
dispersion result in better-calibrated predictions. Introducing the Average
Text Feature Dispersion (ATFD), we establish its relationship with calibration
error and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT),
for optimizing prompts during test-time with enhanced calibration. Through
extensive experiments on different CLIP architectures and datasets, we show
that C-TPT can effectively improve the calibration of test-time prompt tuning
without needing labeled data. The code is publicly accessible at
https://github.com/hee-suk-yoon/C-TPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLEX: Continuous Length Extrapolation for Large Language Models <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16450v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16450v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based Large Language Models (LLMs) are pioneering advances in
many natural language processing tasks, however, their exceptional capabilities
are restricted within the preset context window of Transformer. Position
Embedding (PE) scaling methods, while effective in extending the context window
to a specific length, demonstrate either notable limitations in their
extrapolation abilities or sacrificing partial performance within the context
window. Length extrapolation methods, although theoretically capable of
extending the context window beyond the training sequence length, often
underperform in practical long-context applications. To address these
challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We
generalise the PE scaling approaches to model the continuous dynamics by
ordinary differential equations over the length scaling factor, thereby
overcoming the constraints of current PE scaling methods designed for specific
lengths. Moreover, by extending the dynamics to desired context lengths beyond
the training sequence length, CLEX facilitates the length extrapolation with
impressive performance in practical tasks. We demonstrate that CLEX can be
seamlessly incorporated into LLMs equipped with Rotary Position Embedding, such
as LLaMA and GPT-NeoX, with negligible impact on training and inference
latency. Experimental results reveal that CLEX can effectively extend the
context window to over 4x or almost 8x training length, with no deterioration
in performance. Furthermore, when evaluated on the practical LongBench
benchmark, our model trained on a 4k length exhibits competitive performance
against state-of-the-art open-source models trained on context lengths up to
32k. Our code is available at https://github.com/DAMO-NLP-SG/CLEX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Examining Temporalities on Stance Detection towards COVID-19 Vaccination <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04806v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04806v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Mu, Mali Jin, Kalina Bontcheva, Xingyi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous studies have highlighted the importance of vaccination as an
effective strategy to control the transmission of the COVID-19 virus. It is
crucial for policymakers to have a comprehensive understanding of the public's
stance towards vaccination on a large scale. However, attitudes towards
COVID-19 vaccination, such as pro-vaccine or vaccine hesitancy, have evolved
over time on social media. Thus, it is necessary to account for possible
temporal shifts when analysing these stances. This study aims to examine the
impact of temporal concept drift on stance detection towards COVID-19
vaccination on Twitter. To this end, we evaluate a range of transformer-based
models using chronological (splitting the training, validation, and test sets
in order of time) and random splits (randomly splitting these three sets) of
social media data. Our findings reveal significant discrepancies in model
performance between random and chronological splits in several existing
COVID-19-related datasets; specifically, chronological splits significantly
reduce the accuracy of stance classification. Therefore, real-world stance
detection approaches need to be further refined to incorporate temporal factors
as a key consideration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual
  Natural Language Generalization <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16694v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16694v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiwei Peng, Yekun Chai, Xuhong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have made significant progress in generating
codes from textual prompts. However, existing benchmarks have mainly
concentrated on translating English prompts to multilingual codes or have been
constrained to very limited natural languages (NLs). These benchmarks have
overlooked the vast landscape of massively multilingual NL to multilingual
code, leaving a critical gap in the evaluation of multilingual LLMs. In
response, we introduce HumanEval-XL, a massively multilingual code generation
benchmark specifically crafted to address this deficiency. HumanEval-XL
establishes connections between 23 NLs and 12 programming languages (PLs), and
comprises of a collection of 22,080 prompts with an average of 8.33 test cases.
By ensuring parallel data across multiple NLs and PLs, HumanEval-XL offers a
comprehensive evaluation platform for multilingual LLMs, allowing the
assessment of the understanding of different NLs. Our work serves as a
pioneering step towards filling the void in evaluating NL generalization in the
area of multilingual code generation. We make our evaluation code and data
publicly available at \url{https://github.com/FloatAI/humaneval-xl}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spacerini: Plug-and-play Search Engines with Pyserini and Hugging Face 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Akiki, Odunayo Ogundepo, Aleksandra Piktus, Xinyu Zhang, Akintunde Oladipo, Jimmy Lin, Martin Potthast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Spacerini, a tool that integrates the Pyserini toolkit for
reproducible information retrieval research with Hugging Face to enable the
seamless construction and deployment of interactive search engines. Spacerini
makes state-of-the-art sparse and dense retrieval models more accessible to
non-IR practitioners while minimizing deployment effort. This is useful for NLP
researchers who want to better understand and validate their research by
performing qualitative analyses of training corpora, for IR researchers who
want to demonstrate new retrieval models integrated into the growing Pyserini
ecosystem, and for third parties reproducing the work of other researchers.
Spacerini is open source and includes utilities for loading, preprocessing,
indexing, and deploying search engines locally and remotely. We demonstrate a
portfolio of 13 search engines created with Spacerini for different use cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PWESuite: Phonetic Word Embeddings and Tasks They Facilitate <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02541v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02541v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vilém Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson, Nathaniel Robinson, Mrinmaya Sachan, David Mortensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mapping words into a fixed-dimensional vector space is the backbone of modern
NLP. While most word embedding methods successfully encode semantic
information, they overlook phonetic information that is crucial for many tasks.
We develop three methods that use articulatory features to build phonetically
informed word embeddings. To address the inconsistent evaluation of existing
phonetic word embedding methods, we also contribute a task suite to fairly
evaluate past, current, and future methods. We evaluate both (1) intrinsic
aspects of phonetic word embeddings, such as word retrieval and correlation
with sound similarity, and (2) extrinsic performance on tasks such as rhyme and
cognate detection and sound analogies. We hope our task suite will promote
reproducibility and inspire future phonetic embedding research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.04369v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.04369v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ang Li, Qiangchao Chen, Yiquan Wu, Ming Cai, Xiang Zhou, Fei Wu, Kun Kuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Confusing charge prediction is a challenging task in legal AI, which involves
predicting confusing charges based on fact descriptions. While existing charge
prediction methods have shown impressive performance, they face significant
challenges when dealing with confusing charges, such as Snatch and Robbery. In
the legal domain, constituent elements play a pivotal role in distinguishing
confusing charges. Constituent elements are fundamental behaviors underlying
criminal punishment and have subtle distinctions among charges. In this paper,
we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces
domain knowledge regarding constituent elements to guide the model in making
judgments on confusing charges, much like a judge's reasoning process.
Specifically, we first construct a legal knowledge graph containing constituent
elements to help select keywords for each charge, forming a word bag.
Subsequently, to guide the model's attention towards the differentiating
information for each charge within the context, we expand the attention
mechanism and introduce a new loss function with attention supervision through
words in the word bag. We construct the confusing charges dataset from
real-world judicial documents. Experiments demonstrate the effectiveness of our
method, especially in maintaining exceptional performance in imbalanced label
distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UrbanCLIP: Learning Text-enhanced Urban Region Profiling with
  Contrastive Language-Image <span class="highlight-title">Pretrain</span>ing from the Web 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18340v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18340v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen, Roger Zimmermann, Yuxuan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban region profiling from web-sourced data is of utmost importance for
urban planning and sustainable development. We are witnessing a rising trend of
LLMs for various fields, especially dealing with multi-modal data research such
as vision-language learning, where the text modality serves as a supplement
information for the image. Since textual modality has never been introduced
into modality combinations in urban region profiling, we aim to answer two
fundamental questions in this paper: i) Can textual modality enhance urban
region profiling? ii) and if so, in what ways and with regard to which aspects?
To answer the questions, we leverage the power of Large Language Models (LLMs)
and introduce the first-ever LLM-enhanced framework that integrates the
knowledge of textual modality into urban imagery profiling, named LLM-enhanced
Urban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP).
Specifically, it first generates a detailed textual description for each
satellite image by an open-source Image-to-Text LLM. Then, the model is trained
on the image-text pairs, seamlessly unifying natural language supervision for
urban visual representation learning, jointly with contrastive loss and
language modeling loss. Results on predicting three urban indicators in four
major Chinese metropolises demonstrate its superior performance, with an
average improvement of 6.1% on R^2 compared to the state-of-the-art methods.
Our code and the image-language dataset will be released upon paper
notification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by The Web Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Large Language Models for Enhanced NLP Task Performance
  through Knowledge Distillation and Optimized Training Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09282v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09282v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yining Huang, Keke Tang, Meilian Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging Large Language Models (LLMs) like GPT-4 have revolutionized Natural
Language Processing (NLP), showing potential in traditional tasks such as Named
Entity Recognition (NER). Our study explores a three-phase training strategy
that harnesses GPT-4's capabilities to enhance the BERT model's performance on
NER. Initially, GPT-4 annotates a subset of the CONLL2003 and additional BBC
dataset without fine-tuning. We then train BERT using a mix of original and
LLM-annotated data, analyzing the efficacy of LLM annotations against
traditional methods. The second phase involves comparative experiments with
different training regimens, assessing the synergy between distilled and
original data. We observe that sequential strategies, particularly a simple mix
of training first with distilled data followed by original data, significantly
boost performance. In the third phase, we investigate various data blending
techniques, including sigmoid and power decay functions, to optimize the
training process further. Our results indicate that a strategic mix of
distilled and original data markedly elevates the NER capabilities of BERT. Our
approach presents a scalable methodology that reduces manual annotation costs
and increases efficiency, making it especially pertinent in resource-limited
and closed-network environments. The study concludes that while the 'Simple
Mix' strategy yields the best results, understanding its underlying mechanisms
requires further research. Future work will also focus on refining prompt
designs and enhancing annotation selection processes, aiming to extend our
methodology to diverse NLP tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.10647v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.10647v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rima Hazra, Sayan Layek, Somnath Banerjee, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly advancing field of artificial intelligence, the concept of
Red-Teaming or Jailbreaking large language models (LLMs) has emerged as a
crucial area of study. This approach is especially significant in terms of
assessing and enhancing the safety and robustness of these models. This paper
investigates the intricate consequences of such modifications through model
editing, uncovering a complex relationship between enhancing model accuracy and
preserving its ethical integrity. Our in-depth analysis reveals a striking
paradox: while injecting accurate information is crucial for model reliability,
it can paradoxically destabilize the model's foundational framework, resulting
in unpredictable and potentially unsafe behaviors. Additionally, we propose a
benchmark dataset NicheHazardQA to investigate this unsafe behavior both within
the same and cross topical domain. This aspect of our research sheds light on
how the edits, impact the model's safety metrics and guardrails. Our findings
show that model editing serves as a cost-effective tool for topical red-teaming
by methodically applying targeted edits and evaluating the resultant model
behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review.
  {https://huggingface.co/datasets/SoftMINER-Group/NicheHazardQA}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think-on-Graph: Deep and Responsible Reasoning of Large Language Model
  on Knowledge Graph <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.07697v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.07697v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Lionel M. Ni, Heung-Yeung Shum, Jian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) have achieved significant success in
various tasks, they often struggle with hallucination problems, especially in
scenarios requiring deep and responsible reasoning. These issues could be
partially addressed by introducing external knowledge graphs (KG) in LLM
reasoning. In this paper, we propose a new LLM-KG integrating paradigm
``$\hbox{LLM}\otimes\hbox{KG}$'' which treats the LLM as an agent to
interactively explore related entities and relations on KGs and perform
reasoning based on the retrieved knowledge. We further implement this paradigm
by introducing a new approach called Think-on-Graph (ToG), in which the LLM
agent iteratively executes beam search on KG, discovers the most promising
reasoning paths, and returns the most likely reasoning results. We use a number
of well-designed experiments to examine and illustrate the following advantages
of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has
the ability of knowledge traceability and knowledge correctability by
leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible
plug-and-play framework for different LLMs, KGs and prompting strategies
without any additional training cost; 4) the performance of ToG with small LLM
models could exceed large LLM such as GPT-4 in certain scenarios and this
reduces the cost of LLM deployment and application. As a training-free method
with lower computational cost and better generality, ToG achieves overall SOTA
in 6 out of 9 datasets where most previous SOTAs rely on additional training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture-of-<span class="highlight-title">Prompt</span>-Experts for Multi-modal Semantic Understanding <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11311v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11311v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Wu, Hsiu-Yuan Huang, Fanyi Qu, Yunfang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep multimodal semantic understanding that goes beyond the mere superficial
content relation mining has received increasing attention in the realm of
artificial intelligence. The challenges of collecting and annotating
high-quality multi-modal data have underscored the significance of few-shot
learning. In this paper, we focus on two critical tasks under this context:
few-shot multi-modal sarcasm detection (MSD) and multi-modal sentiment analysis
(MSA). To address them, we propose Mixture-of-Prompt-Experts with Block-Aware
Prompt Fusion (MoPE-BAF), a novel multi-modal soft prompt framework based on
the unified vision-language model (VLM). Specifically, we design three experts
of soft prompts: a text prompt and an image prompt that extract
modality-specific features to enrich the single-modal representation, and a
unified prompt to assist multi-modal interaction. Additionally, we reorganize
Transformer layers into several blocks and introduce cross-modal prompt
attention between adjacent blocks, which smoothens the transition from
single-modal representation to multi-modal fusion. On both MSD and MSA datasets
in few-shot setting, our proposed model not only surpasses the 8.2B model
InstructBLIP with merely 2% parameters (150M), but also significantly
outperforms other widely-used prompt methods on VLMs or task-specific methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LREC-COLING 2024, Long Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models
  through Logic <span class="chip">COLING 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13339v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13339v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun Chu, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models have showcased their remarkable
generalizability across various domains. However, their reasoning abilities
still have significant room for improvement, especially when confronted with
scenarios requiring multi-step reasoning. Although large language models
possess extensive knowledge, their reasoning often fails to effectively utilize
this knowledge to establish a coherent thinking paradigm. These models
sometimes show hallucinations as their reasoning procedures are unconstrained
by logical principles. Aiming at improving the zero-shot chain-of-thought
reasoning ability of large language models, we propose LoT (Logical Thoughts),
a self-improvement prompting framework that leverages principles rooted in
symbolic logic, particularly Reductio ad Absurdum, to systematically verify and
rectify the reasoning processes step by step. Experimental evaluations
conducted on language tasks in diverse domains, including arithmetic,
commonsense, symbolic, causal inference, and social problems, demonstrate the
efficacy of enhanced reasoning by logic. The implementation code for LoT can be
accessed at: \url{https://github.com/xf-zhao/LoT}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in COLING 2024. Code see https://github.com/xf-zhao/LoT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Between Lines of Code: Unraveling the Distinct Patterns of Machine and
  Human Programmers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06461v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06461v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuling Shi, Hongyu Zhang, Chengcheng Wan, Xiaodong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have catalyzed an unprecedented wave in code
generation. While achieving significant advances, they blur the distinctions
between machine- and human-authored source code, causing integrity and
authenticity issues of software artifacts. Previous methods such as DetectGPT
have proven effective in discerning machine-generated texts, but they do not
identify and harness the unique patterns of machine-generated code. Thus, its
applicability falters when applied to code. In this paper, we carefully study
the specific patterns that characterize machine- and human-authored code.
Through a rigorous analysis of code attributes such as lexical diversity,
conciseness, and naturalness, we expose unique patterns inherent to each
source. We particularly notice that the syntactic segmentation of code is a
critical factor in identifying its provenance. Based on our findings, we
propose DetectCodeGPT, a novel method for detecting machine-generated code,
which improves DetectGPT by capturing the distinct stylized patterns of code.
Diverging from conventional techniques that depend on external LLMs for
perturbations, DetectCodeGPT perturbs the code corpus by strategically
inserting spaces and newlines, ensuring both efficacy and efficiency.
Experiment results show that our approach significantly outperforms
state-of-the-art techniques in detecting machine-generated code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>code available at https://github.com/YerbaPage/DetectCodeGPT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Examining the Limitations of Computational Rumor Detection Models
  Trained on Static <span class="highlight-title">Dataset</span>s <span class="chip">LREC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11576v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11576v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Mu, Xingyi Song, Kalina Bontcheva, Nikolaos Aletras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A crucial aspect of a rumor detection model is its ability to generalize,
particularly its ability to detect emerging, previously unknown rumors. Past
research has indicated that content-based (i.e., using solely source posts as
input) rumor detection models tend to perform less effectively on unseen
rumors. At the same time, the potential of context-based models remains largely
untapped. The main contribution of this paper is in the in-depth evaluation of
the performance gap between content and context-based models specifically on
detecting new, unseen rumors. Our empirical findings demonstrate that
context-based models are still overly dependent on the information derived from
the rumors' source post and tend to overlook the significant role that
contextual information can play. We also study the effect of data split
strategies on classifier performance. Based on our experimental results, the
paper also offers practical suggestions on how to minimize the effects of
temporal concept drift in static datasets during the training of rumor
detection methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-03-28T05:25:09.526196497Z">
            2024-03-28 05:25:09 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
